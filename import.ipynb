{
 "cells": [
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "# Importing data from the Internet"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Importing flat files from the web"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 1. Importing flat files from the web\n",
    "\n",
    "You're now able to import data in Python from all sorts of file types:"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 2. You’re already great at importing!\n",
    "\n",
    "flat files such as dot txt's and dot csv's, other file types such as pickled files, Excel spreadsheets and MATLAB files. You've also gained valuable experience in querying relational databases to import data from them using SQL. You have really come a long way, congratulations! However, all of these skills involve importing data from files that you have locally. Much of the time as a data scientist, these skills won't be quite enough because you won't always have the data that you need. You will need to import it from the web."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 3. Can you import web data?\n",
    "\n",
    "Say, for example, you want to import the Wine Quality dataset from the Machine Learning Repository hosted by the University of California, Irvine. How do you get this file from the web? Now you could use your favourite web browser of choice to navigate to the relevant URL, point and click on the appropriate hyperlinks to download the file but this poses a few problems. Firstly, it isn't written in code and so poses reproducibility problems. If another Data Scientist wanted to reproduce your workflow it, she would necessarily have to do so outside Python. Secondly, it is NOT scalable. If you wanted to download one hundred or one thousand such files, it would take one hundred or one thousand times as long, respectively, whereas if you wrote it in code, your workflow could scale."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 4. You’ll learn how to…\n",
    "\n",
    "As reproducibility and scalability are situated at the very heart of Data Science, you're going to learn in this chapter how to use Python code to import and locally save datasets from the world wide web. You'll also learn how to load such datasets into pandas dataframes directly from the web, whether they be flat files or otherwise. Then you'll place these skills in the wider context of making HTTP requests. In particular, you'll make HTTP GET requests, which in plain English means getting data from the web. You'll use these new Request skills to learn the basics of scraping HTML from the internet and you'll use the wonderful Python package BeautifulSoup to parse the HTML and turn it into data. There are a number of great packages to help us import web data: herein, you'll become familiar with the urllib and requests packages. We'll first check out urllib:"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 5. The urllib package\n",
    "\n",
    "\"This module provides a high-level interface for fetching data across the World Wide Web. In particular, the urlopen function is similar to the built-in function open, but accepts Universal Resource Locators (URLs) instead of filenames.\" Let's now dive directly in to importing data from the web with an example, importing the Wine Quality dataset for white wine. Don't get jealous: in the first interactive exercise, it will be your job to import the red wine dataset!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('data/winequality-white.csv', <http.client.HTTPMessage at 0x2307dd718b0>)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "\n",
    "urlretrieve(url,\"data/winequality-white.csv\")"
   ],
   "execution_count": 1,
   "cell_type": "code"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 6. How to automate file download in Python\n",
    "\n",
    "All we have done here is imported a function called urlretrieve from the request subpackage of the urllib package, we assigned the relevant URL as a string to the variable url. We then used the urlretrieve function to write the contents of the url to a file 'winequality-white dot csv'. Now it's your turn to do the same but for red wine!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 7. Let's practice!\n",
    "\n",
    "In the following interactive exercises you'll also figure out how to use pandas to load the contents of web files directly into pandas dataframes without first having to save them locally. Happy hacking!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Importing flat files from the web: your turn!\n",
    "\n",
    "<p>You are about to import your first file from the web! The flat file you will import will be <code>'winequality-red.csv'</code> from the University of California, Irvine's <a href=\"https://archive.ics.uci.edu/ml/index.php\" target=\"_blank\" rel=\"noopener noreferrer\">Machine Learning repository</a>. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.</p>\n",
    "<p>The URL of the file is</p>\n",
    "<pre><code>'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "</code></pre>\n",
    "<p>After you import it, you'll check your working directory to confirm that it is there and then you'll load it into a <code>pandas</code> DataFrame.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Import the function <code>urlretrieve</code> from the subpackage <code>urllib.request</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Assign the URL of the file to the variable <code>url</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Use the function <code>urlretrieve()</code> to save the file locally as <code>'winequality-red.csv'</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Execute the remaining code to load <code>'winequality-red.csv'</code> in a pandas DataFrame and to print its head to the shell."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Assign url of file: url\n",
    "url = \"https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv\"\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url,\"data/winequality-red.csv\")\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('data/winequality-red.csv', sep=';')\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Opening and reading flat files from the web\n",
    "\n",
    "<p>You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using <code>pandas</code>. In particular, you can use the function <code>pd.read_csv()</code> with the URL as the first argument and the separator <code>sep</code> as the second argument.</p>\n",
    "<p>The URL of the file, once again, is</p>\n",
    "<pre><code>'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "</code></pre>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assign the URL of the file to the variable <code>url</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "# Assign url of file: url\n",
    "url = \"https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read file into a DataFrame <code>df</code> using <code>pd.read_csv()</code>, recalling that the separator in the file is <code>';'</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv(url,sep=\";\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the head of the DataFrame <code>df</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Execute the rest of the code to plot histogram of the first feature in the DataFrame <code>df</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot first column of df\n",
    "df.iloc[:, 0].hist()\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Importing non-flat files from the web\n",
    "\n",
    "<p>Congrats! You've just loaded a flat file from the web into a DataFrame without first saving it locally using the <code>pandas</code> function <code>pd.read_csv()</code>. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you'll use <code>pd.read_excel()</code> to import an Excel spreadsheet.</p>\n",
    "<p>The URL of the spreadsheet is</p>\n",
    "<pre><code>'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "</code></pre>\n",
    "<p>Your job is to use <code>pd.read_excel()</code> to read in all of its sheets, print the sheet names and then print the head of the first sheet <em>using its name, not its index</em>.</p>\n",
    "<p>Note that the output of <code>pd.read_excel()</code> is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Assign the URL of the file to the variable <code>url</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Assign url of file: url\n",
    "url ='https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Read the file in <code>url</code> into a dictionary <code>xls</code> using <code>pd.read_excel()</code> recalling that, in order to import all sheets you need to pass <code>None</code> to the argument <code>sheet_name</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "xls = pd.read_excel(url,sheet_name=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary <code>xls</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1700', '1900'])\n"
     ]
    }
   ],
   "source": [
    "print(xls.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Print the head of the first sheet <em>using the sheet name, not the index of the sheet</em>! The sheet name is <code>'1700'</code>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 country       1700\n",
      "0            Afghanistan  34.565000\n",
      "1  Akrotiri and Dhekelia  34.616667\n",
      "2                Albania  41.312000\n",
      "3                Algeria  36.720000\n",
      "4         American Samoa -14.307000\n"
     ]
    }
   ],
   "source": [
    "print(xls[\"1700\"].head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## HTTP requests to import files from the web"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 1. HTTP requests to import files from the web\n",
    "\n",
    "Congrats on importing your first web data! In order to import files from the web,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 2. URL\n",
    "\n",
    "we used the urlretrieve function from urllib dot requests. Lets now unpack this a bit and, in the process, understand a few things about how the internet works. URL stands for Uniform or Universal Resource Locator and all they really are are references to web resources. The vast majority of URLs are web addresses, but they can refer to a few other things, such as file transfer protocols (FTP) and database access. We'll currently focus on those URLs that are web addresses OR the locations of websites. Such a URL consists of 2 parts, a protocol identifier http or https and a resource name such as datacamp dot com. The combination of protocol identifier and resource name uniquely specifies the web address! To explain URLs, I have introduced yet another acronym"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 3. HTTP\n",
    "\n",
    "http, which itself stands for HyperText Transfer Protocol. Wikipedia provides a great description of HTTP. \"The Hypertext Transfer Protocol (HTTP) is an application protocol for distributed, collaborative, hypermedia information systems. HTTP is the foundation of data communication for the World Wide Web.\" Note that HTTPS is a more secure form of HTTP. Each time you go to a website, you are actually sending an HTTP request to a server. This request is known as a GET request, by far the most common type of HTTP request. We are actually performing a GET request when using the function urlretrieve. The ingenuity of urlretrieve also lies in fact that it not only makes a GET request but also saves the relevant data locally. In the following, you'll learn how to make more GET requests to store web data in your environment. In particular, you'll figure out how to get the HTML data from a webpage. HTML stands for Hypertext Markup Language and is the standard markup language for the web."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 4. GET requests using urllib\n",
    "\n",
    "To extract the html from the wikipedia home page, you import the necessary functions, specify the URL, package the GET request using the function Request, send the request and catch the response using the function urlopen. This returns an HTTPResponse object, which has an associated read method. You then apply this read method to the response, which returns the HTML as a string, which you store in the variable html. You remember to be polite and close the response!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen,Request\n",
    "\n",
    "url =  \"https://www.wikipedia.org/\"\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read()\n",
    "response.close()"
   ],
   "execution_count": 20,
   "cell_type": "code"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html lang=\"en\" class=\"no-js\">\\n<head>\\n<meta charset=\"utf-8\">\\n<title>Wikipedia</title>\\n<meta name=\"description\" content=\"Wikipedia is a free online encyclopedia, created and edited by volunteers around the world and hosted by the Wikimedia Foundation.\">\\n<script>\\ndocument.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)no-js(\\\\s|$)/, \"$1js-enabled$2\" );\\n</script>\\n<meta name=\"viewport\" content=\"initial-scale=1,user-scalable=yes\">\\n<link rel=\"apple-touch-icon\" href=\"/static/apple-touch/wikipedia.png\">\\n<link rel=\"shortcut icon\" href=\"/static/favicon/wikipedia.ico\">\\n<link rel=\"license\" href=\"//creativecommons.org/licenses/by-sa/3.0/\">\\n<style>\\n.sprite{background-image:linear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/sprite-e99844f6.svg);background-repeat:no-repeat;display:inline-block;vertical-align:middle}.svg-Commons-logo_sister{background-position:0 0;width:47px;height:47px}.svg-MediaWiki-logo_sister{background-position:0 -47px;width:42px;height:42px}.svg-Meta-Wiki-logo_sister{background-position:0 -89px;width:37px;height:37px}.svg-Wikibooks-logo_sister{background-position:0 -126px;width:37px;height:37px}.svg-Wikidata-logo_sister{background-position:0 -163px;width:49px;height:49px}.svg-Wikimedia-logo_black{background-position:0 -212px;width:42px;height:42px}.svg-Wikipedia_wordmark{background-position:0 -254px;width:176px;height:32px}.svg-Wikiquote-logo_sister{background-position:0 -286px;width:42px;height:42px}.svg-Wikisource-logo_sister{background-position:0 -328px;width:39px;height:39px}.svg-Wikispecies-logo_sister{background-position:0 -367px;width:42px;height:42px}.svg-Wikiversity-logo_sister{background-position:0 -409px;width:43px;height:37px}.svg-Wikivoyage-logo_sister{background-position:0 -446px;width:36px;height:36px}.svg-Wiktionary-logo_sister{background-position:0 -482px;width:37px;height:37px}.svg-arrow-down{background-position:0 -519px;width:12px;height:8px}.svg-arrow-down-blue{background-position:0 -527px;width:14px;height:14px}.svg-badge_google_play_store{background-position:0 -541px;width:124px;height:38px}.svg-badge_ios_app_store{background-position:0 -579px;width:110px;height:38px}.svg-language-icon{background-position:0 -617px;width:22px;height:22px}.svg-noimage{background-position:0 -639px;width:58px;height:58px}.svg-search-icon{background-position:0 -697px;width:22px;height:22px}.svg-wikipedia_app_tile{background-position:0 -719px;width:42px;height:42px}\\n</style>\\n<style>\\nhtml{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;font-size:62.5%}body{margin:0}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:700}dfn{font-style:italic}h1{font-size:32px;font-size:3.2rem;margin:1.2rem 0}mark{background:#fc3;color:#000}small{font-size:13px;font-size:1.3rem}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}svg:not(:root){overflow:hidden}figure{margin:1.6rem 4rem}hr{-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace,monospace;font-size:14px;font-size:1.4rem}button,input,optgroup,select,textarea{color:inherit;font:inherit;margin:0}button{overflow:visible}button,select{text-transform:none}button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}input{line-height:normal}input[type=checkbox],input[type=radio]{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;padding:0}input[type=number]::-webkit-inner-spin-button,input[type=number]::-webkit-outer-spin-button{height:auto}input[type=search]{-webkit-appearance:none;-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}input[type=search]::-webkit-search-cancel-button,input[type=search]::-webkit-search-decoration{-webkit-appearance:none}input[type=search]:focus{outline-offset:-2px}fieldset{border:1px solid #a2a9b1;margin:0 .2rem;padding:.6rem 1rem 1.2rem}legend{border:0;padding:0}textarea{overflow:auto}optgroup{font-weight:700}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}.hidden,[hidden]{display:none!important}.screen-reader-text{display:block;position:absolute!important;clip:rect(1px,1px,1px,1px);width:1px;height:1px;margin:-1px;border:0;padding:0;overflow:hidden}body{background-color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Lato,Helvetica,Arial,sans-serif;font-size:14px;font-size:1.4rem;line-height:1.5;margin:.4rem 0 1.6rem}a{-ms-touch-action:manipulation;touch-action:manipulation}a,a:active,a:focus{unicode-bidi:embed;outline:0;color:#36c;text-decoration:none}a:focus{outline:1px solid #36c}a:hover{text-decoration:underline}img{vertical-align:middle}hr,img{border:0}hr{clear:both;height:0;border-bottom:1px solid #c8ccd1;margin:.26rem 1.3rem}.pure-button{display:inline-block;zoom:1;line-height:normal;white-space:nowrap;text-align:center;cursor:pointer;-webkit-user-drag:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;background-color:#f8f9fa;color:#202122;position:relative;min-height:19.2px;min-height:1.92rem;min-width:16px;min-width:1.6rem;margin:.16rem 0;border:1px solid #a2a9b1;-moz-border-radius:2px;border-radius:2px;padding:.8rem 1.6rem;font-family:inherit;font-size:inherit;font-weight:700;text-decoration:none;vertical-align:top;-webkit-transition:background .1s ease,color .1s ease,border-color .1s ease,-webkit-box-shadow .1s ease;transition:background .1s ease,color .1s ease,border-color .1s ease,-webkit-box-shadow .1s ease;-o-transition:background .1s ease,color .1s ease,border-color .1s ease,box-shadow .1s ease;-moz-transition:background .1s ease,color .1s ease,border-color .1s ease,box-shadow .1s ease,-moz-box-shadow .1s ease;transition:background .1s ease,color .1s ease,border-color .1s ease,box-shadow .1s ease;transition:background .1s ease,color .1s ease,border-color .1s ease,box-shadow .1s ease,-webkit-box-shadow .1s ease,-moz-box-shadow .1s ease}.pure-button::-moz-focus-inner{padding:0;border:0}.pure-button-hover,.pure-button:hover{background-color:#fff;border-color:#a2a9b1;color:#404244}.pure-button-active,.pure-button:active{background-color:#c8ccd1;border-color:#72777d;color:#000}.pure-button:focus{outline:0;border-color:#36c;-webkit-box-shadow:inset 0 0 0 1px #36c;-moz-box-shadow:inset 0 0 0 1px #36c;box-shadow:inset 0 0 0 1px #36c}.pure-button-primary-progressive{background-color:#36c;border-color:#36c;color:#fff}.pure-button-primary-progressive:hover{background:#447ff5;border-color:#447ff5}.pure-button-primary-progressive:active{background-color:#2a4b8d;border-color:#2a4b8d;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none;color:#fff}.pure-button-primary-progressive:focus{-webkit-box-shadow:inset 0 0 0 1px #36c,inset 0 0 0 2px #fff;-moz-box-shadow:inset 0 0 0 1px #36c,inset 0 0 0 2px #fff;box-shadow:inset 0 0 0 1px #36c,inset 0 0 0 2px #fff;border-color:#36c}.pure-form input[type=search]{background-color:#fff;display:inline-block;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;border:1px solid #a2a9b1;-moz-border-radius:2px;border-radius:2px;padding:.8rem;-webkit-box-shadow:inset 0 0 0 1px #fff;-moz-box-shadow:inset 0 0 0 1px #fff;box-shadow:inset 0 0 0 1px #fff;vertical-align:middle}.pure-form input:focus:invalid{color:#b32424;border-color:#d33}.pure-form fieldset{margin:0;padding:.56rem 0 1.2rem;border:0}@media only screen and (max-width:480px){.pure-form input[type=search]{display:block}}.central-textlogo-wrapper{display:inline-block;vertical-align:bottom}.central-textlogo{position:relative;margin:4rem auto .5rem;width:270px;font-family:Linux Libertine,Hoefler Text,Georgia,Times New Roman,Times,serif;font-size:30px;font-size:3rem;font-weight:400;line-height:33px;line-height:3.3rem;text-align:center;-moz-font-feature-settings:\"ss05=1\";-moz-font-feature-settings:\"ss05\";-webkit-font-feature-settings:\"ss05\";-ms-font-feature-settings:\"ss05\";font-feature-settings:\"ss05\"}.localized-slogan{display:block;font-family:Linux Libertine,Georgia,Times,serif;font-size:15px;font-size:1.5rem;font-weight:400}.central-textlogo__image{color:transparent;display:inline-block;overflow:hidden;text-indent:-10000px}.central-featured-logo{position:absolute;top:158px;left:35px}@media (max-width:480px){.central-textlogo{position:relative;height:70px;width:auto;margin:2rem 0 0;text-align:center;line-height:25px;line-height:2.5rem;text-indent:-10px;text-indent:-1rem;font-size:1em}.central-textlogo-wrapper{position:relative;top:12px;text-indent:2px;text-indent:.2rem}.svg-Wikipedia_wordmark{width:150px;height:25px;background-position:0 -218px;-webkit-background-size:100% 100%;-moz-background-size:100%;background-size:100%}.localized-slogan{font-size:14px;font-size:1.4rem}.central-featured-logo{position:relative;display:inline-block;width:57px;height:auto;left:0;top:0}}@media (max-width:240px){.central-textlogo__image{height:auto}}.central-featured{position:relative;height:325px;height:32.5rem;width:546px;width:54.6rem;max-width:100%;margin:0 auto;text-align:center;vertical-align:middle}.central-featured-lang{position:absolute;width:156px;width:15.6rem}.central-featured-lang .link-box{display:block;padding:0;text-decoration:none;white-space:normal}.central-featured-lang .link-box:hover strong{text-decoration:underline}.central-featured-lang :hover{background-color:#eaecf0}.central-featured-lang strong{display:block;font-size:16px;font-size:1.6rem}.central-featured-lang small{color:#54595d;display:inline-block;font-size:13px;font-size:1.3rem;line-height:1.6}.central-featured-lang em{font-style:italic}.central-featured-lang .emNonItalicLang{font-style:normal}.lang1{top:0;right:60%}.lang2{top:0;left:60%}.lang3{top:20%;right:70%}.lang4{top:20%;left:70%}.lang5{top:40%;right:72%}.lang6{top:40%;left:72%}.lang7{top:60%;right:70%}.lang8{top:60%;left:70%}.lang9{top:80%;right:60%}.lang10{top:80%;left:60%}@media (max-width:480px){.central-featured{width:auto;height:auto;margin-top:7rem;font-size:13px;font-size:1.3rem;text-align:left}.central-featured:after{content:\" \";display:block;visibility:hidden;clear:both;height:0;font-size:0}.central-featured-lang{display:block;float:left;position:relative;top:auto;left:auto;right:auto;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;height:64px;height:6.4rem;width:33%;margin:0 0 16px;padding:0 1.6rem;font-size:14px;font-size:1.4rem;text-align:center}.central-featured-lang strong{font-size:14px;font-size:1.4rem;margin-bottom:4px}.central-featured-lang small{line-height:1.4}}@media (max-width:375px){.central-featured-lang{font-size:13px;font-size:1.3rem}}@media (max-width:240px){.central-featured-lang{width:100%}}.search-container{float:none;max-width:95%;width:540px;margin:.4rem auto 1.95rem;text-align:center;vertical-align:middle}.search-container fieldset{word-spacing:-4px}.search-container button{min-height:44px;min-height:4.4rem;margin:0;-moz-border-radius:0 2px 2px 0;border-radius:0 2px 2px 0;padding:.8rem 1.6rem;font-size:16px;font-size:1.6rem;z-index:2}.search-container button .svg-search-icon{text-indent:-9999px}.search-container input[type=search]::-webkit-search-results-button,.search-container input[type=search]::-webkit-search-results-decoration{-webkit-appearance:none}.search-container input::-webkit-calendar-picker-indicator{display:none}.search-container .sprite.svg-arrow-down{position:absolute;top:8px;top:.8rem;right:6px;right:.6rem}#searchInput{-webkit-appearance:none;width:100%;height:44px;height:4.4rem;border-width:1px 0 1px 1px;-moz-border-radius:2px 0 0 2px;border-radius:2px 0 0 2px;padding:.8rem 9.6rem .8rem 1.2rem;font-size:16px;font-size:1.6rem;line-height:1.6;-webkit-transition:background .1s ease,border-color .1s ease,-webkit-box-shadow .1s ease;transition:background .1s ease,border-color .1s ease,-webkit-box-shadow .1s ease;-o-transition:background .1s ease,border-color .1s ease,box-shadow .1s ease;-moz-transition:background .1s ease,border-color .1s ease,box-shadow .1s ease,-moz-box-shadow .1s ease;transition:background .1s ease,border-color .1s ease,box-shadow .1s ease;transition:background .1s ease,border-color .1s ease,box-shadow .1s ease,-webkit-box-shadow .1s ease,-moz-box-shadow .1s ease}#searchInput:hover{border-color:#72777d}#searchInput:focus{border-color:#36c;-webkit-box-shadow:inset 0 0 0 1px #36c;-moz-box-shadow:inset 0 0 0 1px #36c;box-shadow:inset 0 0 0 1px #36c;outline:0}.search-container .search-input{display:inline-block;position:relative;width:73%;vertical-align:top}@media only screen and (max-width:480px){.search-container .pure-form fieldset{margin-left:1rem;margin-right:6.6rem}.search-container .search-input{width:100%;margin-right:-6.6rem}.search-container .pure-form button{float:right;right:-56px;right:-5.6rem}}.suggestions-dropdown{background-color:#fff;display:inline-block;position:absolute;left:0;z-index:2;margin:0;padding:0;border:1px solid #a2a9b1;border-top:0;-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,.25);-moz-box-shadow:0 2px 2px 0 rgba(0,0,0,.25);box-shadow:0 2px 2px 0 rgba(0,0,0,.25);list-style-type:none;word-spacing:normal}.suggestion-link,.suggestions-dropdown{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;width:100%;text-align:left}.suggestion-link{display:block;position:relative;min-height:70px;min-height:7rem;padding:1rem 1rem 1rem 8.5rem;border-bottom:1px solid #eaecf0;color:inherit;text-decoration:none;text-align:initial;white-space:normal}.suggestion-link.active{background-color:#eaf3ff}a.suggestion-link:hover{text-decoration:none}a.suggestion-link:active,a.suggestion-link:focus{outline:0;white-space:normal}.suggestion-thumbnail{background-color:#eaecf0;background-image:url(portal/wikipedia.org/assets/img/noimage.png);background-image:-webkit-linear-gradient(transparent,transparent),url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns=\\'http://www.w3.org/2000/svg\\' viewBox=\\'0 0 56 56\\'%3E%3Cpath fill=\\'%23eee\\' d=\\'M0 0h56v56H0z\\'/%3E%3Cpath fill=\\'%23999\\' d=\\'M36.4 13.5H17.8v24.9c0 1.4.9 2.3 2.3 2.3h18.7v-25c.1-1.4-1-2.2-2.4-2.2zM30.2 17h5.1v6.4h-5.1V17zm-8.8 0h6v1.8h-6V17zm0 4.6h6v1.8h-6v-1.8zm0 15.5v-1.8h13.8v1.8H21.4zm13.8-4.5H21.4v-1.8h13.8v1.8zm0-4.7H21.4v-1.8h13.8v1.8z\\'/%3E%3C/svg%3E\");background-image:-webkit-linear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/noimage.svg) !ie;background-image:-webkit-gradient(linear,left top,left bottom,from(transparent),to(transparent)),url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns=\\'http://www.w3.org/2000/svg\\' viewBox=\\'0 0 56 56\\'%3E%3Cpath fill=\\'%23eee\\' d=\\'M0 0h56v56H0z\\'/%3E%3Cpath fill=\\'%23999\\' d=\\'M36.4 13.5H17.8v24.9c0 1.4.9 2.3 2.3 2.3h18.7v-25c.1-1.4-1-2.2-2.4-2.2zM30.2 17h5.1v6.4h-5.1V17zm-8.8 0h6v1.8h-6V17zm0 4.6h6v1.8h-6v-1.8zm0 15.5v-1.8h13.8v1.8H21.4zm13.8-4.5H21.4v-1.8h13.8v1.8zm0-4.7H21.4v-1.8h13.8v1.8z\\'/%3E%3C/svg%3E\");background-image:-moz- oldlinear-gradient(transparent,transparent),url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns=\\'http://www.w3.org/2000/svg\\' viewBox=\\'0 0 56 56\\'%3E%3Cpath fill=\\'%23eee\\' d=\\'M0 0h56v56H0z\\'/%3E%3Cpath fill=\\'%23999\\' d=\\'M36.4 13.5H17.8v24.9c0 1.4.9 2.3 2.3 2.3h18.7v-25c.1-1.4-1-2.2-2.4-2.2zM30.2 17h5.1v6.4h-5.1V17zm-8.8 0h6v1.8h-6V17zm0 4.6h6v1.8h-6v-1.8zm0 15.5v-1.8h13.8v1.8H21.4zm13.8-4.5H21.4v-1.8h13.8v1.8zm0-4.7H21.4v-1.8h13.8v1.8z\\'/%3E%3C/svg%3E\");background-image:-o-linear-gradient(transparent,transparent),url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns=\\'http://www.w3.org/2000/svg\\' viewBox=\\'0 0 56 56\\'%3E%3Cpath fill=\\'%23eee\\' d=\\'M0 0h56v56H0z\\'/%3E%3Cpath fill=\\'%23999\\' d=\\'M36.4 13.5H17.8v24.9c0 1.4.9 2.3 2.3 2.3h18.7v-25c.1-1.4-1-2.2-2.4-2.2zM30.2 17h5.1v6.4h-5.1V17zm-8.8 0h6v1.8h-6V17zm0 4.6h6v1.8h-6v-1.8zm0 15.5v-1.8h13.8v1.8H21.4zm13.8-4.5H21.4v-1.8h13.8v1.8zm0-4.7H21.4v-1.8h13.8v1.8z\\'/%3E%3C/svg%3E\");background-image:linear-gradient(transparent,transparent),url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns=\\'http://www.w3.org/2000/svg\\' viewBox=\\'0 0 56 56\\'%3E%3Cpath fill=\\'%23eee\\' d=\\'M0 0h56v56H0z\\'/%3E%3Cpath fill=\\'%23999\\' d=\\'M36.4 13.5H17.8v24.9c0 1.4.9 2.3 2.3 2.3h18.7v-25c.1-1.4-1-2.2-2.4-2.2zM30.2 17h5.1v6.4h-5.1V17zm-8.8 0h6v1.8h-6V17zm0 4.6h6v1.8h-6v-1.8zm0 15.5v-1.8h13.8v1.8H21.4zm13.8-4.5H21.4v-1.8h13.8v1.8zm0-4.7H21.4v-1.8h13.8v1.8z\\'/%3E%3C/svg%3E\");background-image:-webkit-gradient(linear,left top,left bottom,from(transparent),to(transparent)),url(portal/wikipedia.org/assets/img/noimage.svg) !ie;background-image:-moz- oldlinear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/noimage.svg) !ie;background-image:-o-linear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/noimage.svg) !ie;background-image:linear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/noimage.svg) !ie;background-image:-o-linear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/noimage.png);background-position:50%;background-repeat:no-repeat;-webkit-background-size:100% auto;-moz-background-size:100% auto;background-size:100% auto;-webkit-background-size:cover;-moz-background-size:cover;background-size:cover;height:100%;width:70px;width:7rem;position:absolute;top:0;left:0}.suggestion-title{margin:0 0 .78rem;color:#54595d;font-size:16px;font-size:1.6rem;line-height:18.72px;line-height:1.872rem}.suggestion-link.active .suggestion-title{color:#36c}.suggestion-highlight{font-style:normal;text-decoration:underline}.suggestion-description{color:#72777d;margin:0;font-size:13px;font-size:1.3rem;line-height:14.299px;line-height:1.43rem}.styled-select{display:none;position:absolute;top:10px;top:1rem;bottom:12px;bottom:1.2rem;right:12px;right:1.2rem;max-width:95px;max-width:9.5rem;height:24px;height:2.4rem;-moz-border-radius:2px;border-radius:2px}.styled-select:hover{background-color:#f8f9fa}.styled-select .hide-arrow{right:32px;right:3.2rem;max-width:68px;max-width:6.8rem;height:24px;height:2.4rem;overflow:hidden;text-align:right}.styled-select select{background:transparent;display:inline;overflow:hidden;height:24px;height:2.4rem;min-width:110px;min-width:11rem;max-width:110px;max-width:11rem;width:110px;width:11rem;outline:0;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;border:0;line-height:24px;line-height:2.4rem;-webkit-appearance:none;-moz-appearance:window;text-indent:.01px;-o-text-overflow:\"\";text-overflow:\"\";opacity:0;-moz-appearance:none;appearance:none;cursor:pointer}.styled-select.no-js{width:95px;width:9.5rem}.styled-select.no-js select{opacity:1;margin:0;padding:0 2.4rem 0 .8rem;color:#54595d}.styled-select.no-js .hide-arrow{width:68px;width:6.8rem}.search-container .styled-select.no-js .js-langpicker-label{display:none}.styled-select.js-enabled .hide-arrow{padding:0 2.4rem 0 .8rem}.styled-select.js-enabled select{background:transparent;position:absolute;top:0;left:0;height:100%;z-index:1;width:100%;border:0;margin:0;padding:0 2.4rem;color:transparent;color:hsla(0,0%,100%,0)}.styled-select.js-enabled select option{color:#54595d}.styled-select.js-enabled select:hover{background-color:transparent}.styled-select-active-helper{display:none}.styled-select.js-enabled select:focus+.styled-select-active-helper{display:block;position:absolute;top:0;left:0;z-index:0;width:100%;height:100%;outline:1px solid #36c}.search-container .js-langpicker-label{display:inline-block;margin:0;color:#54595d;font-size:13px;font-size:1.3rem;line-height:24px;line-height:2.4rem;text-transform:uppercase}.styled-select select:hover{background-color:#f8f9fa}.styled-select select::-ms-expand{display:none}.styled-select select:focus{outline:0;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none}@-moz-document url-prefix(){.styled-select select{width:110%}}.other-projects{display:inline-block;width:65%}.other-project{float:left;position:relative;width:33%;height:90px;height:9rem}.other-project-link{display:inline-block;min-height:50px;width:90%;padding:1em;white-space:nowrap}.other-project-link:hover{background-color:#eaecf0}a.other-project-link{text-decoration:none}.other-project-icon{display:inline-block;width:50px;text-align:center}.svg-Wikinews-logo_sister{background-image:url(portal/wikipedia.org/assets/img/Wikinews-logo_sister.png);background-position:0 0;-webkit-background-size:47px 26px;-moz-background-size:47px 26px;background-size:47px 26px;width:47px;height:26px}@media (-o-min-device-pixel-ratio:5/4),(-webkit-min-device-pixel-ratio:1.25),(min-resolution:120dpi){.svg-Wikinews-logo_sister{background-image:url(portal/wikipedia.org/assets/img/Wikinews-logo_sister@2x.png)}}.other-project-text,.other-project .sprite-project-logos{display:inline-block}.other-project-text{max-width:65%;font-size:14px;font-size:1.4rem;vertical-align:middle;white-space:normal}.other-project-tagline,.other-project-title{display:block}.other-project-tagline{color:#54595d;font-size:13px;font-size:1.3rem}@media screen and (max-width:768px){.other-projects{width:100%}.other-project{width:33%}}@media screen and (max-width:480px){.other-project{width:50%}.other-project-tagline{-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}}@media screen and (max-width:320px){.other-project-text{margin-right:5px;font-size:13px;font-size:1.3rem}}.lang-list-container{background-color:#f8f9fa;overflow:hidden;position:relative;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;max-height:0;width:80%;margin:-1.6rem auto 4.8rem;-webkit-transition:max-height .5s ease-out .16s,visibility .5s ease-in 1s;-o-transition:max-height .5s ease-out .16s,visibility .5s ease-in 1s;-moz-transition:max-height .5s ease-out .16s,visibility .5s ease-in 1s;transition:max-height .5s ease-out .16s,visibility .5s ease-in 1s}.js-enabled .lang-list-container{visibility:hidden}.lang-list-active .lang-list-container,.no-js .lang-list-container{visibility:visible;max-height:10000px;-webkit-transition:max-height 1s ease-in .2s,visibility 1000s ease-in 0ms;-o-transition:max-height 1s ease-in .2s,visibility 1000s ease-in 0ms;-moz-transition:max-height 1s ease-in .2s,visibility 1000s ease-in 0ms;transition:max-height 1s ease-in .2s,visibility 1000s ease-in 0ms}.no-js .lang-list-button{display:none}.lang-list-button-wrapper{text-align:center}.lang-list-button{background-color:#f8f9fa;display:inline;position:relative;z-index:1;margin:0 auto;padding:.6rem 1.2rem;outline:16px solid #fff;outline:1.6rem solid #fff;border:1px solid #a2a9b1;-moz-border-radius:2px;border-radius:2px;color:#36c;font-size:14px;font-size:1.4rem;font-weight:700;line-height:1;-webkit-transition:outline-width .1s ease-in .5s;-o-transition:outline-width .1s ease-in .5s;-moz-transition:outline-width .1s ease-in .5s;transition:outline-width .1s ease-in .5s}.lang-list-button:hover{background-color:#fff;border-color:#a2a9b1}.lang-list-button:focus{border-color:#36c;-webkit-box-shadow:inset 0 0 0 1px #36c;-moz-box-shadow:inset 0 0 0 1px #36c;box-shadow:inset 0 0 0 1px #36c}.lang-list-active .lang-list-button{background-color:#fff;outline:1px solid #fff;border-color:#72777d;-webkit-transition-delay:0s;-moz-transition-delay:0s;-o-transition-delay:0s;transition-delay:0s}.lang-list-button-text{padding:0 .64rem;vertical-align:middle}.lang-list-button i{display:inline-block;vertical-align:middle}.no-js .lang-list-border,.no-js .lang-list-button{display:none}.lang-list-border{background-color:#c8ccd1;display:block;position:relative;max-width:460px;width:80%;margin:-1.6rem auto 1.6rem;height:1px;-webkit-transition:max-width .2s ease-out .4s;-o-transition:max-width .2s ease-out .4s;-moz-transition:max-width .2s ease-out .4s;transition:max-width .2s ease-out .4s}.lang-list-active .lang-list-border{max-width:85%;-webkit-transition-delay:0s;-moz-transition-delay:0s;-o-transition-delay:0s;transition-delay:0s}.no-js .lang-list-content{padding:0}.lang-list-content{position:relative;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;width:100%;padding:1.6rem 1.6rem 0}.svg-arrow-down-blue{-webkit-transition:-webkit-transform .2s ease-out;transition:-webkit-transform .2s ease-out;-o-transition:transform .2s ease-out;-moz-transition:transform .2s ease-out,-moz-transform .2s ease-out;transition:transform .2s ease-out;transition:transform .2s ease-out,-webkit-transform .2s ease-out,-moz-transform .2s ease-out}.lang-list-active .svg-arrow-down-blue{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);transform:rotate(180deg)}.langlist{width:auto;margin:1.6rem 0;text-align:left}.langlist-others{font-weight:700;text-align:center}.hlist ul{margin:0;padding:0}.hlist li,.hlist ul ul{display:inline}.hlist li:before{content:\" \\xc2\\xb7 \";font-weight:700}.hlist li:first-child:before{content:none}.hlist li>ul:before{content:\"\\\\00a0(\"}.hlist li>ul:after{content:\") \"}.langlist>ul{-webkit-column-width:11.2rem;-moz-column-width:11.2rem;column-width:11.2rem}.langlist>ul>li{display:block;line-height:1.7;-webkit-column-break-inside:avoid;page-break-inside:avoid;break-inside:avoid}.no-js .langlist>ul{text-align:center;list-style-type:circle}.no-js .langlist>ul>li{display:inline-block;padding:0 .8rem}.langlist>ul>li:before{content:none}.langlist>ul>li a{white-space:normal}@media (max-width:480px){.langlist{font-size:inherit}.langlist a{word-wrap:break-word;white-space:normal}.lang-list-container{width:auto;margin-left:.8rem;margin-right:.8rem}.bookshelf{overflow:visible}}.bookshelf{display:block;border-top:1px solid #c8ccd1;-webkit-box-shadow:0 -1px 0 #fff;-moz-box-shadow:0 -1px 0 #fff;box-shadow:0 -1px 0 #fff;text-align:center;white-space:nowrap}.bookshelf .text{background-color:#f8f9fa;position:relative;top:-11.2px;top:-1.12rem;font-weight:400;padding:0 .8rem}.bookshelf-container{display:block;overflow:visible;width:100%;height:1px;margin:2.4rem 0 1.6rem;font-size:13px;font-size:1.3rem;font-weight:700;line-height:1.5}@media (max-width:480px){.bookshelf{width:auto;left:auto}.bookshelf-container{text-align:left;width:auto}}.app-badges .footer-sidebar-content{background-color:#f8f9fa}.app-badges .footer-sidebar-text{padding-top:.8rem;padding-bottom:.8rem}.app-badges .sprite.footer-sidebar-icon{top:8px;top:.8rem}.app-badges ul{margin:0;padding:0;list-style-type:none}.app-badge{display:inline-block}.app-badge a{color:transparent}@media screen and (max-width:768px){.app-badges .footer-sidebar-content{text-align:center}.app-badges .sprite.footer-sidebar-icon{display:inline-block;position:relative;margin:0;top:-3px;left:0;vertical-align:middle;-webkit-transform:scale(.7);-moz-transform:scale(.7);-ms-transform:scale(.7);transform:scale(.7)}}.footer{overflow:hidden;max-width:100%;margin:0 auto;padding:4.16rem 1.28rem 1.28rem;font-size:13px;font-size:1.3rem}.footer:after,.footer:before{content:\" \";display:table}.footer:after{clear:both}.footer-sidebar{width:35%;float:left;clear:left;margin-bottom:3.2rem;vertical-align:top}.footer-sidebar-content{position:relative;max-width:350px;margin:0 auto}.sprite.footer-sidebar-icon{position:absolute;top:0;left:8px;left:.8rem}.footer-sidebar-text{position:relative;margin:0;padding-left:6rem;padding-right:2rem;color:#54595d}.site-license{color:#54595d;text-align:center}.site-license small:after{content:\"\\\\2022\";display:inline-block;font-size:13px;font-size:1.3rem;line-height:inherit;margin-left:.8rem;margin-right:.5rem}.site-license small:last-child:after{display:none}@media screen and (max-width:768px){.footer{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;-moz-box-orient:vertical;-moz-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding-top:1.28rem}.footer .footer-sidebar{-webkit-box-ordinal-group:1;-moz-box-ordinal-group:1;-webkit-order:1;-ms-flex-order:1;order:1}.footer .other-projects{-webkit-box-ordinal-group:2;-moz-box-ordinal-group:2;-webkit-order:2;-ms-flex-order:2;order:2}.footer .app-badges{-webkit-box-ordinal-group:3;-moz-box-ordinal-group:3;-webkit-order:3;-ms-flex-order:3;order:3}.footer-sidebar{width:100%}.sprite.footer-sidebar-icon{display:block;position:relative;left:0;margin:0 auto 1.28rem}.footer-sidebar-content{max-width:none}.footer-sidebar-text{margin:0;padding:0;text-align:center}}@media screen and (max-width:480px){.footer{padding:.96rem .64rem 1.28rem}}@media (max-width:480px){.search-container{margin-top:0;height:78px;height:7.8rem;position:absolute;top:96px;top:9.6rem;left:0;right:0;max-width:100%;width:auto;padding:0;text-align:left}.search-container label{display:none}.search-form #searchInput{max-width:40%;vertical-align:middle}.search-form .formBtn{max-width:25%;vertical-align:middle}form fieldset{margin:0;border-left:0;border-right:0}hr{margin-top:.65rem}}@media (-o-min-device-pixel-ratio:2/1),(-webkit-min-device-pixel-ratio:2),(min--moz-device-pixel-ratio:2),(min-resolution:2dppx),(min-resolution:192dpi){hr{border-bottom-width:.5px}}@supports (-webkit-marquee-style:slide){hr{border-bottom-width:1px}}.js-enabled .central-featured,.js-enabled .jsl10n{visibility:hidden}.jsl10n-visible .central-featured,.jsl10n-visible .jsl10n{visibility:visible}@media print{body{background-color:transparent}a{color:#000!important;background:none!important;padding:0!important}a:link,a:visited{color:#520;background:transparent}img{border:0}}\\n</style>\\n<link rel=\"preconnect\" href=\"//upload.wikimedia.org\">\\n</head>\\n<body id=\"www-wikipedia-org\">\\n<div class=\"central-textlogo\">\\n<img class=\"central-featured-logo\" src=\"portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png\" srcset=\"portal/wikipedia.org/assets/img/Wikipedia-logo-v2@1.5x.png 1.5x, portal/wikipedia.org/assets/img/Wikipedia-logo-v2@2x.png 2x\" width=\"200\" height=\"183\" alt=\"Wikipedia\">\\n<h1 class=\"central-textlogo-wrapper\">\\n<span class=\"central-textlogo__image sprite svg-Wikipedia_wordmark\">\\nWikipedia\\n</span>\\n<strong class=\"jsl10n localized-slogan\" data-jsl10n=\"portal.slogan\">The Free Encyclopedia</strong>\\n</h1>\\n</div>\\n<div class=\"central-featured\" data-el-section=\"primary links\">\\n<!-- #1. en.wikipedia.org - 1 602 819 000 views/day -->\\n<div class=\"central-featured-lang lang1\" lang=\"en\" dir=\"ltr\">\\n<a id=\"js-link-box-en\" href=\"//en.wikipedia.org/\" title=\"English \\xe2\\x80\\x94 Wikipedia \\xe2\\x80\\x94 The Free Encyclopedia\" class=\"link-box\" data-slogan=\"The Free Encyclopedia\">\\n<strong>English</strong>\\n<small><bdi dir=\"ltr\">6&nbsp;458&nbsp;000+</bdi> <span>articles</span></small>\\n</a>\\n</div>\\n<!-- #2. ja.wikipedia.org - 232 786 000 views/day -->\\n<div class=\"central-featured-lang lang2\" lang=\"ja\" dir=\"ltr\">\\n<a id=\"js-link-box-ja\" href=\"//ja.wikipedia.org/\" title=\"Nihongo \\xe2\\x80\\x94 \\xe3\\x82\\xa6\\xe3\\x82\\xa3\\xe3\\x82\\xad\\xe3\\x83\\x9a\\xe3\\x83\\x87\\xe3\\x82\\xa3\\xe3\\x82\\xa2 \\xe2\\x80\\x94 \\xe3\\x83\\x95\\xe3\\x83\\xaa\\xe3\\x83\\xbc\\xe7\\x99\\xbe\\xe7\\xa7\\x91\\xe4\\xba\\x8b\\xe5\\x85\\xb8\" class=\"link-box\" data-slogan=\"\\xe3\\x83\\x95\\xe3\\x83\\xaa\\xe3\\x83\\xbc\\xe7\\x99\\xbe\\xe7\\xa7\\x91\\xe4\\xba\\x8b\\xe5\\x85\\xb8\">\\n<strong>\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe8\\xaa\\x9e</strong>\\n<small><bdi dir=\"ltr\">1&nbsp;314&nbsp;000+</bdi> <span>\\xe8\\xa8\\x98\\xe4\\xba\\x8b</span></small>\\n</a>\\n</div>\\n<!-- #3. ru.wikipedia.org - 179 012 000 views/day -->\\n<div class=\"central-featured-lang lang3\" lang=\"ru\" dir=\"ltr\">\\n<a id=\"js-link-box-ru\" href=\"//ru.wikipedia.org/\" title=\"Russkiy \\xe2\\x80\\x94 \\xd0\\x92\\xd0\\xb8\\xd0\\xba\\xd0\\xb8\\xd0\\xbf\\xd0\\xb5\\xd0\\xb4\\xd0\\xb8\\xd1\\x8f \\xe2\\x80\\x94 \\xd0\\xa1\\xd0\\xb2\\xd0\\xbe\\xd0\\xb1\\xd0\\xbe\\xd0\\xb4\\xd0\\xbd\\xd0\\xb0\\xd1\\x8f \\xd1\\x8d\\xd0\\xbd\\xd1\\x86\\xd0\\xb8\\xd0\\xba\\xd0\\xbb\\xd0\\xbe\\xd0\\xbf\\xd0\\xb5\\xd0\\xb4\\xd0\\xb8\\xd1\\x8f\" class=\"link-box\" data-slogan=\"\\xd0\\xa1\\xd0\\xb2\\xd0\\xbe\\xd0\\xb1\\xd0\\xbe\\xd0\\xb4\\xd0\\xbd\\xd0\\xb0\\xd1\\x8f \\xd1\\x8d\\xd0\\xbd\\xd1\\x86\\xd0\\xb8\\xd0\\xba\\xd0\\xbb\\xd0\\xbe\\xd0\\xbf\\xd0\\xb5\\xd0\\xb4\\xd0\\xb8\\xd1\\x8f\">\\n<strong>\\xd0\\xa0\\xd1\\x83\\xd1\\x81\\xd1\\x81\\xd0\\xba\\xd0\\xb8\\xd0\\xb9</strong>\\n<small><bdi dir=\"ltr\">1&nbsp;798&nbsp;000+</bdi> <span>\\xd1\\x81\\xd1\\x82\\xd0\\xb0\\xd1\\x82\\xd0\\xb5\\xd0\\xb9</span></small>\\n</a>\\n</div>\\n<!-- #4. de.wikipedia.org - 176 185 000 views/day -->\\n<div class=\"central-featured-lang lang4\" lang=\"de\" dir=\"ltr\">\\n<a id=\"js-link-box-de\" href=\"//de.wikipedia.org/\" title=\"Deutsch \\xe2\\x80\\x94 Wikipedia \\xe2\\x80\\x94 Die freie Enzyklop\\xc3\\xa4die\" class=\"link-box\" data-slogan=\"Die freie Enzyklop\\xc3\\xa4die\">\\n<strong>Deutsch</strong>\\n<small><bdi dir=\"ltr\">2&nbsp;667&nbsp;000+</bdi> <span>Artikel</span></small>\\n</a>\\n</div>\\n<!-- #5. es.wikipedia.org - 172 935 000 views/day -->\\n<div class=\"central-featured-lang lang5\" lang=\"es\" dir=\"ltr\">\\n<a id=\"js-link-box-es\" href=\"//es.wikipedia.org/\" title=\"Espa\\xc3\\xb1ol \\xe2\\x80\\x94 Wikipedia \\xe2\\x80\\x94 La enciclopedia libre\" class=\"link-box\" data-slogan=\"La enciclopedia libre\">\\n<strong>Espa\\xc3\\xb1ol</strong>\\n<small><bdi dir=\"ltr\">1&nbsp;755&nbsp;000+</bdi> <span>art\\xc3\\xadculos</span></small>\\n</a>\\n</div>\\n<!-- #6. fr.wikipedia.org - 162 045 000 views/day -->\\n<div class=\"central-featured-lang lang6\" lang=\"fr\" dir=\"ltr\">\\n<a id=\"js-link-box-fr\" href=\"//fr.wikipedia.org/\" title=\"fran\\xc3\\xa7ais \\xe2\\x80\\x94 Wikip\\xc3\\xa9dia \\xe2\\x80\\x94 L\\xe2\\x80\\x99encyclop\\xc3\\xa9die libre\" class=\"link-box\" data-slogan=\"L\\xe2\\x80\\x99encyclop\\xc3\\xa9die libre\">\\n<strong>Fran\\xc3\\xa7ais</strong>\\n<small><bdi dir=\"ltr\">2&nbsp;400&nbsp;000+</bdi> <span>articles</span></small>\\n</a>\\n</div>\\n<!-- #7. it.wikipedia.org - 95 412 000 views/day -->\\n<div class=\"central-featured-lang lang7\" lang=\"it\" dir=\"ltr\">\\n<a id=\"js-link-box-it\" href=\"//it.wikipedia.org/\" title=\"Italiano \\xe2\\x80\\x94 Wikipedia \\xe2\\x80\\x94 L&#x27;enciclopedia libera\" class=\"link-box\" data-slogan=\"L&#x27;enciclopedia libera\">\\n<strong>Italiano</strong>\\n<small><bdi dir=\"ltr\">1&nbsp;742&nbsp;000+</bdi> <span>voci</span></small>\\n</a>\\n</div>\\n<!-- #8. zh.wikipedia.org - 87 143 000 views/day -->\\n<div class=\"central-featured-lang lang8\" lang=\"zh\" dir=\"ltr\">\\n<a id=\"js-link-box-zh\" href=\"//zh.wikipedia.org/\" title=\"Zh\\xc5\\x8dngw\\xc3\\xa9n \\xe2\\x80\\x94 \\xe7\\xbb\\xb4\\xe5\\x9f\\xba\\xe7\\x99\\xbe\\xe7\\xa7\\x91 / \\xe7\\xb6\\xad\\xe5\\x9f\\xba\\xe7\\x99\\xbe\\xe7\\xa7\\x91 \\xe2\\x80\\x94 \\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe7\\x9a\\x84\\xe7\\x99\\xbe\\xe7\\xa7\\x91\\xe5\\x85\\xa8\\xe4\\xb9\\xa6 / \\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe7\\x9a\\x84\\xe7\\x99\\xbe\\xe7\\xa7\\x91\\xe5\\x85\\xa8\\xe6\\x9b\\xb8\" class=\"link-box jscnconv\" data-title-hans=\"Zh\\xc5\\x8dngw\\xc3\\xa9n \\xe2\\x80\\x94 \\xe7\\xbb\\xb4\\xe5\\x9f\\xba\\xe7\\x99\\xbe\\xe7\\xa7\\x91 \\xe2\\x80\\x94 \\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe7\\x9a\\x84\\xe7\\x99\\xbe\\xe7\\xa7\\x91\\xe5\\x85\\xa8\\xe4\\xb9\\xa6\" data-title-hant=\"Zh\\xc5\\x8dngw\\xc3\\xa9n \\xe2\\x80\\x94 \\xe7\\xb6\\xad\\xe5\\x9f\\xba\\xe7\\x99\\xbe\\xe7\\xa7\\x91 \\xe2\\x80\\x94 \\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe7\\x9a\\x84\\xe7\\x99\\xbe\\xe7\\xa7\\x91\\xe5\\x85\\xa8\\xe6\\x9b\\xb8\" data-slogan=\"\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe7\\x9a\\x84\\xe7\\x99\\xbe\\xe7\\xa7\\x91\\xe5\\x85\\xa8\\xe4\\xb9\\xa6 / \\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe7\\x9a\\x84\\xe7\\x99\\xbe\\xe7\\xa7\\x91\\xe5\\x85\\xa8\\xe6\\x9b\\xb8\">\\n<strong>\\xe4\\xb8\\xad\\xe6\\x96\\x87</strong>\\n<small><bdi dir=\"ltr\">1&nbsp;256&nbsp;000+</bdi> <span data-hans=\"\\xe6\\x9d\\xa1\\xe7\\x9b\\xae\" data-hant=\"\\xe6\\xa2\\x9d\\xe7\\x9b\\xae\" class=\"jscnconv\">\\xe6\\x9d\\xa1\\xe7\\x9b\\xae / \\xe6\\xa2\\x9d\\xe7\\x9b\\xae</span></small>\\n</a>\\n</div>\\n<!-- #9. pt.wikipedia.org - 51 172 000 views/day -->\\n<div class=\"central-featured-lang lang9\" lang=\"pt\" dir=\"ltr\">\\n<a id=\"js-link-box-pt\" href=\"//pt.wikipedia.org/\" title=\"Portugu\\xc3\\xaas \\xe2\\x80\\x94 Wikip\\xc3\\xa9dia \\xe2\\x80\\x94 A enciclop\\xc3\\xa9dia livre\" class=\"link-box\" data-slogan=\"A enciclop\\xc3\\xa9dia livre\">\\n<strong>Portugu\\xc3\\xaas</strong>\\n<small><bdi dir=\"ltr\">1&nbsp;085&nbsp;000+</bdi> <span>artigos</span></small>\\n</a>\\n</div>\\n<!-- #10. pl.wikipedia.org - 42 752 000 views/day -->\\n<div class=\"central-featured-lang lang10\" lang=\"pl\" dir=\"ltr\">\\n<a id=\"js-link-box-pl\" href=\"//pl.wikipedia.org/\" title=\"Polski \\xe2\\x80\\x94 Wikipedia \\xe2\\x80\\x94 Wolna encyklopedia\" class=\"link-box\" data-slogan=\"Wolna encyklopedia\">\\n<strong>Polski</strong>\\n<small><bdi dir=\"ltr\">1&nbsp;512&nbsp;000+</bdi> <span>hase\\xc5\\x82</span></small>\\n</a>\\n</div>\\n</div>\\n<div class=\"search-container\">\\n<form class=\"pure-form\" id=\"search-form\" action=\"//www.wikipedia.org/search-redirect.php\" data-el-section=\"search\">\\n<fieldset>\\n<input type=\"hidden\" name=\"family\" value=\"Wikipedia\">\\n<input type=\"hidden\" id=\"hiddenLanguageInput\" name=\"language\" value=\"en\">\\n<div class=\"search-input\" id=\"search-input\">\\n<label for=\"searchInput\" class=\"screen-reader-text\" data-jsl10n=\"portal.search-input-label\">Search Wikipedia</label>\\n<input id=\"searchInput\" name=\"search\" type=\"search\" size=\"20\" autofocus=\"autofocus\" accesskey=\"F\" dir=\"auto\" autocomplete=\"off\">\\n<div class=\"styled-select no-js\">\\n<div class=\"hide-arrow\">\\n<select id=\"searchLanguage\" name=\"language\">\\n<option value=\"af\" lang=\"af\">Afrikaans</option>\\n<option value=\"pl\" lang=\"pl\">Polski</option>\\n<option value=\"ar\" lang=\"ar\">\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd9\\x8a\\xd8\\xa9</option><!-- Al-\\xca\\xbfArab\\xc4\\xabyah -->\\n<option value=\"ast\" lang=\"ast\">Asturianu</option>\\n<option value=\"az\" lang=\"az\">Az\\xc9\\x99rbaycanca</option>\\n<option value=\"bg\" lang=\"bg\">\\xd0\\x91\\xd1\\x8a\\xd0\\xbb\\xd0\\xb3\\xd0\\xb0\\xd1\\x80\\xd1\\x81\\xd0\\xba\\xd0\\xb8</option><!-- B\\xc7\\x8elgarski -->\\n<option value=\"nan\" lang=\"nan\">B\\xc3\\xa2n-l\\xc3\\xa2m-g\\xc3\\xba / H\\xc5\\x8d-l\\xc3\\xb3-o\\xc4\\x93</option>\\n<option value=\"bn\" lang=\"bn\">\\xe0\\xa6\\xac\\xe0\\xa6\\xbe\\xe0\\xa6\\x82\\xe0\\xa6\\xb2\\xe0\\xa6\\xbe</option><!-- Bangla -->\\n<option value=\"be\" lang=\"be\">\\xd0\\x91\\xd0\\xb5\\xd0\\xbb\\xd0\\xb0\\xd1\\x80\\xd1\\x83\\xd1\\x81\\xd0\\xba\\xd0\\xb0\\xd1\\x8f</option><!-- Belaruskaya -->\\n<option value=\"ca\" lang=\"ca\">Catal\\xc3\\xa0</option>\\n<option value=\"cs\" lang=\"cs\">\\xc4\\x8ce\\xc5\\xa1tina</option><!-- \\xc4\\x8de\\xc5\\xa1tina -->\\n<option value=\"cy\" lang=\"cy\">Cymraeg</option><!-- Cymraeg -->\\n<option value=\"da\" lang=\"da\">Dansk</option>\\n<option value=\"de\" lang=\"de\">Deutsch</option>\\n<option value=\"et\" lang=\"et\">Eesti</option>\\n<option value=\"el\" lang=\"el\">\\xce\\x95\\xce\\xbb\\xce\\xbb\\xce\\xb7\\xce\\xbd\\xce\\xb9\\xce\\xba\\xce\\xac</option><!-- Ell\\xc4\\xabnik\\xc3\\xa1 -->\\n<option value=\"en\" lang=\"en\" selected=selected>English</option><!-- English -->\\n<option value=\"es\" lang=\"es\">Espa\\xc3\\xb1ol</option>\\n<option value=\"eo\" lang=\"eo\">Esperanto</option>\\n<option value=\"eu\" lang=\"eu\">Euskara</option>\\n<option value=\"fa\" lang=\"fa\">\\xd9\\x81\\xd8\\xa7\\xd8\\xb1\\xd8\\xb3\\xdb\\x8c</option><!-- F\\xc4\\x81rsi -->\\n<option value=\"fr\" lang=\"fr\">Fran\\xc3\\xa7ais</option><!-- fran\\xc3\\xa7ais -->\\n<option value=\"gl\" lang=\"gl\">Galego</option>\\n<option value=\"ko\" lang=\"ko\">\\xed\\x95\\x9c\\xea\\xb5\\xad\\xec\\x96\\xb4</option><!-- Hangugeo -->\\n<option value=\"hy\" lang=\"hy\">\\xd5\\x80\\xd5\\xa1\\xd5\\xb5\\xd5\\xa5\\xd6\\x80\\xd5\\xa5\\xd5\\xb6</option><!-- Hayeren -->\\n<option value=\"hi\" lang=\"hi\">\\xe0\\xa4\\xb9\\xe0\\xa4\\xbf\\xe0\\xa4\\xa8\\xe0\\xa5\\x8d\\xe0\\xa4\\xa6\\xe0\\xa5\\x80</option><!-- Hind\\xc4\\xab -->\\n<option value=\"hr\" lang=\"hr\">Hrvatski</option>\\n<option value=\"id\" lang=\"id\">Bahasa Indonesia</option>\\n<option value=\"it\" lang=\"it\">Italiano</option>\\n<option value=\"he\" lang=\"he\">\\xd7\\xa2\\xd7\\x91\\xd7\\xa8\\xd7\\x99\\xd7\\xaa</option><!-- Ivrit -->\\n<option value=\"ka\" lang=\"ka\">\\xe1\\x83\\xa5\\xe1\\x83\\x90\\xe1\\x83\\xa0\\xe1\\x83\\x97\\xe1\\x83\\xa3\\xe1\\x83\\x9a\\xe1\\x83\\x98</option><!-- Kartuli -->\\n<option value=\"la\" lang=\"la\">Latina</option>\\n<option value=\"lv\" lang=\"lv\">Latvie\\xc5\\xa1u</option>\\n<option value=\"lt\" lang=\"lt\">Lietuvi\\xc5\\xb3</option>\\n<option value=\"hu\" lang=\"hu\">Magyar</option>\\n<option value=\"mk\" lang=\"mk\">\\xd0\\x9c\\xd0\\xb0\\xd0\\xba\\xd0\\xb5\\xd0\\xb4\\xd0\\xbe\\xd0\\xbd\\xd1\\x81\\xd0\\xba\\xd0\\xb8</option><!-- Makedonski -->\\n<option value=\"arz\" lang=\"arz\">\\xd9\\x85\\xd8\\xb5\\xd8\\xb1\\xd9\\x89</option><!-- Ma\\xe1\\xb9\\xa3r\\xc4\\xab -->\\n<option value=\"ms\" lang=\"ms\">Bahasa Melayu</option>\\n<option value=\"min\" lang=\"min\">Bahaso Minangkabau</option>\\n<option value=\"my\" lang=\"my\">\\xe1\\x80\\x99\\xe1\\x80\\xbc\\xe1\\x80\\x94\\xe1\\x80\\xba\\xe1\\x80\\x99\\xe1\\x80\\xac\\xe1\\x80\\x98\\xe1\\x80\\xac\\xe1\\x80\\x9e\\xe1\\x80\\xac</option><!-- Myanmarsar -->\\n<option value=\"nl\" lang=\"nl\">Nederlands</option>\\n<option value=\"ja\" lang=\"ja\">\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe8\\xaa\\x9e</option><!-- Nihongo -->\\n<option value=\"no\" lang=\"nb\">Norsk (bokm\\xc3\\xa5l)</option>\\n<option value=\"nn\" lang=\"nn\">Norsk (nynorsk)</option>\\n<option value=\"ce\" lang=\"ce\">\\xd0\\x9d\\xd0\\xbe\\xd1\\x85\\xd1\\x87\\xd0\\xb8\\xd0\\xb9\\xd0\\xbd</option><!-- Nox\\xc3\\xa7iyn -->\\n<option value=\"uz\" lang=\"uz\">O\\xca\\xbbzbekcha / \\xd0\\x8e\\xd0\\xb7\\xd0\\xb1\\xd0\\xb5\\xd0\\xba\\xd1\\x87\\xd0\\xb0</option><!-- O\\xca\\xbbzbekcha -->\\n<option value=\"pt\" lang=\"pt\">Portugu\\xc3\\xaas</option>\\n<option value=\"kk\" lang=\"kk\">\\xd2\\x9a\\xd0\\xb0\\xd0\\xb7\\xd0\\xb0\\xd2\\x9b\\xd1\\x88\\xd0\\xb0 / Qazaq\\xc5\\x9fa / \\xd9\\x82\\xd8\\xa7\\xd8\\xb2\\xd8\\xa7\\xd9\\x82\\xd8\\xb4\\xd8\\xa7</option>\\n<option value=\"ro\" lang=\"ro\">Rom\\xc3\\xa2n\\xc4\\x83</option><!-- Rom\\xc3\\xa2n\\xc4\\x83 -->\\n<option value=\"ru\" lang=\"ru\">\\xd0\\xa0\\xd1\\x83\\xd1\\x81\\xd1\\x81\\xd0\\xba\\xd0\\xb8\\xd0\\xb9</option><!-- Russkiy -->\\n<option value=\"simple\" lang=\"en\">Simple English</option>\\n<option value=\"ceb\" lang=\"ceb\">Sinugboanong Binisaya</option>\\n<option value=\"sk\" lang=\"sk\">Sloven\\xc4\\x8dina</option>\\n<option value=\"sl\" lang=\"sl\">Sloven\\xc5\\xa1\\xc4\\x8dina</option>\\n<option value=\"sr\" lang=\"sr\">\\xd0\\xa1\\xd1\\x80\\xd0\\xbf\\xd1\\x81\\xd0\\xba\\xd0\\xb8 / Srpski</option>\\n<option value=\"sh\" lang=\"sh\">Srpskohrvatski / \\xd0\\xa1\\xd1\\x80\\xd0\\xbf\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd1\\x85\\xd1\\x80\\xd0\\xb2\\xd0\\xb0\\xd1\\x82\\xd1\\x81\\xd0\\xba\\xd0\\xb8</option>\\n<option value=\"fi\" lang=\"fi\">Suomi</option><!-- suomi -->\\n<option value=\"sv\" lang=\"sv\">Svenska</option>\\n<option value=\"ta\" lang=\"ta\">\\xe0\\xae\\xa4\\xe0\\xae\\xae\\xe0\\xae\\xbf\\xe0\\xae\\xb4\\xe0\\xaf\\x8d</option><!-- Tami\\xe1\\xb8\\xbb -->\\n<option value=\"tt\" lang=\"tt\">\\xd0\\xa2\\xd0\\xb0\\xd1\\x82\\xd0\\xb0\\xd1\\x80\\xd1\\x87\\xd0\\xb0 / Tatar\\xc3\\xa7a</option>\\n<option value=\"th\" lang=\"th\">\\xe0\\xb8\\xa0\\xe0\\xb8\\xb2\\xe0\\xb8\\xa9\\xe0\\xb8\\xb2\\xe0\\xb9\\x84\\xe0\\xb8\\x97\\xe0\\xb8\\xa2</option><!-- Phasa Thai -->\\n<option value=\"tg\" lang=\"tg\">\\xd0\\xa2\\xd0\\xbe\\xd2\\xb7\\xd0\\xb8\\xd0\\xba\\xd3\\xa3</option><!-- Tojik\\xc4\\xab -->\\n<option value=\"azb\" lang=\"azb\">\\xd8\\xaa\\xdb\\x86\\xd8\\xb1\\xda\\xa9\\xd8\\xac\\xd9\\x87</option><!-- T\\xc3\\xbcrkce -->\\n<option value=\"tr\" lang=\"tr\">T\\xc3\\xbcrk\\xc3\\xa7e</option><!-- T\\xc3\\xbcrk\\xc3\\xa7e -->\\n<option value=\"uk\" lang=\"uk\">\\xd0\\xa3\\xd0\\xba\\xd1\\x80\\xd0\\xb0\\xd1\\x97\\xd0\\xbd\\xd1\\x81\\xd1\\x8c\\xd0\\xba\\xd0\\xb0</option><!-- Ukrayins\\xe2\\x80\\x99ka -->\\n<option value=\"ur\" lang=\"ur\">\\xd8\\xa7\\xd8\\xb1\\xd8\\xaf\\xd9\\x88</option><!-- Urdu -->\\n<option value=\"vi\" lang=\"vi\">Ti\\xe1\\xba\\xbfng Vi\\xe1\\xbb\\x87t</option>\\n<option value=\"vo\" lang=\"vo\">Volap\\xc3\\xbck</option>\\n<option value=\"war\" lang=\"war\">Winaray</option>\\n<option value=\"zh-yue\" lang=\"yue\">\\xe7\\xb2\\xb5\\xe8\\xaa\\x9e</option><!-- Yuht Y\\xc3\\xbah / Jyut6 jyu5 -->\\n<option value=\"zh\" lang=\"zh\">\\xe4\\xb8\\xad\\xe6\\x96\\x87</option><!-- Zh\\xc5\\x8dngw\\xc3\\xa9n -->\\n</select>\\n<div class=\"styled-select-active-helper\"></div>\\n</div>\\n<i class=\"sprite svg-arrow-down\"></i>\\n</div>\\n</div>\\n<button class=\"pure-button pure-button-primary-progressive\" type=\"submit\">\\n<i class=\"sprite svg-search-icon\" data-jsl10n=\"search-input-button\">Search</i>\\n</button>\\n<input type=\"hidden\" value=\"Go\" name=\"go\">\\n</fieldset>\\n</form>\\n</div>\\n<div class=\"lang-list-button-wrapper\">\\n<button id=\"js-lang-list-button\" class=\"lang-list-button\">\\n<i class=\"sprite svg-language-icon\"></i>\\n<span class=\"lang-list-button-text jsl10n\" data-jsl10n=\"portal.language-button-text\">Read Wikipedia in your language </span>\\n<i class=\"sprite svg-arrow-down-blue\"></i>\\n</button>\\n</div>\\n<div class=\"lang-list-border\"></div>\\n<div class=\"lang-list-container\">\\n<div id=\"js-lang-lists\" class=\"lang-list-content\">\\n<h2 class=\"bookshelf-container\">\\n<span class=\"bookshelf\">\\n<span class=\"text\">\\n<bdi dir=\"ltr\">\\n1&nbsp;000&nbsp;000+\\n</bdi>\\n<span class=\"jsl10n\" data-jsl10n=\"entries\">\\narticles\\n</span>\\n</span>\\n</span>\\n</h2>\\n<div class=\"langlist langlist-large hlist\" data-el-section=\"secondary links\">\\n<ul>\\n<li><a href=\"//pl.wikipedia.org/\" lang=\"pl\">Polski</a></li>\\n<li><a href=\"//ar.wikipedia.org/\" lang=\"ar\" title=\"Al-\\xca\\xbfArab\\xc4\\xabyah\"><bdi dir=\"rtl\">\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd9\\x8a\\xd8\\xa9</bdi></a></li>\\n<li><a href=\"//de.wikipedia.org/\" lang=\"de\">Deutsch</a></li>\\n<li><a href=\"//en.wikipedia.org/\" lang=\"en\" title=\"English\">English</a></li>\\n<li><a href=\"//es.wikipedia.org/\" lang=\"es\">Espa\\xc3\\xb1ol</a></li>\\n<li><a href=\"//fr.wikipedia.org/\" lang=\"fr\" title=\"fran\\xc3\\xa7ais\">Fran\\xc3\\xa7ais</a></li>\\n<li><a href=\"//it.wikipedia.org/\" lang=\"it\">Italiano</a></li>\\n<li><a href=\"//arz.wikipedia.org/\" lang=\"arz\" title=\"Ma\\xe1\\xb9\\xa3r\\xc4\\xab\"><bdi dir=\"rtl\">\\xd9\\x85\\xd8\\xb5\\xd8\\xb1\\xd9\\x89</bdi></a></li>\\n<li><a href=\"//nl.wikipedia.org/\" lang=\"nl\">Nederlands</a></li>\\n<li><a href=\"//ja.wikipedia.org/\" lang=\"ja\" title=\"Nihongo\">\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe8\\xaa\\x9e</a></li>\\n<li><a href=\"//pt.wikipedia.org/\" lang=\"pt\">Portugu\\xc3\\xaas</a></li>\\n<li><a href=\"//ru.wikipedia.org/\" lang=\"ru\" title=\"Russkiy\">\\xd0\\xa0\\xd1\\x83\\xd1\\x81\\xd1\\x81\\xd0\\xba\\xd0\\xb8\\xd0\\xb9</a></li>\\n<li><a href=\"//ceb.wikipedia.org/\" lang=\"ceb\">Sinugboanong Binisaya</a></li>\\n<li><a href=\"//sv.wikipedia.org/\" lang=\"sv\">Svenska</a></li>\\n<li><a href=\"//uk.wikipedia.org/\" lang=\"uk\" title=\"Ukrayins\\xe2\\x80\\x99ka\">\\xd0\\xa3\\xd0\\xba\\xd1\\x80\\xd0\\xb0\\xd1\\x97\\xd0\\xbd\\xd1\\x81\\xd1\\x8c\\xd0\\xba\\xd0\\xb0</a></li>\\n<li><a href=\"//vi.wikipedia.org/\" lang=\"vi\">Ti\\xe1\\xba\\xbfng Vi\\xe1\\xbb\\x87t</a></li>\\n<li><a href=\"//war.wikipedia.org/\" lang=\"war\">Winaray</a></li>\\n<li><a href=\"//zh.wikipedia.org/\" lang=\"zh\" title=\"Zh\\xc5\\x8dngw\\xc3\\xa9n\">\\xe4\\xb8\\xad\\xe6\\x96\\x87</a></li>\\n</ul>\\n</div>\\n<h2 class=\"bookshelf-container\">\\n<span class=\"bookshelf\">\\n<span class=\"text\">\\n<bdi dir=\"ltr\">\\n100&nbsp;000+\\n</bdi>\\n<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\\narticles\\n</span>\\n</span>\\n</span>\\n</h2>\\n<div class=\"langlist langlist-large hlist\" data-el-section=\"secondary links\">\\n<ul>\\n<li><a href=\"//af.wikipedia.org/\" lang=\"af\">Afrikaans</a></li>\\n<li><a href=\"//ast.wikipedia.org/\" lang=\"ast\">Asturianu</a></li>\\n<li><a href=\"//az.wikipedia.org/\" lang=\"az\">Az\\xc9\\x99rbaycanca</a></li>\\n<li><a href=\"//bg.wikipedia.org/\" lang=\"bg\" title=\"B\\xc7\\x8elgarski\">\\xd0\\x91\\xd1\\x8a\\xd0\\xbb\\xd0\\xb3\\xd0\\xb0\\xd1\\x80\\xd1\\x81\\xd0\\xba\\xd0\\xb8</a></li>\\n<li><a href=\"//zh-min-nan.wikipedia.org/\" lang=\"nan\">B\\xc3\\xa2n-l\\xc3\\xa2m-g\\xc3\\xba / H\\xc5\\x8d-l\\xc3\\xb3-o\\xc4\\x93</a></li>\\n<li><a href=\"//bn.wikipedia.org/\" lang=\"bn\" title=\"Bangla\">\\xe0\\xa6\\xac\\xe0\\xa6\\xbe\\xe0\\xa6\\x82\\xe0\\xa6\\xb2\\xe0\\xa6\\xbe</a></li>\\n<li><a href=\"//be.wikipedia.org/\" lang=\"be\" title=\"Belaruskaya\">\\xd0\\x91\\xd0\\xb5\\xd0\\xbb\\xd0\\xb0\\xd1\\x80\\xd1\\x83\\xd1\\x81\\xd0\\xba\\xd0\\xb0\\xd1\\x8f</a></li>\\n<li><a href=\"//ca.wikipedia.org/\" lang=\"ca\">Catal\\xc3\\xa0</a></li>\\n<li><a href=\"//cs.wikipedia.org/\" lang=\"cs\" title=\"\\xc4\\x8de\\xc5\\xa1tina\">\\xc4\\x8ce\\xc5\\xa1tina</a></li>\\n<li><a href=\"//cy.wikipedia.org/\" lang=\"cy\" title=\"Cymraeg\">Cymraeg</a></li>\\n<li><a href=\"//da.wikipedia.org/\" lang=\"da\">Dansk</a></li>\\n<li><a href=\"//et.wikipedia.org/\" lang=\"et\">Eesti</a></li>\\n<li><a href=\"//el.wikipedia.org/\" lang=\"el\" title=\"Ell\\xc4\\xabnik\\xc3\\xa1\">\\xce\\x95\\xce\\xbb\\xce\\xbb\\xce\\xb7\\xce\\xbd\\xce\\xb9\\xce\\xba\\xce\\xac</a></li>\\n<li><a href=\"//eo.wikipedia.org/\" lang=\"eo\">Esperanto</a></li>\\n<li><a href=\"//eu.wikipedia.org/\" lang=\"eu\">Euskara</a></li>\\n<li><a href=\"//fa.wikipedia.org/\" lang=\"fa\" title=\"F\\xc4\\x81rsi\"><bdi dir=\"rtl\">\\xd9\\x81\\xd8\\xa7\\xd8\\xb1\\xd8\\xb3\\xdb\\x8c</bdi></a></li>\\n<li><a href=\"//gl.wikipedia.org/\" lang=\"gl\">Galego</a></li>\\n<li><a href=\"//ko.wikipedia.org/\" lang=\"ko\" title=\"Hangugeo\">\\xed\\x95\\x9c\\xea\\xb5\\xad\\xec\\x96\\xb4</a></li>\\n<li><a href=\"//hy.wikipedia.org/\" lang=\"hy\" title=\"Hayeren\">\\xd5\\x80\\xd5\\xa1\\xd5\\xb5\\xd5\\xa5\\xd6\\x80\\xd5\\xa5\\xd5\\xb6</a></li>\\n<li><a href=\"//hi.wikipedia.org/\" lang=\"hi\" title=\"Hind\\xc4\\xab\">\\xe0\\xa4\\xb9\\xe0\\xa4\\xbf\\xe0\\xa4\\xa8\\xe0\\xa5\\x8d\\xe0\\xa4\\xa6\\xe0\\xa5\\x80</a></li>\\n<li><a href=\"//hr.wikipedia.org/\" lang=\"hr\">Hrvatski</a></li>\\n<li><a href=\"//id.wikipedia.org/\" lang=\"id\">Bahasa Indonesia</a></li>\\n<li><a href=\"//he.wikipedia.org/\" lang=\"he\" title=\"Ivrit\"><bdi dir=\"rtl\">\\xd7\\xa2\\xd7\\x91\\xd7\\xa8\\xd7\\x99\\xd7\\xaa</bdi></a></li>\\n<li><a href=\"//ka.wikipedia.org/\" lang=\"ka\" title=\"Kartuli\">\\xe1\\x83\\xa5\\xe1\\x83\\x90\\xe1\\x83\\xa0\\xe1\\x83\\x97\\xe1\\x83\\xa3\\xe1\\x83\\x9a\\xe1\\x83\\x98</a></li>\\n<li><a href=\"//la.wikipedia.org/\" lang=\"la\">Latina</a></li>\\n<li><a href=\"//lv.wikipedia.org/\" lang=\"lv\">Latvie\\xc5\\xa1u</a></li>\\n<li><a href=\"//lt.wikipedia.org/\" lang=\"lt\">Lietuvi\\xc5\\xb3</a></li>\\n<li><a href=\"//hu.wikipedia.org/\" lang=\"hu\">Magyar</a></li>\\n<li><a href=\"//mk.wikipedia.org/\" lang=\"mk\" title=\"Makedonski\">\\xd0\\x9c\\xd0\\xb0\\xd0\\xba\\xd0\\xb5\\xd0\\xb4\\xd0\\xbe\\xd0\\xbd\\xd1\\x81\\xd0\\xba\\xd0\\xb8</a></li>\\n<li><a href=\"//ms.wikipedia.org/\" lang=\"ms\">Bahasa Melayu</a></li>\\n<li><a href=\"//min.wikipedia.org/\" lang=\"min\">Bahaso Minangkabau</a></li>\\n<li><a href=\"//my.wikipedia.org/\" lang=\"my\" title=\"Myanmarsar\">\\xe1\\x80\\x99\\xe1\\x80\\xbc\\xe1\\x80\\x94\\xe1\\x80\\xba\\xe1\\x80\\x99\\xe1\\x80\\xac\\xe1\\x80\\x98\\xe1\\x80\\xac\\xe1\\x80\\x9e\\xe1\\x80\\xac</a></li>\\n<li lang=\"no\">Norsk<ul><li><a href=\"//no.wikipedia.org/\" lang=\"nb\">bokm\\xc3\\xa5l</a></li><li><a href=\"//nn.wikipedia.org/\" lang=\"nn\">nynorsk</a></li></ul></li>\\n<li><a href=\"//ce.wikipedia.org/\" lang=\"ce\" title=\"Nox\\xc3\\xa7iyn\">\\xd0\\x9d\\xd0\\xbe\\xd1\\x85\\xd1\\x87\\xd0\\xb8\\xd0\\xb9\\xd0\\xbd</a></li>\\n<li><a href=\"//uz.wikipedia.org/\" lang=\"uz\" title=\"O\\xca\\xbbzbekcha\">O\\xca\\xbbzbekcha / \\xd0\\x8e\\xd0\\xb7\\xd0\\xb1\\xd0\\xb5\\xd0\\xba\\xd1\\x87\\xd0\\xb0</a></li>\\n<li><a href=\"//kk.wikipedia.org/\" lang=\"kk\"><span lang=\"kk-Cyrl\">\\xd2\\x9a\\xd0\\xb0\\xd0\\xb7\\xd0\\xb0\\xd2\\x9b\\xd1\\x88\\xd0\\xb0</span> / <span lang=\"kk-Latn\">Qazaq\\xc5\\x9fa</span> / <bdi lang=\"kk-Arab\" dir=\"rtl\">\\xd9\\x82\\xd8\\xa7\\xd8\\xb2\\xd8\\xa7\\xd9\\x82\\xd8\\xb4\\xd8\\xa7</bdi></a></li>\\n<li><a href=\"//ro.wikipedia.org/\" lang=\"ro\" title=\"Rom\\xc3\\xa2n\\xc4\\x83\">Rom\\xc3\\xa2n\\xc4\\x83</a></li>\\n<li><a href=\"//simple.wikipedia.org/\" lang=\"en\">Simple English</a></li>\\n<li><a href=\"//sk.wikipedia.org/\" lang=\"sk\">Sloven\\xc4\\x8dina</a></li>\\n<li><a href=\"//sl.wikipedia.org/\" lang=\"sl\">Sloven\\xc5\\xa1\\xc4\\x8dina</a></li>\\n<li><a href=\"//sr.wikipedia.org/\" lang=\"sr\">\\xd0\\xa1\\xd1\\x80\\xd0\\xbf\\xd1\\x81\\xd0\\xba\\xd0\\xb8 / Srpski</a></li>\\n<li><a href=\"//sh.wikipedia.org/\" lang=\"sh\">Srpskohrvatski / \\xd0\\xa1\\xd1\\x80\\xd0\\xbf\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd1\\x85\\xd1\\x80\\xd0\\xb2\\xd0\\xb0\\xd1\\x82\\xd1\\x81\\xd0\\xba\\xd0\\xb8</a></li>\\n<li><a href=\"//fi.wikipedia.org/\" lang=\"fi\" title=\"suomi\">Suomi</a></li>\\n<li><a href=\"//ta.wikipedia.org/\" lang=\"ta\" title=\"Tami\\xe1\\xb8\\xbb\">\\xe0\\xae\\xa4\\xe0\\xae\\xae\\xe0\\xae\\xbf\\xe0\\xae\\xb4\\xe0\\xaf\\x8d</a></li>\\n<li><a href=\"//tt.wikipedia.org/\" lang=\"tt\">\\xd0\\xa2\\xd0\\xb0\\xd1\\x82\\xd0\\xb0\\xd1\\x80\\xd1\\x87\\xd0\\xb0 / Tatar\\xc3\\xa7a</a></li>\\n<li><a href=\"//th.wikipedia.org/\" lang=\"th\" title=\"Phasa Thai\">\\xe0\\xb8\\xa0\\xe0\\xb8\\xb2\\xe0\\xb8\\xa9\\xe0\\xb8\\xb2\\xe0\\xb9\\x84\\xe0\\xb8\\x97\\xe0\\xb8\\xa2</a></li>\\n<li><a href=\"//tg.wikipedia.org/\" lang=\"tg\" title=\"Tojik\\xc4\\xab\">\\xd0\\xa2\\xd0\\xbe\\xd2\\xb7\\xd0\\xb8\\xd0\\xba\\xd3\\xa3</a></li>\\n<li><a href=\"//azb.wikipedia.org/\" lang=\"azb\" title=\"T\\xc3\\xbcrkce\"><bdi dir=\"rtl\">\\xd8\\xaa\\xdb\\x86\\xd8\\xb1\\xda\\xa9\\xd8\\xac\\xd9\\x87</bdi></a></li>\\n<li><a href=\"//tr.wikipedia.org/\" lang=\"tr\" title=\"T\\xc3\\xbcrk\\xc3\\xa7e\">T\\xc3\\xbcrk\\xc3\\xa7e</a></li>\\n<li><a href=\"//ur.wikipedia.org/\" lang=\"ur\" title=\"Urdu\"><bdi dir=\"rtl\">\\xd8\\xa7\\xd8\\xb1\\xd8\\xaf\\xd9\\x88</bdi></a></li>\\n<li><a href=\"//vo.wikipedia.org/\" lang=\"vo\">Volap\\xc3\\xbck</a></li>\\n<li><a href=\"//zh-yue.wikipedia.org/\" lang=\"yue\" title=\"Yuht Y\\xc3\\xbah / Jyut6 jyu5\">\\xe7\\xb2\\xb5\\xe8\\xaa\\x9e</a></li>\\n</ul>\\n</div>\\n<h2 class=\"bookshelf-container\">\\n<span class=\"bookshelf\">\\n<span class=\"text\">\\n<bdi dir=\"ltr\">\\n10&nbsp;000+\\n</bdi>\\n<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\\narticles\\n</span>\\n</span>\\n</span>\\n</h2>\\n<div class=\"langlist hlist\" data-el-section=\"secondary links\">\\n<ul>\\n<li><a href=\"//ace.wikipedia.org/\" lang=\"ace\">Bahsa Ac\\xc3\\xa8h</a></li>\\n<li><a href=\"//als.wikipedia.org/\" lang=\"gsw\">Alemannisch</a></li>\\n<li><a href=\"//am.wikipedia.org/\" lang=\"am\" title=\"\\xc4\\x80mari\\xc3\\xb1\\xc3\\xb1\\xc4\\x81\">\\xe1\\x8a\\xa0\\xe1\\x88\\x9b\\xe1\\x88\\xad\\xe1\\x8a\\x9b</a></li>\\n<li><a href=\"//an.wikipedia.org/\" lang=\"an\">Aragon\\xc3\\xa9s</a></li>\\n<li><a href=\"//ban.wikipedia.org/\" lang=\"ban\" title=\"Basa Bali\">Basa Bali</a></li>\\n<li><a href=\"//map-bms.wikipedia.org/\" lang=\"map-x-bms\">Basa Banyumasan</a></li>\\n<li><a href=\"//ba.wikipedia.org/\" lang=\"ba\" title=\"Ba\\xc5\\x9fqortsa\">\\xd0\\x91\\xd0\\xb0\\xd1\\x88\\xd2\\xa1\\xd0\\xbe\\xd1\\x80\\xd1\\x82\\xd1\\x81\\xd0\\xb0</a></li>\\n<li><a href=\"//be-tarask.wikipedia.org/\" lang=\"be\" title=\"Belaruskaya (Tara\\xc5\\xa1kievica)\">\\xd0\\x91\\xd0\\xb5\\xd0\\xbb\\xd0\\xb0\\xd1\\x80\\xd1\\x83\\xd1\\x81\\xd0\\xba\\xd0\\xb0\\xd1\\x8f (\\xd0\\xa2\\xd0\\xb0\\xd1\\x80\\xd0\\xb0\\xd1\\x88\\xd0\\xba\\xd0\\xb5\\xd0\\xb2\\xd1\\x96\\xd1\\x86\\xd0\\xb0)</a></li>\\n<li><a href=\"//bcl.wikipedia.org/\" lang=\"bcl\">Bikol Central</a></li>\\n<li><a href=\"//bpy.wikipedia.org/\" lang=\"bpy\" title=\"Bishnupriya Manipuri\">\\xe0\\xa6\\xac\\xe0\\xa6\\xbf\\xe0\\xa6\\xb7\\xe0\\xa7\\x8d\\xe0\\xa6\\xa3\\xe0\\xa7\\x81\\xe0\\xa6\\xaa\\xe0\\xa7\\x8d\\xe0\\xa6\\xb0\\xe0\\xa6\\xbf\\xe0\\xa6\\xaf\\xe0\\xa6\\xbc\\xe0\\xa6\\xbe \\xe0\\xa6\\xae\\xe0\\xa6\\xa3\\xe0\\xa6\\xbf\\xe0\\xa6\\xaa\\xe0\\xa7\\x81\\xe0\\xa6\\xb0\\xe0\\xa7\\x80</a></li>\\n<li><a href=\"//bar.wikipedia.org/\" lang=\"bar\">Boarisch</a></li>\\n<li><a href=\"//bs.wikipedia.org/\" lang=\"bs\">Bosanski</a></li>\\n<li><a href=\"//br.wikipedia.org/\" lang=\"br\">Brezhoneg</a></li>\\n<li><a href=\"//cv.wikipedia.org/\" lang=\"cv\" title=\"\\xc4\\x8c\\xc4\\x83va\\xc5\\xa1la\">\\xd0\\xa7\\xd3\\x91\\xd0\\xb2\\xd0\\xb0\\xd1\\x88\\xd0\\xbb\\xd0\\xb0</a></li>\\n<li><a href=\"//nv.wikipedia.org/\" lang=\"nv\">Din\\xc3\\xa9 Bizaad</a></li>\\n<li><a href=\"//eml.wikipedia.org/\" lang=\"roa-x-eml\">Emigli\\xc3\\xa0n\\xe2\\x80\\x93Rumagn\\xc3\\xb2l</a></li>\\n<li><a href=\"//hif.wikipedia.org/\" lang=\"hif\">Fiji Hindi</a></li>\\n<li><a href=\"//fo.wikipedia.org/\" lang=\"fo\">F\\xc3\\xb8royskt</a></li>\\n<li><a href=\"//fy.wikipedia.org/\" lang=\"fy\">Frysk</a></li>\\n<li><a href=\"//ga.wikipedia.org/\" lang=\"ga\">Gaeilge</a></li>\\n<li><a href=\"//gd.wikipedia.org/\" lang=\"gd\">G\\xc3\\xa0idhlig</a></li>\\n<li><a href=\"//gu.wikipedia.org/\" lang=\"gu\" title=\"Gujarati\">\\xe0\\xaa\\x97\\xe0\\xab\\x81\\xe0\\xaa\\x9c\\xe0\\xaa\\xb0\\xe0\\xaa\\xbe\\xe0\\xaa\\xa4\\xe0\\xab\\x80</a></li>\\n<li><a href=\"//ha.wikipedia.org/\" lang=\"ha\" title=\"Hausa\">Hausa</a></li>\\n<li><a href=\"//hsb.wikipedia.org/\" lang=\"hsb\">Hornjoserbsce</a></li>\\n<li><a href=\"//io.wikipedia.org/\" lang=\"io\" title=\"Ido\">Ido</a></li>\\n<li><a href=\"//ilo.wikipedia.org/\" lang=\"ilo\">Ilokano</a></li>\\n<li><a href=\"//ia.wikipedia.org/\" lang=\"ia\">Interlingua</a></li>\\n<li><a href=\"//os.wikipedia.org/\" lang=\"os\" title=\"Iron\">\\xd0\\x98\\xd1\\x80\\xd0\\xbe\\xd0\\xbd</a></li>\\n<li><a href=\"//is.wikipedia.org/\" lang=\"is\">\\xc3\\x8dslenska</a></li>\\n<li><a href=\"//jv.wikipedia.org/\" lang=\"jv\" title=\"Jawa\">Jawa</a></li>\\n<li><a href=\"//kn.wikipedia.org/\" lang=\"kn\" title=\"Kannada\">\\xe0\\xb2\\x95\\xe0\\xb2\\xa8\\xe0\\xb3\\x8d\\xe0\\xb2\\xa8\\xe0\\xb2\\xa1</a></li>\\n<li><a href=\"//ht.wikipedia.org/\" lang=\"ht\">Krey\\xc3\\xb2l Ayisyen</a></li>\\n<li><a href=\"//ku.wikipedia.org/\" lang=\"ku\"><span lang=\"ku-Latn\">Kurd\\xc3\\xae</span> / <bdi lang=\"ku-Arab\" dir=\"rtl\">\\xd9\\x83\\xd9\\x88\\xd8\\xb1\\xd8\\xaf\\xdb\\x8c</bdi></a></li>\\n<li><a href=\"//ckb.wikipedia.org/\" lang=\"ckb\" title=\"Kurd\\xc3\\xaey Nawend\\xc3\\xae\"><bdi dir=\"rtl\">\\xda\\xa9\\xd9\\x88\\xd8\\xb1\\xd8\\xaf\\xdb\\x8c\\xdb\\x8c \\xd9\\x86\\xd8\\xa7\\xd9\\x88\\xdb\\x95\\xd9\\x86\\xd8\\xaf\\xdb\\x8c</bdi></a></li>\\n<li><a href=\"//ky.wikipedia.org/\" lang=\"ky\" title=\"Kyrgyz\\xc4\\x8da\">\\xd0\\x9a\\xd1\\x8b\\xd1\\x80\\xd0\\xb3\\xd1\\x8b\\xd0\\xb7\\xd1\\x87\\xd0\\xb0</a></li>\\n<li><a href=\"//mrj.wikipedia.org/\" lang=\"mjr\" title=\"Kyryk Mary\">\\xd0\\x9a\\xd1\\x8b\\xd1\\x80\\xd1\\x8b\\xd0\\xba \\xd0\\xbc\\xd0\\xb0\\xd1\\x80\\xd1\\x8b</a></li>\\n<li><a href=\"//lb.wikipedia.org/\" lang=\"lb\">L\\xc3\\xabtzebuergesch</a></li>\\n<li><a href=\"//lij.wikipedia.org/\" lang=\"lij\">L\\xc3\\xacgure</a></li>\\n<li><a href=\"//li.wikipedia.org/\" lang=\"li\">Limburgs</a></li>\\n<li><a href=\"//lmo.wikipedia.org/\" lang=\"lmo\">Lombard</a></li>\\n<li><a href=\"//mai.wikipedia.org/\" lang=\"mai\" title=\"Maithil\\xc4\\xab\">\\xe0\\xa4\\xae\\xe0\\xa5\\x88\\xe0\\xa4\\xa5\\xe0\\xa4\\xbf\\xe0\\xa4\\xb2\\xe0\\xa5\\x80</a></li>\\n<li><a href=\"//mg.wikipedia.org/\" lang=\"mg\">Malagasy</a></li>\\n<li><a href=\"//ml.wikipedia.org/\" lang=\"ml\" title=\"Malayalam\">\\xe0\\xb4\\xae\\xe0\\xb4\\xb2\\xe0\\xb4\\xaf\\xe0\\xb4\\xbe\\xe0\\xb4\\xb3\\xe0\\xb4\\x82</a></li>\\n<li><a href=\"//zh-classical.wikipedia.org/\" lang=\"lzh\" title=\"Man4jin4 / W\\xc3\\xa9ny\\xc3\\xa1n\">\\xe6\\x96\\x87\\xe8\\xa8\\x80</a></li>\\n<li><a href=\"//mr.wikipedia.org/\" lang=\"mr\" title=\"Marathi\">\\xe0\\xa4\\xae\\xe0\\xa4\\xb0\\xe0\\xa4\\xbe\\xe0\\xa4\\xa0\\xe0\\xa5\\x80</a></li>\\n<li><a href=\"//xmf.wikipedia.org/\" lang=\"xmf\" title=\"Margaluri\">\\xe1\\x83\\x9b\\xe1\\x83\\x90\\xe1\\x83\\xa0\\xe1\\x83\\x92\\xe1\\x83\\x90\\xe1\\x83\\x9a\\xe1\\x83\\xa3\\xe1\\x83\\xa0\\xe1\\x83\\x98</a></li>\\n<li><a href=\"//mzn.wikipedia.org/\" lang=\"mzn\" title=\"M\\xc3\\xa4zeruni\"><bdi dir=\"rtl\">\\xd9\\x85\\xd8\\xa7\\xd8\\xb2\\xd9\\x90\\xd8\\xb1\\xd9\\x88\\xd9\\x86\\xdb\\x8c</bdi></a></li>\\n<li><a href=\"//cdo.wikipedia.org/\" lang=\"cdo\" title=\"Ming-deng-ngu\">M\\xc3\\xacng-d\\xc4\\x95\\xcc\\xa4ng-ng\\xe1\\xb9\\xb3\\xcc\\x84 / \\xe9\\x96\\xa9\\xe6\\x9d\\xb1\\xe8\\xaa\\x9e</a></li>\\n<li><a href=\"//mn.wikipedia.org/\" lang=\"mn\" title=\"Mongol\">\\xd0\\x9c\\xd0\\xbe\\xd0\\xbd\\xd0\\xb3\\xd0\\xbe\\xd0\\xbb</a></li>\\n<li><a href=\"//nap.wikipedia.org/\" lang=\"nap\">Napulitano</a></li>\\n<li><a href=\"//new.wikipedia.org/\" lang=\"new\" title=\"Nepal Bhasa\">\\xe0\\xa4\\xa8\\xe0\\xa5\\x87\\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2 \\xe0\\xa4\\xad\\xe0\\xa4\\xbe\\xe0\\xa4\\xb7\\xe0\\xa4\\xbe</a></li>\\n<li><a href=\"//ne.wikipedia.org/\" lang=\"ne\" title=\"Nep\\xc4\\x81l\\xc4\\xab\">\\xe0\\xa4\\xa8\\xe0\\xa5\\x87\\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\xe0\\xa5\\x80</a></li>\\n<li><a href=\"//frr.wikipedia.org/\" lang=\"frr\">Nordfriisk</a></li>\\n<li><a href=\"//oc.wikipedia.org/\" lang=\"oc\">Occitan</a></li>\\n<li><a href=\"//mhr.wikipedia.org/\" lang=\"mhr\" title=\"Olyk Marij\">\\xd0\\x9e\\xd0\\xbb\\xd1\\x8b\\xd0\\xba \\xd0\\xbc\\xd0\\xb0\\xd1\\x80\\xd0\\xb8\\xd0\\xb9</a></li>\\n<li><a href=\"//or.wikipedia.org/\" lang=\"or\" title=\"O\\xe1\\xb9\\x9bi\\xc4\\x81\">\\xe0\\xac\\x93\\xe0\\xac\\xa1\\xe0\\xac\\xbf\\xe0\\xac\\xbc\\xe0\\xac\\x86</a></li>\\n<li><a href=\"//as.wikipedia.org/\" lang=\"as\" title=\"\\xc3\\x94x\\xc3\\xb4miya\">\\xe0\\xa6\\x85\\xe0\\xa6\\xb8\\xe0\\xa6\\xae\\xe0\\xa7\\x80\\xe0\\xa6\\xaf\\xe0\\xa6\\xbe\\xe0\\xa6\\xbc</a></li>\\n<li><a href=\"//pa.wikipedia.org/\" lang=\"pa\" title=\"Pa\\xc3\\xb1j\\xc4\\x81b\\xc4\\xab (Gurmukh\\xc4\\xab)\">\\xe0\\xa8\\xaa\\xe0\\xa9\\xb0\\xe0\\xa8\\x9c\\xe0\\xa8\\xbe\\xe0\\xa8\\xac\\xe0\\xa9\\x80 (\\xe0\\xa8\\x97\\xe0\\xa9\\x81\\xe0\\xa8\\xb0\\xe0\\xa8\\xae\\xe0\\xa9\\x81\\xe0\\xa8\\x96\\xe0\\xa9\\x80)</a></li>\\n<li><a href=\"//pnb.wikipedia.org/\" lang=\"pnb\" title=\"Pa\\xc3\\xb1j\\xc4\\x81b\\xc4\\xab (Sh\\xc4\\x81hmukh\\xc4\\xab)\"><bdi dir=\"rtl\">\\xd9\\xbe\\xd9\\x86\\xd8\\xac\\xd8\\xa7\\xd8\\xa8\\xdb\\x8c (\\xd8\\xb4\\xd8\\xa7\\xdb\\x81 \\xd9\\x85\\xda\\xa9\\xda\\xbe\\xdb\\x8c)</bdi></a></li>\\n<li><a href=\"//ps.wikipedia.org/\" lang=\"ps\" title=\"Pa\\xca\\x82to\"><bdi dir=\"rtl\">\\xd9\\xbe\\xda\\x9a\\xd8\\xaa\\xd9\\x88</bdi></a></li>\\n<li><a href=\"//pms.wikipedia.org/\" lang=\"pms\">Piemont\\xc3\\xa8is</a></li>\\n<li><a href=\"//nds.wikipedia.org/\" lang=\"nds\">Plattd\\xc3\\xbc\\xc3\\xbctsch</a></li>\\n<li><a href=\"//crh.wikipedia.org/\" lang=\"crh\">Q\\xc4\\xb1r\\xc4\\xb1mtatarca</a></li>\\n<li><a href=\"//qu.wikipedia.org/\" lang=\"qu\">Runa Simi</a></li>\\n<li><a href=\"//sa.wikipedia.org/\" lang=\"sa\" title=\"Sa\\xe1\\xb9\\x83sk\\xe1\\xb9\\x9btam\">\\xe0\\xa4\\xb8\\xe0\\xa4\\x82\\xe0\\xa4\\xb8\\xe0\\xa5\\x8d\\xe0\\xa4\\x95\\xe0\\xa5\\x83\\xe0\\xa4\\xa4\\xe0\\xa4\\xae\\xe0\\xa5\\x8d</a></li>\\n<li><a href=\"//sah.wikipedia.org/\" lang=\"sah\" title=\"Saxa Tyla\">\\xd0\\xa1\\xd0\\xb0\\xd1\\x85\\xd0\\xb0 \\xd0\\xa2\\xd1\\x8b\\xd0\\xbb\\xd0\\xb0</a></li>\\n<li><a href=\"//sco.wikipedia.org/\" lang=\"sco\">Scots</a></li>\\n<li><a href=\"//sq.wikipedia.org/\" lang=\"sq\">Shqip</a></li>\\n<li><a href=\"//scn.wikipedia.org/\" lang=\"scn\">Sicilianu</a></li>\\n<li><a href=\"//si.wikipedia.org/\" lang=\"si\" title=\"Si\\xe1\\xb9\\x83hala\">\\xe0\\xb7\\x83\\xe0\\xb7\\x92\\xe0\\xb6\\x82\\xe0\\xb7\\x84\\xe0\\xb6\\xbd</a></li>\\n<li><a href=\"//sd.wikipedia.org/\" lang=\"sd\" title=\"Sindh\\xc4\\xab\"><bdi dir=\"rtl\">\\xd8\\xb3\\xd9\\x86\\xda\\x8c\\xd9\\x8a</bdi></a></li>\\n<li><a href=\"//szl.wikipedia.org/\" lang=\"szl\">\\xc5\\x9al\\xc5\\xafnski</a></li>\\n<li><a href=\"//su.wikipedia.org/\" lang=\"su\">Basa Sunda</a></li>\\n<li><a href=\"//sw.wikipedia.org/\" lang=\"sw\">Kiswahili</a></li>\\n<li><a href=\"//tl.wikipedia.org/\" lang=\"tl\">Tagalog</a></li>\\n<li><a href=\"//shn.wikipedia.org/\" lang=\"shn\">\\xe1\\x81\\xbd\\xe1\\x82\\x83\\xe1\\x82\\x87\\xe1\\x80\\x9e\\xe1\\x82\\x83\\xe1\\x82\\x87\\xe1\\x80\\x90\\xe1\\x82\\x86\\xe1\\x80\\xb8</a></li>\\n<li><a href=\"//te.wikipedia.org/\" lang=\"te\" title=\"Telugu\">\\xe0\\xb0\\xa4\\xe0\\xb1\\x86\\xe0\\xb0\\xb2\\xe0\\xb1\\x81\\xe0\\xb0\\x97\\xe0\\xb1\\x81</a></li>\\n<li><a href=\"//bug.wikipedia.org/\" lang=\"bug\">\\xe1\\xa8\\x85\\xe1\\xa8\\x94 \\xe1\\xa8\\x95\\xe1\\xa8\\x99\\xe1\\xa8\\x81\\xe1\\xa8\\x97 / Basa Ugi</a></li>\\n<li><a href=\"//vec.wikipedia.org/\" lang=\"vec\">V\\xc3\\xa8neto</a></li>\\n<li><a href=\"//wa.wikipedia.org/\" lang=\"wa\">Walon</a></li>\\n<li><a href=\"//wuu.wikipedia.org/\" lang=\"wuu\" title=\"W\\xc3\\xbay\\xc7\\x94\">\\xe5\\x90\\xb4\\xe8\\xaf\\xad</a></li>\\n<li><a href=\"//yi.wikipedia.org/\" lang=\"yi\" title=\"Yidi\\xc5\\xa1\"><bdi dir=\"rtl\">\\xd7\\x99\\xd7\\x99\\xd6\\xb4\\xd7\\x93\\xd7\\x99\\xd7\\xa9</bdi></a></li>\\n<li><a href=\"//yo.wikipedia.org/\" lang=\"yo\">Yor\\xc3\\xb9b\\xc3\\xa1</a></li>\\n<li><a href=\"//diq.wikipedia.org/\" lang=\"diq\" title=\"Zazaki\">Zazaki</a></li>\\n<li><a href=\"//bat-smg.wikipedia.org/\" lang=\"sgs\">\\xc5\\xbdemait\\xc4\\x97\\xc5\\xa1ka</a></li>\\n<li><a href=\"//zu.wikipedia.org/\" lang=\"zu\">isiZulu</a></li>\\n</ul>\\n</div>\\n<h2 class=\"bookshelf-container\">\\n<span class=\"bookshelf\">\\n<span class=\"text\">\\n<bdi dir=\"ltr\">\\n1&nbsp;000+\\n</bdi>\\n<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\\narticles\\n</span>\\n</span>\\n</span>\\n</h2>\\n<div class=\"langlist hlist\" data-el-section=\"secondary links\">\\n<ul>\\n<li><a href=\"//lad.wikipedia.org/\" lang=\"lad\"><span lang=\"lad-Latn\">Dzhudezmo</span> / <bdi lang=\"lad-Hebr\" dir=\"rtl\">\\xd7\\x9c\\xd7\\x90\\xd7\\x93\\xd7\\x99\\xd7\\xa0\\xd7\\x95</bdi></a></li>\\n<li><a href=\"//kbd.wikipedia.org/\" lang=\"kbd\" title=\"Adighabze\">\\xd0\\x90\\xd0\\xb4\\xd1\\x8b\\xd0\\xb3\\xd1\\x8d\\xd0\\xb1\\xd0\\xb7\\xd1\\x8d</a></li>\\n<li><a href=\"//ang.wikipedia.org/\" lang=\"ang\">\\xc3\\x86nglisc</a></li>\\n<li><a href=\"//smn.wikipedia.org/\" lang=\"smn\" title=\"anar\\xc3\\xa2\\xc5\\xa1kiel\\xc3\\xa2\">Anar\\xc3\\xa2\\xc5\\xa1kiel\\xc3\\xa2</a></li>\\n<li><a href=\"//ab.wikipedia.org/\" lang=\"ab\" title=\"a\\xe1\\xb9\\x97sshwa\">\\xd0\\xb0\\xd4\\xa5\\xd1\\x81\\xd1\\x88\\xd3\\x99\\xd0\\xb0</a></li>\\n<li><a href=\"//hyw.wikipedia.org/\" lang=\"hyw\" title=\"Arevmdahayeren\">\\xd4\\xb1\\xd6\\x80\\xd5\\xa5\\xd6\\x82\\xd5\\xb4\\xd5\\xbf\\xd5\\xa1\\xd5\\xb0\\xd5\\xa1\\xd5\\xb5\\xd5\\xa5\\xd6\\x80\\xd5\\xa7\\xd5\\xb6</a></li>\\n<li><a href=\"//roa-rup.wikipedia.org/\" lang=\"roa-rup\">Arm\\xc3\\xa3neashce</a></li>\\n<li><a href=\"//frp.wikipedia.org/\" lang=\"frp\">Arpitan</a></li>\\n<li><a href=\"//arc.wikipedia.org/\" lang=\"arc\" title=\"\\xc4\\x80t\\xc3\\xbbr\\xc4\\x81y\\xc3\\xa2\"><bdi dir=\"rtl\">\\xdc\\x90\\xdc\\xac\\xdc\\x98\\xdc\\xaa\\xdc\\x9d\\xdc\\x90</bdi></a></li>\\n<li><a href=\"//gn.wikipedia.org/\" lang=\"gn\">Ava\\xc3\\xb1e\\xe2\\x80\\x99\\xe1\\xba\\xbd</a></li>\\n<li><a href=\"//av.wikipedia.org/\" lang=\"av\" title=\"Avar\">\\xd0\\x90\\xd0\\xb2\\xd0\\xb0\\xd1\\x80</a></li>\\n<li><a href=\"//ay.wikipedia.org/\" lang=\"ay\">Aymar</a></li>\\n<li><a href=\"//bjn.wikipedia.org/\" lang=\"bjn\">Bahasa Banjar</a></li>\\n<li><a href=\"//bh.wikipedia.org/\" lang=\"bh\" title=\"Bh\\xc5\\x8djapur\\xc4\\xab\">\\xe0\\xa4\\xad\\xe0\\xa5\\x8b\\xe0\\xa4\\x9c\\xe0\\xa4\\xaa\\xe0\\xa5\\x81\\xe0\\xa4\\xb0\\xe0\\xa5\\x80</a></li>\\n<li><a href=\"//bi.wikipedia.org/\" lang=\"bi\">Bislama</a></li>\\n<li><a href=\"//bo.wikipedia.org/\" lang=\"bo\" title=\"Bod Skad\">\\xe0\\xbd\\x96\\xe0\\xbd\\xbc\\xe0\\xbd\\x91\\xe0\\xbc\\x8b\\xe0\\xbd\\xa1\\xe0\\xbd\\xb2\\xe0\\xbd\\x82</a></li>\\n<li><a href=\"//bxr.wikipedia.org/\" lang=\"bxr\" title=\"Buryad\">\\xd0\\x91\\xd1\\x83\\xd1\\x80\\xd1\\x8f\\xd0\\xb0\\xd0\\xb4</a></li>\\n<li><a href=\"//cbk-zam.wikipedia.org/\" lang=\"cbk-x-zam\">Chavacano de Zamboanga</a></li>\\n<li><a href=\"//co.wikipedia.org/\" lang=\"co\">Corsu</a></li>\\n<li><a href=\"//za.wikipedia.org/\" lang=\"za\">Vahcuengh / \\xe8\\xa9\\xb1\\xe5\\x83\\xae</a></li>\\n<li><a href=\"//dag.wikipedia.org/\" lang=\"dag\">Dagbanli</a></li>\\n<li><a href=\"//ary.wikipedia.org/\" lang=\"ary\" title=\"Darija\"><bdi dir=\"rtl\">\\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1\\xd8\\xac\\xd8\\xa9</bdi></a></li>\\n<li><a href=\"//se.wikipedia.org/\" lang=\"se\">Davvis\\xc3\\xa1megiella</a></li>\\n<li><a href=\"//pdc.wikipedia.org/\" lang=\"pdc\">Deitsch</a></li>\\n<li><a href=\"//dv.wikipedia.org/\" lang=\"dv\" title=\"Divehi\"><bdi dir=\"rtl\">\\xde\\x8b\\xde\\xa8\\xde\\x88\\xde\\xac\\xde\\x80\\xde\\xa8\\xde\\x84\\xde\\xa6\\xde\\x90\\xde\\xb0</bdi></a></li>\\n<li><a href=\"//dsb.wikipedia.org/\" lang=\"dsb\">Dolnoserbski</a></li>\\n<li><a href=\"//myv.wikipedia.org/\" lang=\"myv\" title=\"Erzjanj\">\\xd0\\xad\\xd1\\x80\\xd0\\xb7\\xd1\\x8f\\xd0\\xbd\\xd1\\x8c</a></li>\\n<li><a href=\"//ext.wikipedia.org/\" lang=\"ext\">Estreme\\xc3\\xb1u</a></li>\\n<li><a href=\"//fur.wikipedia.org/\" lang=\"fur\">Furlan</a></li>\\n<li><a href=\"//gv.wikipedia.org/\" lang=\"gv\">Gaelg</a></li>\\n<li><a href=\"//gag.wikipedia.org/\" lang=\"gag\">Gagauz</a></li>\\n<li><a href=\"//inh.wikipedia.org/\" lang=\"inh\" title=\"Ghalghai\">\\xd0\\x93\\xd3\\x80\\xd0\\xb0\\xd0\\xbb\\xd0\\xb3\\xd3\\x80\\xd0\\xb0\\xd0\\xb9</a></li>\\n<li><a href=\"//ki.wikipedia.org/\" lang=\"ki\">G\\xc4\\xa9k\\xc5\\xa9y\\xc5\\xa9</a></li>\\n<li><a href=\"//glk.wikipedia.org/\" lang=\"glk\" title=\"Gil\\xc9\\x99ki\"><bdi dir=\"rtl\">\\xda\\xaf\\xdb\\x8c\\xd9\\x84\\xda\\xa9\\xdb\\x8c</bdi></a></li>\\n<li><a href=\"//gan.wikipedia.org/\" lang=\"gan\" title=\"Gon ua\" data-hans=\"\\xe8\\xb5\\xa3\\xe8\\xaf\\xad\" data-hant=\"\\xe8\\xb4\\x9b\\xe8\\xaa\\x9e\" class=\"jscnconv\">\\xe8\\xb5\\xa3\\xe8\\xaf\\xad / \\xe8\\xb4\\x9b\\xe8\\xaa\\x9e</a></li>\\n<li><a href=\"//hak.wikipedia.org/\" lang=\"hak\">Hak-k\\xc3\\xa2-ng\\xc3\\xae / \\xe5\\xae\\xa2\\xe5\\xae\\xb6\\xe8\\xaa\\x9e</a></li>\\n<li><a href=\"//xal.wikipedia.org/\" lang=\"xal\" title=\"Hal\\xca\\xb9mg\">\\xd0\\xa5\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbc\\xd0\\xb3</a></li>\\n<li><a href=\"//haw.wikipedia.org/\" lang=\"haw\">\\xca\\xbb\\xc5\\x8clelo Hawai\\xca\\xbbi</a></li>\\n<li><a href=\"//ig.wikipedia.org/\" lang=\"ig\">Igbo</a></li>\\n<li><a href=\"//rw.wikipedia.org/\" lang=\"rw\">Ikinyarwanda</a></li>\\n<li><a href=\"//ie.wikipedia.org/\" lang=\"ie\">Interlingue</a></li>\\n<li><a href=\"//kbp.wikipedia.org/\" lang=\"kbp\">Kab\\xc9\\xa9y\\xc9\\x9b</a></li>\\n<li><a href=\"//pam.wikipedia.org/\" lang=\"pam\">Kapampangan</a></li>\\n<li><a href=\"//csb.wikipedia.org/\" lang=\"csb\">Kasz\\xc3\\xabbsczi</a></li>\\n<li><a href=\"//kw.wikipedia.org/\" lang=\"kw\">Kernewek</a></li>\\n<li><a href=\"//km.wikipedia.org/\" lang=\"km\" title=\"Ph\\xc3\\xa9asa Khm\\xc3\\xa9r\">\\xe1\\x9e\\x97\\xe1\\x9e\\xb6\\xe1\\x9e\\x9f\\xe1\\x9e\\xb6\\xe1\\x9e\\x81\\xe1\\x9f\\x92\\xe1\\x9e\\x98\\xe1\\x9f\\x82\\xe1\\x9e\\x9a</a></li>\\n<li><a href=\"//kv.wikipedia.org/\" lang=\"kv\" title=\"Komi\">\\xd0\\x9a\\xd0\\xbe\\xd0\\xbc\\xd0\\xb8</a></li>\\n<li><a href=\"//koi.wikipedia.org/\" lang=\"koi\" title=\"Perem Komi\">\\xd0\\x9f\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xbc \\xd0\\xba\\xd0\\xbe\\xd0\\xbc\\xd0\\xb8</a></li>\\n<li><a href=\"//kg.wikipedia.org/\" lang=\"kg\">Kongo</a></li>\\n<li><a href=\"//gom.wikipedia.org/\" lang=\"gom\">\\xe0\\xa4\\x95\\xe0\\xa5\\x8b\\xe0\\xa4\\x82\\xe0\\xa4\\x95\\xe0\\xa4\\xa3\\xe0\\xa5\\x80 / Konknni</a></li>\\n<li><a href=\"//ks.wikipedia.org/\" lang=\"ks\" title=\"Koshur\"><bdi dir=\"rtl\">\\xd9\\x83\\xd9\\xb2\\xd8\\xb4\\xd9\\x8f\\xd8\\xb1</bdi></a></li>\\n<li><a href=\"//gcr.wikipedia.org/\" lang=\"gcr\" title=\"Kriy\\xc3\\xb2l Gwiyannen\">Kriy\\xc3\\xb2l Gwiyannen</a></li>\\n<li><a href=\"//lo.wikipedia.org/\" lang=\"lo\" title=\"Phaasaa Laao\">\\xe0\\xba\\x9e\\xe0\\xba\\xb2\\xe0\\xba\\xaa\\xe0\\xba\\xb2\\xe0\\xba\\xa5\\xe0\\xba\\xb2\\xe0\\xba\\xa7</a></li>\\n<li><a href=\"//lbe.wikipedia.org/\" lang=\"lbe\" title=\"Lakku\">\\xd0\\x9b\\xd0\\xb0\\xd0\\xba\\xd0\\xba\\xd1\\x83</a></li>\\n<li><a href=\"//ltg.wikipedia.org/\" lang=\"ltg\">Latga\\xc4\\xbcu</a></li>\\n<li><a href=\"//lez.wikipedia.org/\" lang=\"lez\" title=\"Lezgi\">\\xd0\\x9b\\xd0\\xb5\\xd0\\xb7\\xd0\\xb3\\xd0\\xb8</a></li>\\n<li><a href=\"//nia.wikipedia.org/\" lang=\"nia\">Li Niha</a></li>\\n<li><a href=\"//ln.wikipedia.org/\" lang=\"ln\">Ling\\xc3\\xa1la</a></li>\\n<li><a href=\"//jbo.wikipedia.org/\" lang=\"jbo\">lojban</a></li>\\n<li><a href=\"//lg.wikipedia.org/\" lang=\"lg\">Luganda</a></li>\\n<li><a href=\"//mt.wikipedia.org/\" lang=\"mt\">Malti</a></li>\\n<li><a href=\"//mi.wikipedia.org/\" lang=\"mi\">M\\xc4\\x81ori</a></li>\\n<li><a href=\"//tw.wikipedia.org/\" lang=\"tw\" title=\"Mfantse\">Twi</a></li>\\n<li><a href=\"//mwl.wikipedia.org/\" lang=\"mwl\">Mirand\\xc3\\xa9s</a></li>\\n<li><a href=\"//mdf.wikipedia.org/\" lang=\"mdf\" title=\"Mok\\xc5\\xa1enj\">\\xd0\\x9c\\xd0\\xbe\\xd0\\xba\\xd1\\x88\\xd0\\xb5\\xd0\\xbd\\xd1\\x8c</a></li>\\n<li><a href=\"//mnw.wikipedia.org/\" lang=\"mnw\">\\xe1\\x80\\x98\\xe1\\x80\\xac\\xe1\\x80\\x9e\\xe1\\x80\\xac \\xe1\\x80\\x99\\xe1\\x80\\x94\\xe1\\x80\\xba</a></li>\\n<li><a href=\"//nqo.wikipedia.org/\" lang=\"nqo\" title=\"N&#x27;Ko\">\\xdf\\x92\\xdf\\x9e\\xdf\\x8f</a></li>\\n<li><a href=\"//fj.wikipedia.org/\" lang=\"fj\">Na Vosa Vaka-Viti</a></li>\\n<li><a href=\"//nah.wikipedia.org/\" lang=\"nah\">N\\xc4\\x81huatlaht\\xc5\\x8dlli</a></li>\\n<li><a href=\"//na.wikipedia.org/\" lang=\"na\">Dorerin Naoero</a></li>\\n<li><a href=\"//nds-nl.wikipedia.org/\" lang=\"nds-nl\">Nedersaksisch</a></li>\\n<li><a href=\"//nrm.wikipedia.org/\" lang=\"roa-x-nrm\">Nouormand / Normaund</a></li>\\n<li><a href=\"//nov.wikipedia.org/\" lang=\"nov\">Novial</a></li>\\n<li><a href=\"//om.wikipedia.org/\" lang=\"om\" title=\"Ingiliffaa\">Afaan Oromoo</a></li>\\n<li><a href=\"//pi.wikipedia.org/\" lang=\"pi\" title=\"P\\xc4\\x81\\xe1\\xb8\\xb7i\">\\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\xe0\\xa4\\xbf</a></li>\\n<li><a href=\"//pag.wikipedia.org/\" lang=\"pag\">Pangasin\\xc3\\xa1n</a></li>\\n<li><a href=\"//pap.wikipedia.org/\" lang=\"pap\">Papiamentu</a></li>\\n<li><a href=\"//pfl.wikipedia.org/\" lang=\"pfl\">Pf\\xc3\\xa4lzisch</a></li>\\n<li><a href=\"//pcd.wikipedia.org/\" lang=\"pcd\">Picard</a></li>\\n<li><a href=\"//krc.wikipedia.org/\" lang=\"krc\" title=\"Qara\\xc3\\xa7ay\\xe2\\x80\\x93Malqar\">\\xd0\\x9a\\xd1\\x8a\\xd0\\xb0\\xd1\\x80\\xd0\\xb0\\xd1\\x87\\xd0\\xb0\\xd0\\xb9\\xe2\\x80\\x93\\xd0\\xbc\\xd0\\xb0\\xd0\\xbb\\xd0\\xba\\xd1\\x8a\\xd0\\xb0\\xd1\\x80</a></li>\\n<li><a href=\"//kaa.wikipedia.org/\" lang=\"kaa\">Qaraqalpaqsha</a></li>\\n<li><a href=\"//ksh.wikipedia.org/\" lang=\"ksh\">Ripoarisch</a></li>\\n<li><a href=\"//rm.wikipedia.org/\" lang=\"rm\">Rumantsch</a></li>\\n<li><a href=\"//rue.wikipedia.org/\" lang=\"rue\" title=\"Rusin\\xe2\\x80\\x99skyj\">\\xd0\\xa0\\xd1\\x83\\xd1\\x81\\xd0\\xb8\\xd0\\xbd\\xd1\\x8c\\xd1\\x81\\xd0\\xba\\xd1\\x8b\\xd0\\xb9</a></li>\\n<li><a href=\"//sm.wikipedia.org/\" lang=\"sm\">Gagana S\\xc4\\x81moa</a></li>\\n<li><a href=\"//sat.wikipedia.org/\" lang=\"sat\" title=\"Santali\">\\xe1\\xb1\\xa5\\xe1\\xb1\\x9f\\xe1\\xb1\\xb1\\xe1\\xb1\\x9b\\xe1\\xb1\\x9f\\xe1\\xb1\\xb2\\xe1\\xb1\\xa4</a></li>\\n<li><a href=\"//sc.wikipedia.org/\" lang=\"sc\" title=\"Sardu\">Sardu</a></li>\\n<li><a href=\"//trv.wikipedia.org/\" lang=\"trv\">Seediq</a></li>\\n<li><a href=\"//stq.wikipedia.org/\" lang=\"stq\">Seeltersk</a></li>\\n<li><a href=\"//nso.wikipedia.org/\" lang=\"nso\">Sesotho sa Leboa</a></li>\\n<li><a href=\"//sn.wikipedia.org/\" lang=\"sn\">ChiShona</a></li>\\n<li><a href=\"//cu.wikipedia.org/\" lang=\"cu\" title=\"Slov\\xc4\\x9bn\\xc4\\xadsk\\xc5\\xad\">\\xd0\\xa1\\xd0\\xbb\\xd0\\xbe\\xd0\\xb2\\xd1\\xa3\\xcc\\x81\\xd0\\xbd\\xd1\\x8c\\xd1\\x81\\xd0\\xba\\xd1\\x8a / \\xe2\\xb0\\x94\\xe2\\xb0\\x8e\\xe2\\xb0\\x91\\xe2\\xb0\\x82\\xe2\\xb0\\xa1\\xe2\\xb0\\x90\\xe2\\xb0\\xa0\\xe2\\xb0\\x94\\xe2\\xb0\\x8d\\xe2\\xb0\\x9f</a></li>\\n<li><a href=\"//so.wikipedia.org/\" lang=\"so\">Soomaaliga</a></li>\\n<li><a href=\"//srn.wikipedia.org/\" lang=\"srn\">Sranantongo</a></li>\\n<li><a href=\"//kab.wikipedia.org/\" lang=\"kab\" title=\"Taqbaylit\">Taqbaylit</a></li>\\n<li><a href=\"//roa-tara.wikipedia.org/\" lang=\"roa\">Tarand\\xc3\\xadne</a></li>\\n<li><a href=\"//tet.wikipedia.org/\" lang=\"tet\">Tetun</a></li>\\n<li><a href=\"//tpi.wikipedia.org/\" lang=\"tpi\">Tok Pisin</a></li>\\n<li><a href=\"//to.wikipedia.org/\" lang=\"to\">faka Tonga</a></li>\\n<li><a href=\"//chr.wikipedia.org/\" lang=\"chr\" title=\"Tsalagi\">\\xe1\\x8f\\xa3\\xe1\\x8e\\xb3\\xe1\\x8e\\xa9</a></li>\\n<li><a href=\"//tum.wikipedia.org/\" lang=\"tum\">chiTumbuka</a></li>\\n<li><a href=\"//tk.wikipedia.org/\" lang=\"tk\">T\\xc3\\xbcrkmen\\xc3\\xa7e</a></li>\\n<li><a href=\"//tyv.wikipedia.org/\" lang=\"tyv\" title=\"Tyva dyl\">\\xd0\\xa2\\xd1\\x8b\\xd0\\xb2\\xd0\\xb0 \\xd0\\xb4\\xd1\\x8b\\xd0\\xbb</a></li>\\n<li><a href=\"//udm.wikipedia.org/\" lang=\"udm\" title=\"Udmurt\">\\xd0\\xa3\\xd0\\xb4\\xd0\\xbc\\xd1\\x83\\xd1\\x80\\xd1\\x82</a></li>\\n<li><a href=\"//ug.wikipedia.org/\" lang=\"ug\"><bdi dir=\"rtl\">\\xd8\\xa6\\xdb\\x87\\xd9\\x8a\\xd8\\xba\\xdb\\x87\\xd8\\xb1\\xda\\x86\\xd9\\x87</bdi></a></li>\\n<li><a href=\"//vep.wikipedia.org/\" lang=\"vep\">Veps\\xc3\\xa4n</a></li>\\n<li><a href=\"//fiu-vro.wikipedia.org/\" lang=\"fiu-vro\">V\\xc3\\xb5ro</a></li>\\n<li><a href=\"//vls.wikipedia.org/\" lang=\"vls\">West-Vlams</a></li>\\n<li><a href=\"//wo.wikipedia.org/\" lang=\"wo\">Wolof</a></li>\\n<li><a href=\"//xh.wikipedia.org/\" lang=\"xh\">isiXhosa</a></li>\\n<li><a href=\"//zea.wikipedia.org/\" lang=\"zea\">Ze\\xc3\\xaauws</a></li>\\n<li><a href=\"//ty.wikipedia.org/\" lang=\"ty\">Reo tahiti</a></li>\\n</ul>\\n</div>\\n<h2 class=\"bookshelf-container\">\\n<span class=\"bookshelf\">\\n<span class=\"text\">\\n<bdi dir=\"ltr\">\\n100+\\n</bdi>\\n<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\\narticles\\n</span>\\n</span>\\n</span>\\n</h2>\\n<div class=\"langlist langlist-tiny hlist\" data-el-section=\"secondary links\">\\n<ul>\\n<li><a href=\"//ak.wikipedia.org/\" lang=\"ak\">Akan</a></li>\\n<li><a href=\"//bm.wikipedia.org/\" lang=\"bm\">Bamanankan</a></li>\\n<li><a href=\"//ch.wikipedia.org/\" lang=\"ch\">Chamoru</a></li>\\n<li><a href=\"//ny.wikipedia.org/\" lang=\"ny\">Chichewa</a></li>\\n<li><a href=\"//ee.wikipedia.org/\" lang=\"ee\">E\\xca\\x8begbe</a></li>\\n<li><a href=\"//ff.wikipedia.org/\" lang=\"ff\">Fulfulde</a></li>\\n<li><a href=\"//got.wikipedia.org/\" lang=\"got\" title=\"Gutisk\">\\xf0\\x90\\x8c\\xb2\\xf0\\x90\\x8c\\xbf\\xf0\\x90\\x8d\\x84\\xf0\\x90\\x8c\\xb9\\xf0\\x90\\x8d\\x83\\xf0\\x90\\x8c\\xba</a></li>\\n<li><a href=\"//iu.wikipedia.org/\" lang=\"iu\">\\xe1\\x90\\x83\\xe1\\x93\\x84\\xe1\\x92\\x83\\xe1\\x91\\x8e\\xe1\\x91\\x90\\xe1\\x91\\xa6 / Inuktitut</a></li>\\n<li><a href=\"//ik.wikipedia.org/\" lang=\"ik\">I\\xc3\\xb1upiak</a></li>\\n<li><a href=\"//kl.wikipedia.org/\" lang=\"kl\">Kalaallisut</a></li>\\n<li><a href=\"//mad.wikipedia.org/\" lang=\"mad\">Madhur\\xc3\\xa2</a></li>\\n<li><a href=\"//cr.wikipedia.org/\" lang=\"cr\">N\\xc4\\x93hiyaw\\xc4\\x93win / \\xe1\\x93\\x80\\xe1\\x90\\xa6\\xe1\\x90\\x83\\xe1\\x94\\xad\\xe1\\x90\\x8d\\xe1\\x90\\x8f\\xe1\\x90\\xa3</a></li>\\n<li><a href=\"//pih.wikipedia.org/\" lang=\"pih\">Norfuk / Pitkern</a></li>\\n<li><a href=\"//ami.wikipedia.org/\" lang=\"ami\">Pangcah</a></li>\\n<li><a href=\"//pwn.wikipedia.org/\" lang=\"pwn\">pinayuanan</a></li>\\n<li><a href=\"//pnt.wikipedia.org/\" lang=\"pnt\" title=\"Pontiak\\xc3\\xa1\">\\xce\\xa0\\xce\\xbf\\xce\\xbd\\xcf\\x84\\xce\\xb9\\xce\\xb1\\xce\\xba\\xce\\xac</a></li>\\n<li><a href=\"//dz.wikipedia.org/\" lang=\"dz\" title=\"Rdzong-Kha\">\\xe0\\xbd\\xa2\\xe0\\xbe\\xab\\xe0\\xbd\\xbc\\xe0\\xbd\\x84\\xe0\\xbc\\x8b\\xe0\\xbd\\x81</a></li>\\n<li><a href=\"//rmy.wikipedia.org/\" lang=\"rmy\">romani \\xc4\\x8dhib</a></li>\\n<li><a href=\"//rn.wikipedia.org/\" lang=\"rn\">Ikirundi</a></li>\\n<li><a href=\"//sg.wikipedia.org/\" lang=\"sg\">S\\xc3\\xa4ng\\xc3\\xb6</a></li>\\n<li><a href=\"//st.wikipedia.org/\" lang=\"st\">Sesotho</a></li>\\n<li><a href=\"//tn.wikipedia.org/\" lang=\"tn\">Setswana</a></li>\\n<li><a href=\"//ss.wikipedia.org/\" lang=\"ss\">SiSwati</a></li>\\n<li><a href=\"//ti.wikipedia.org/\" lang=\"ti\" title=\"T\\xc9\\x99g\\xc9\\x99r\\xc9\\x99\\xc3\\xb1a\">\\xe1\\x89\\xb5\\xe1\\x8c\\x8d\\xe1\\x88\\xad\\xe1\\x8a\\x9b</a></li>\\n<li><a href=\"//din.wikipedia.org/\" lang=\"din\">Thu\\xc9\\x94\\xc5\\x8bj\\xc3\\xa4\\xc5\\x8b</a></li>\\n<li><a href=\"//chy.wikipedia.org/\" lang=\"chy\">Ts\\xc4\\x97hesen\\xc4\\x97stsestotse</a></li>\\n<li><a href=\"//ts.wikipedia.org/\" lang=\"ts\">Xitsonga</a></li>\\n<li><a href=\"//ve.wikipedia.org/\" lang=\"ve\">Tshiven\\xe1\\xb8\\x93a</a></li>\\n</ul>\\n</div>\\n<div class=\"langlist langlist-others hlist\" data-el-section=\"other languages\">\\n<a class=\"jsl10n\" href=\"https://meta.wikimedia.org/wiki/Special:MyLanguage/List_of_Wikipedias\" lang data-jsl10n=\"other-languages-label\">Other languages</a>\\n</div></div>\\n</div>\\n<hr>\\n<div class=\"footer\" data-el-section=\"other projects\">\\n<div class=\"footer-sidebar\">\\n<div class=\"footer-sidebar-content\">\\n<div class=\"footer-sidebar-icon sprite svg-Wikimedia-logo_black\">\\n</div>\\n<div class=\"footer-sidebar-text jsl10n\" data-jsl10n=\"portal.footer-description\">\\nWikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects.\\n</div>\\n<div class=\"footer-sidebar-text\">\\n<a href=\"https://donate.wikimedia.org/?utm_medium=portal&utm_campaign=portalFooter&utm_source=portalFooter\" target=\"_blank\">\\n<span class=\"jsl10n\" data-jsl10n=\"footer-donate\">You can support our work with a donation.</span>\\n</a>\\n</div>\\n</div>\\n</div>\\n<div class=\"footer-sidebar app-badges\">\\n<div class=\"footer-sidebar-content\">\\n<div class=\"footer-sidebar-text\">\\n<div class=\"footer-sidebar-icon sprite svg-wikipedia_app_tile\"></div>\\n<strong class=\"jsl10n\" data-jsl10n=\"portal.app-links.title\">\\n<a class=\"jsl10n\" data-jsl10n=\"portal.app-links.url\" href=\"https://en.wikipedia.org/wiki/List_of_Wikipedia_mobile_applications\">\\nDownload Wikipedia for Android or iOS\\n</a>\\n</strong>\\n<p class=\"jsl10n\" data-jsl10n=\"portal.app-links.description\">\\nSave your favorite articles to read offline, sync your reading lists across devices and customize your reading experience with the official Wikipedia app.\\n</p>\\n<ul>\\n<li class=\"app-badge app-badge-android\">\\n<a target=\"_blank\" rel=\"noreferrer\" href=\"https://play.google.com/store/apps/details?id=org.wikipedia&referrer=utm_source%3Dportal%26utm_medium%3Dbutton%26anid%3Dadmob\">\\n<span class=\"jsl10n sprite svg-badge_google_play_store\" data-jsl10n=\"portal.app-links.google-store\">Google Play Store</span>\\n</a>\\n</li>\\n<li class=\"app-badge app-badge-ios\">\\n<a target=\"_blank\" rel=\"noreferrer\" href=\"https://itunes.apple.com/app/apple-store/id324715238?pt=208305&ct=portal&mt=8\">\\n<span class=\"jsl10n sprite svg-badge_ios_app_store\" data-jsl10n=\"portal.app-links.apple-store\">Apple App Store</span>\\n</a>\\n</li>\\n</ul>\\n</div>\\n</div>\\n</div>\\n<div class=\"other-projects\">\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//commons.wikimedia.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-Commons-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"commons.name\">Commons</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"commons.slogan\">Freely usable photos &amp; more</span>\\n</div>\\n</a>\\n</div>\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//www.wikivoyage.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-Wikivoyage-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikivoyage.name\">Wikivoyage</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikivoyage.slogan\">Free travel guide</span>\\n</div>\\n</a>\\n</div>\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//www.wiktionary.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-Wiktionary-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"wiktionary.name\">Wiktionary</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wiktionary.slogan\">Free dictionary</span>\\n</div>\\n</a>\\n</div>\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//www.wikibooks.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-Wikibooks-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikibooks.name\">Wikibooks</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikibooks.slogan\">Free textbooks</span>\\n</div>\\n</a>\\n</div>\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//www.wikinews.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-Wikinews-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikinews.name\">Wikinews</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikinews.slogan\">Free news source</span>\\n</div>\\n</a>\\n</div>\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//www.wikidata.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-Wikidata-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikidata.name\">Wikidata</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikidata.slogan\">Free knowledge base</span>\\n</div>\\n</a>\\n</div>\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//www.wikiversity.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-Wikiversity-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikiversity.name\">Wikiversity</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikiversity.slogan\">Free course materials</span>\\n</div>\\n</a>\\n</div>\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//www.wikiquote.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-Wikiquote-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikiquote.name\">Wikiquote</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikiquote.slogan\">Free quote compendium</span>\\n</div>\\n</a>\\n</div>\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//www.mediawiki.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-MediaWiki-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"mediawiki.name\">MediaWiki</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"mediawiki.slogan\">Free &amp; open wiki application</span>\\n</div>\\n</a>\\n</div>\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//www.wikisource.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-Wikisource-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikisource.name\">Wikisource</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikisource.slogan\">Free library</span>\\n</div>\\n</a>\\n</div>\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//species.wikimedia.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-Wikispecies-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikispecies.name\">Wikispecies</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikispecies.slogan\">Free species directory</span>\\n</div>\\n</a>\\n</div>\\n<div class=\"other-project\">\\n<a class=\"other-project-link\" href=\"//meta.wikimedia.org/\">\\n<div class=\"other-project-icon\">\\n<div class=\"sprite svg-Meta-Wiki-logo_sister\"></div>\\n</div>\\n<div class=\"other-project-text\">\\n<span class=\"other-project-title jsl10n\" data-jsl10n=\"metawiki.name\">Meta-Wiki</span>\\n<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"metawiki.slogan\">Community coordination &amp; documentation</span>\\n</div>\\n</a>\\n</div>\\n</div>\\n</div>\\n<hr>\\n<p class=\"site-license\">\\n<small class=\"jsl10n\" data-jsl10n=\"license\">This page is available under the <a href=\"https://creativecommons.org/licenses/by-sa/3.0/\">Creative Commons Attribution-ShareAlike License</a></small>\\n<small class=\"jsl10n\" data-jsl10n=\"terms\"><a href=\"https://meta.wikimedia.org/wiki/Terms_of_use\">Terms of Use</a></small>\\n<small class=\"jsl10n\" data-jsl10n=\"privacy-policy\"><a href=\"https://meta.wikimedia.org/wiki/Privacy_policy\">Privacy Policy</a></small>\\n</p>\\n<script>\\nvar rtlLangs = [\\'ar\\',\\'arc\\',\\'ary\\',\\'arz\\',\\'bcc\\',\\'bgn\\',\\'bqi\\',\\'ckb\\',\\'dv\\',\\'fa\\',\\'glk\\',\\'he\\',\\'kk-cn\\',\\'kk-arab\\',\\'khw\\',\\'ks\\',\\'ku-arab\\',\\'lki\\',\\'luz\\',\\'mzn\\',\\'nqo\\',\\'pnb\\',\\'ps\\',\\'sd\\',\\'sdh\\',\\'skr\\',\\'ug\\',\\'ur\\',\\'yi\\'],\\n    translationsHash = \\'88c7e1eb\\',\\n    /**\\n     * This variable is used to convert the generic \"portal\" keyword in the data-jsl10n attributes\\n     * e.g. \\'data-jsl10n=\"portal.footer-description\"\\' into a portal-specific key, e.g. \"wiki\"\\n     * for the Wikipedia portal.\\n     */\\n    translationsPortalKey = \\'wiki\\';\\n    /**\\n     * The wm-typeahead.js feature is used for search,and it uses domain name for searching. We want domain\\n     * name to be portal Specific (different for every portal).So by declaring variable \\'portalSearchDomain\\'\\n     * in index.handlebars we will make this portal Specific.\\n    **/\\n    portalSearchDomain = \\'wikipedia.org\\'\\n    /*\\n     This object is used by l10n scripts (page-localized.js, topten-localized.js)\\n     to reveal the page content after l10n json is loaded.\\n     A timer is also set to prevent JS from hiding page content indefinitelty.\\n     This script is inlined to safeguard againt script loading errors and placed\\n     at the top of the page to safeguard against any HTML loading/parsing errors.\\n    */\\n    wmL10nVisible = {\\n        ready: false,\\n        makeVisible: function(){\\n            if ( !wmL10nVisible.ready ) {\\n                wmL10nVisible.ready = true;\\n                document.body.className += \\' jsl10n-visible\\';\\n            }\\n        }\\n    };\\n    window.setTimeout( wmL10nVisible.makeVisible, 1000 )\\n</script>\\n<script src=\"portal/wikipedia.org/assets/js/index-f1d77ed19b.js\"></script>\\n<!--[if gt IE 9]><!-->\\n<script src=\"portal/wikipedia.org/assets/js/gt-ie9-ce3fe8e88d.js\"></script>\\n<!--<![endif]-->\\n<!--[if lte IE 9]><!-->\\n<style>\\n.styled-select {\\n        display: block;\\n    }\\n</style>\\n<!--<![endif]-->\\n<!--[if lte IE 9]>\\n<style>\\n    .langlist > ul {\\n        text-align: center;\\n    }\\n    .langlist > ul > li {\\n        display: inline;\\n        padding: 0 0.5em;\\n    }\\n</style>\\n<![endif]-->\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "print(html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 5. GET requests using requests\n",
    "\n",
    "Now we are going to do the same, however here we'll use the requests package, which provides a wonderful API for making requests. According to the requests package website. \"Requests allows you to send organic, grass-fed HTTP/1 dot 1 requests, without the need for manual labor.\" and the following organizations claim to use requests internally: \"Her Majesty's Government, Amazon, Google, Twilio, NPR, Obama for America, Twitter, Sony, and Federal U.S. Institutions that prefer to be unnamed.\""
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 6. GET requests using requests\n",
    "\n",
    "Moreover, \"Requests is one of the most downloaded Python packages of all time, pulling in over 7,000,000 downloads every month. All the cool kids are doing it!\" Lets now see requests at work. Here, you import the package requests, specify the URL, package the request, send the request and catch the response with a single function requests dot get; apply the text method to the response which returns the HTML as a string."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" class=\"no-js\">\n",
      "<head>\n",
      "<meta charset=\"utf-8\">\n",
      "<title>Wikipedia</title>\n",
      "<meta name=\"description\" content=\"Wikipedia is a free online encyclopedia, created and edited by volunteers around the world and hosted by the Wikimedia Foundation.\">\n",
      "<script>\n",
      "document.documentElement.className = document.documentElement.className.replace( /(^|\\s)no-js(\\s|$)/, \"$1js-enabled$2\" );\n",
      "</script>\n",
      "<meta name=\"viewport\" content=\"initial-scale=1,user-scalable=yes\">\n",
      "<link rel=\"apple-touch-icon\" href=\"/static/apple-touch/wikipedia.png\">\n",
      "<link rel=\"shortcut icon\" href=\"/static/favicon/wikipedia.ico\">\n",
      "<link rel=\"license\" href=\"//creativecommons.org/licenses/by-sa/3.0/\">\n",
      "<style>\n",
      ".sprite{background-image:linear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/sprite-e99844f6.svg);background-repeat:no-repeat;display:inline-block;vertical-align:middle}.svg-Commons-logo_sister{background-position:0 0;width:47px;height:47px}.svg-MediaWiki-logo_sister{background-position:0 -47px;width:42px;height:42px}.svg-Meta-Wiki-logo_sister{background-position:0 -89px;width:37px;height:37px}.svg-Wikibooks-logo_sister{background-position:0 -126px;width:37px;height:37px}.svg-Wikidata-logo_sister{background-position:0 -163px;width:49px;height:49px}.svg-Wikimedia-logo_black{background-position:0 -212px;width:42px;height:42px}.svg-Wikipedia_wordmark{background-position:0 -254px;width:176px;height:32px}.svg-Wikiquote-logo_sister{background-position:0 -286px;width:42px;height:42px}.svg-Wikisource-logo_sister{background-position:0 -328px;width:39px;height:39px}.svg-Wikispecies-logo_sister{background-position:0 -367px;width:42px;height:42px}.svg-Wikiversity-logo_sister{background-position:0 -409px;width:43px;height:37px}.svg-Wikivoyage-logo_sister{background-position:0 -446px;width:36px;height:36px}.svg-Wiktionary-logo_sister{background-position:0 -482px;width:37px;height:37px}.svg-arrow-down{background-position:0 -519px;width:12px;height:8px}.svg-arrow-down-blue{background-position:0 -527px;width:14px;height:14px}.svg-badge_google_play_store{background-position:0 -541px;width:124px;height:38px}.svg-badge_ios_app_store{background-position:0 -579px;width:110px;height:38px}.svg-language-icon{background-position:0 -617px;width:22px;height:22px}.svg-noimage{background-position:0 -639px;width:58px;height:58px}.svg-search-icon{background-position:0 -697px;width:22px;height:22px}.svg-wikipedia_app_tile{background-position:0 -719px;width:42px;height:42px}\n",
      "</style>\n",
      "<style>\n",
      "html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;font-size:62.5%}body{margin:0}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:700}dfn{font-style:italic}h1{font-size:32px;font-size:3.2rem;margin:1.2rem 0}mark{background:#fc3;color:#000}small{font-size:13px;font-size:1.3rem}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}svg:not(:root){overflow:hidden}figure{margin:1.6rem 4rem}hr{-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace,monospace;font-size:14px;font-size:1.4rem}button,input,optgroup,select,textarea{color:inherit;font:inherit;margin:0}button{overflow:visible}button,select{text-transform:none}button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}input{line-height:normal}input[type=checkbox],input[type=radio]{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;padding:0}input[type=number]::-webkit-inner-spin-button,input[type=number]::-webkit-outer-spin-button{height:auto}input[type=search]{-webkit-appearance:none;-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}input[type=search]::-webkit-search-cancel-button,input[type=search]::-webkit-search-decoration{-webkit-appearance:none}input[type=search]:focus{outline-offset:-2px}fieldset{border:1px solid #a2a9b1;margin:0 .2rem;padding:.6rem 1rem 1.2rem}legend{border:0;padding:0}textarea{overflow:auto}optgroup{font-weight:700}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}.hidden,[hidden]{display:none!important}.screen-reader-text{display:block;position:absolute!important;clip:rect(1px,1px,1px,1px);width:1px;height:1px;margin:-1px;border:0;padding:0;overflow:hidden}body{background-color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Lato,Helvetica,Arial,sans-serif;font-size:14px;font-size:1.4rem;line-height:1.5;margin:.4rem 0 1.6rem}a{-ms-touch-action:manipulation;touch-action:manipulation}a,a:active,a:focus{unicode-bidi:embed;outline:0;color:#36c;text-decoration:none}a:focus{outline:1px solid #36c}a:hover{text-decoration:underline}img{vertical-align:middle}hr,img{border:0}hr{clear:both;height:0;border-bottom:1px solid #c8ccd1;margin:.26rem 1.3rem}.pure-button{display:inline-block;zoom:1;line-height:normal;white-space:nowrap;text-align:center;cursor:pointer;-webkit-user-drag:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;background-color:#f8f9fa;color:#202122;position:relative;min-height:19.2px;min-height:1.92rem;min-width:16px;min-width:1.6rem;margin:.16rem 0;border:1px solid #a2a9b1;-moz-border-radius:2px;border-radius:2px;padding:.8rem 1.6rem;font-family:inherit;font-size:inherit;font-weight:700;text-decoration:none;vertical-align:top;-webkit-transition:background .1s ease,color .1s ease,border-color .1s ease,-webkit-box-shadow .1s ease;transition:background .1s ease,color .1s ease,border-color .1s ease,-webkit-box-shadow .1s ease;-o-transition:background .1s ease,color .1s ease,border-color .1s ease,box-shadow .1s ease;-moz-transition:background .1s ease,color .1s ease,border-color .1s ease,box-shadow .1s ease,-moz-box-shadow .1s ease;transition:background .1s ease,color .1s ease,border-color .1s ease,box-shadow .1s ease;transition:background .1s ease,color .1s ease,border-color .1s ease,box-shadow .1s ease,-webkit-box-shadow .1s ease,-moz-box-shadow .1s ease}.pure-button::-moz-focus-inner{padding:0;border:0}.pure-button-hover,.pure-button:hover{background-color:#fff;border-color:#a2a9b1;color:#404244}.pure-button-active,.pure-button:active{background-color:#c8ccd1;border-color:#72777d;color:#000}.pure-button:focus{outline:0;border-color:#36c;-webkit-box-shadow:inset 0 0 0 1px #36c;-moz-box-shadow:inset 0 0 0 1px #36c;box-shadow:inset 0 0 0 1px #36c}.pure-button-primary-progressive{background-color:#36c;border-color:#36c;color:#fff}.pure-button-primary-progressive:hover{background:#447ff5;border-color:#447ff5}.pure-button-primary-progressive:active{background-color:#2a4b8d;border-color:#2a4b8d;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none;color:#fff}.pure-button-primary-progressive:focus{-webkit-box-shadow:inset 0 0 0 1px #36c,inset 0 0 0 2px #fff;-moz-box-shadow:inset 0 0 0 1px #36c,inset 0 0 0 2px #fff;box-shadow:inset 0 0 0 1px #36c,inset 0 0 0 2px #fff;border-color:#36c}.pure-form input[type=search]{background-color:#fff;display:inline-block;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;border:1px solid #a2a9b1;-moz-border-radius:2px;border-radius:2px;padding:.8rem;-webkit-box-shadow:inset 0 0 0 1px #fff;-moz-box-shadow:inset 0 0 0 1px #fff;box-shadow:inset 0 0 0 1px #fff;vertical-align:middle}.pure-form input:focus:invalid{color:#b32424;border-color:#d33}.pure-form fieldset{margin:0;padding:.56rem 0 1.2rem;border:0}@media only screen and (max-width:480px){.pure-form input[type=search]{display:block}}.central-textlogo-wrapper{display:inline-block;vertical-align:bottom}.central-textlogo{position:relative;margin:4rem auto .5rem;width:270px;font-family:Linux Libertine,Hoefler Text,Georgia,Times New Roman,Times,serif;font-size:30px;font-size:3rem;font-weight:400;line-height:33px;line-height:3.3rem;text-align:center;-moz-font-feature-settings:\"ss05=1\";-moz-font-feature-settings:\"ss05\";-webkit-font-feature-settings:\"ss05\";-ms-font-feature-settings:\"ss05\";font-feature-settings:\"ss05\"}.localized-slogan{display:block;font-family:Linux Libertine,Georgia,Times,serif;font-size:15px;font-size:1.5rem;font-weight:400}.central-textlogo__image{color:transparent;display:inline-block;overflow:hidden;text-indent:-10000px}.central-featured-logo{position:absolute;top:158px;left:35px}@media (max-width:480px){.central-textlogo{position:relative;height:70px;width:auto;margin:2rem 0 0;text-align:center;line-height:25px;line-height:2.5rem;text-indent:-10px;text-indent:-1rem;font-size:1em}.central-textlogo-wrapper{position:relative;top:12px;text-indent:2px;text-indent:.2rem}.svg-Wikipedia_wordmark{width:150px;height:25px;background-position:0 -218px;-webkit-background-size:100% 100%;-moz-background-size:100%;background-size:100%}.localized-slogan{font-size:14px;font-size:1.4rem}.central-featured-logo{position:relative;display:inline-block;width:57px;height:auto;left:0;top:0}}@media (max-width:240px){.central-textlogo__image{height:auto}}.central-featured{position:relative;height:325px;height:32.5rem;width:546px;width:54.6rem;max-width:100%;margin:0 auto;text-align:center;vertical-align:middle}.central-featured-lang{position:absolute;width:156px;width:15.6rem}.central-featured-lang .link-box{display:block;padding:0;text-decoration:none;white-space:normal}.central-featured-lang .link-box:hover strong{text-decoration:underline}.central-featured-lang :hover{background-color:#eaecf0}.central-featured-lang strong{display:block;font-size:16px;font-size:1.6rem}.central-featured-lang small{color:#54595d;display:inline-block;font-size:13px;font-size:1.3rem;line-height:1.6}.central-featured-lang em{font-style:italic}.central-featured-lang .emNonItalicLang{font-style:normal}.lang1{top:0;right:60%}.lang2{top:0;left:60%}.lang3{top:20%;right:70%}.lang4{top:20%;left:70%}.lang5{top:40%;right:72%}.lang6{top:40%;left:72%}.lang7{top:60%;right:70%}.lang8{top:60%;left:70%}.lang9{top:80%;right:60%}.lang10{top:80%;left:60%}@media (max-width:480px){.central-featured{width:auto;height:auto;margin-top:7rem;font-size:13px;font-size:1.3rem;text-align:left}.central-featured:after{content:\" \";display:block;visibility:hidden;clear:both;height:0;font-size:0}.central-featured-lang{display:block;float:left;position:relative;top:auto;left:auto;right:auto;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;height:64px;height:6.4rem;width:33%;margin:0 0 16px;padding:0 1.6rem;font-size:14px;font-size:1.4rem;text-align:center}.central-featured-lang strong{font-size:14px;font-size:1.4rem;margin-bottom:4px}.central-featured-lang small{line-height:1.4}}@media (max-width:375px){.central-featured-lang{font-size:13px;font-size:1.3rem}}@media (max-width:240px){.central-featured-lang{width:100%}}.search-container{float:none;max-width:95%;width:540px;margin:.4rem auto 1.95rem;text-align:center;vertical-align:middle}.search-container fieldset{word-spacing:-4px}.search-container button{min-height:44px;min-height:4.4rem;margin:0;-moz-border-radius:0 2px 2px 0;border-radius:0 2px 2px 0;padding:.8rem 1.6rem;font-size:16px;font-size:1.6rem;z-index:2}.search-container button .svg-search-icon{text-indent:-9999px}.search-container input[type=search]::-webkit-search-results-button,.search-container input[type=search]::-webkit-search-results-decoration{-webkit-appearance:none}.search-container input::-webkit-calendar-picker-indicator{display:none}.search-container .sprite.svg-arrow-down{position:absolute;top:8px;top:.8rem;right:6px;right:.6rem}#searchInput{-webkit-appearance:none;width:100%;height:44px;height:4.4rem;border-width:1px 0 1px 1px;-moz-border-radius:2px 0 0 2px;border-radius:2px 0 0 2px;padding:.8rem 9.6rem .8rem 1.2rem;font-size:16px;font-size:1.6rem;line-height:1.6;-webkit-transition:background .1s ease,border-color .1s ease,-webkit-box-shadow .1s ease;transition:background .1s ease,border-color .1s ease,-webkit-box-shadow .1s ease;-o-transition:background .1s ease,border-color .1s ease,box-shadow .1s ease;-moz-transition:background .1s ease,border-color .1s ease,box-shadow .1s ease,-moz-box-shadow .1s ease;transition:background .1s ease,border-color .1s ease,box-shadow .1s ease;transition:background .1s ease,border-color .1s ease,box-shadow .1s ease,-webkit-box-shadow .1s ease,-moz-box-shadow .1s ease}#searchInput:hover{border-color:#72777d}#searchInput:focus{border-color:#36c;-webkit-box-shadow:inset 0 0 0 1px #36c;-moz-box-shadow:inset 0 0 0 1px #36c;box-shadow:inset 0 0 0 1px #36c;outline:0}.search-container .search-input{display:inline-block;position:relative;width:73%;vertical-align:top}@media only screen and (max-width:480px){.search-container .pure-form fieldset{margin-left:1rem;margin-right:6.6rem}.search-container .search-input{width:100%;margin-right:-6.6rem}.search-container .pure-form button{float:right;right:-56px;right:-5.6rem}}.suggestions-dropdown{background-color:#fff;display:inline-block;position:absolute;left:0;z-index:2;margin:0;padding:0;border:1px solid #a2a9b1;border-top:0;-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,.25);-moz-box-shadow:0 2px 2px 0 rgba(0,0,0,.25);box-shadow:0 2px 2px 0 rgba(0,0,0,.25);list-style-type:none;word-spacing:normal}.suggestion-link,.suggestions-dropdown{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;width:100%;text-align:left}.suggestion-link{display:block;position:relative;min-height:70px;min-height:7rem;padding:1rem 1rem 1rem 8.5rem;border-bottom:1px solid #eaecf0;color:inherit;text-decoration:none;text-align:initial;white-space:normal}.suggestion-link.active{background-color:#eaf3ff}a.suggestion-link:hover{text-decoration:none}a.suggestion-link:active,a.suggestion-link:focus{outline:0;white-space:normal}.suggestion-thumbnail{background-color:#eaecf0;background-image:url(portal/wikipedia.org/assets/img/noimage.png);background-image:-webkit-linear-gradient(transparent,transparent),url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 56 56'%3E%3Cpath fill='%23eee' d='M0 0h56v56H0z'/%3E%3Cpath fill='%23999' d='M36.4 13.5H17.8v24.9c0 1.4.9 2.3 2.3 2.3h18.7v-25c.1-1.4-1-2.2-2.4-2.2zM30.2 17h5.1v6.4h-5.1V17zm-8.8 0h6v1.8h-6V17zm0 4.6h6v1.8h-6v-1.8zm0 15.5v-1.8h13.8v1.8H21.4zm13.8-4.5H21.4v-1.8h13.8v1.8zm0-4.7H21.4v-1.8h13.8v1.8z'/%3E%3C/svg%3E\");background-image:-webkit-linear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/noimage.svg) !ie;background-image:-webkit-gradient(linear,left top,left bottom,from(transparent),to(transparent)),url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 56 56'%3E%3Cpath fill='%23eee' d='M0 0h56v56H0z'/%3E%3Cpath fill='%23999' d='M36.4 13.5H17.8v24.9c0 1.4.9 2.3 2.3 2.3h18.7v-25c.1-1.4-1-2.2-2.4-2.2zM30.2 17h5.1v6.4h-5.1V17zm-8.8 0h6v1.8h-6V17zm0 4.6h6v1.8h-6v-1.8zm0 15.5v-1.8h13.8v1.8H21.4zm13.8-4.5H21.4v-1.8h13.8v1.8zm0-4.7H21.4v-1.8h13.8v1.8z'/%3E%3C/svg%3E\");background-image:-moz- oldlinear-gradient(transparent,transparent),url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 56 56'%3E%3Cpath fill='%23eee' d='M0 0h56v56H0z'/%3E%3Cpath fill='%23999' d='M36.4 13.5H17.8v24.9c0 1.4.9 2.3 2.3 2.3h18.7v-25c.1-1.4-1-2.2-2.4-2.2zM30.2 17h5.1v6.4h-5.1V17zm-8.8 0h6v1.8h-6V17zm0 4.6h6v1.8h-6v-1.8zm0 15.5v-1.8h13.8v1.8H21.4zm13.8-4.5H21.4v-1.8h13.8v1.8zm0-4.7H21.4v-1.8h13.8v1.8z'/%3E%3C/svg%3E\");background-image:-o-linear-gradient(transparent,transparent),url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 56 56'%3E%3Cpath fill='%23eee' d='M0 0h56v56H0z'/%3E%3Cpath fill='%23999' d='M36.4 13.5H17.8v24.9c0 1.4.9 2.3 2.3 2.3h18.7v-25c.1-1.4-1-2.2-2.4-2.2zM30.2 17h5.1v6.4h-5.1V17zm-8.8 0h6v1.8h-6V17zm0 4.6h6v1.8h-6v-1.8zm0 15.5v-1.8h13.8v1.8H21.4zm13.8-4.5H21.4v-1.8h13.8v1.8zm0-4.7H21.4v-1.8h13.8v1.8z'/%3E%3C/svg%3E\");background-image:linear-gradient(transparent,transparent),url(\"data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 56 56'%3E%3Cpath fill='%23eee' d='M0 0h56v56H0z'/%3E%3Cpath fill='%23999' d='M36.4 13.5H17.8v24.9c0 1.4.9 2.3 2.3 2.3h18.7v-25c.1-1.4-1-2.2-2.4-2.2zM30.2 17h5.1v6.4h-5.1V17zm-8.8 0h6v1.8h-6V17zm0 4.6h6v1.8h-6v-1.8zm0 15.5v-1.8h13.8v1.8H21.4zm13.8-4.5H21.4v-1.8h13.8v1.8zm0-4.7H21.4v-1.8h13.8v1.8z'/%3E%3C/svg%3E\");background-image:-webkit-gradient(linear,left top,left bottom,from(transparent),to(transparent)),url(portal/wikipedia.org/assets/img/noimage.svg) !ie;background-image:-moz- oldlinear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/noimage.svg) !ie;background-image:-o-linear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/noimage.svg) !ie;background-image:linear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/noimage.svg) !ie;background-image:-o-linear-gradient(transparent,transparent),url(portal/wikipedia.org/assets/img/noimage.png);background-position:50%;background-repeat:no-repeat;-webkit-background-size:100% auto;-moz-background-size:100% auto;background-size:100% auto;-webkit-background-size:cover;-moz-background-size:cover;background-size:cover;height:100%;width:70px;width:7rem;position:absolute;top:0;left:0}.suggestion-title{margin:0 0 .78rem;color:#54595d;font-size:16px;font-size:1.6rem;line-height:18.72px;line-height:1.872rem}.suggestion-link.active .suggestion-title{color:#36c}.suggestion-highlight{font-style:normal;text-decoration:underline}.suggestion-description{color:#72777d;margin:0;font-size:13px;font-size:1.3rem;line-height:14.299px;line-height:1.43rem}.styled-select{display:none;position:absolute;top:10px;top:1rem;bottom:12px;bottom:1.2rem;right:12px;right:1.2rem;max-width:95px;max-width:9.5rem;height:24px;height:2.4rem;-moz-border-radius:2px;border-radius:2px}.styled-select:hover{background-color:#f8f9fa}.styled-select .hide-arrow{right:32px;right:3.2rem;max-width:68px;max-width:6.8rem;height:24px;height:2.4rem;overflow:hidden;text-align:right}.styled-select select{background:transparent;display:inline;overflow:hidden;height:24px;height:2.4rem;min-width:110px;min-width:11rem;max-width:110px;max-width:11rem;width:110px;width:11rem;outline:0;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;border:0;line-height:24px;line-height:2.4rem;-webkit-appearance:none;-moz-appearance:window;text-indent:.01px;-o-text-overflow:\"\";text-overflow:\"\";opacity:0;-moz-appearance:none;appearance:none;cursor:pointer}.styled-select.no-js{width:95px;width:9.5rem}.styled-select.no-js select{opacity:1;margin:0;padding:0 2.4rem 0 .8rem;color:#54595d}.styled-select.no-js .hide-arrow{width:68px;width:6.8rem}.search-container .styled-select.no-js .js-langpicker-label{display:none}.styled-select.js-enabled .hide-arrow{padding:0 2.4rem 0 .8rem}.styled-select.js-enabled select{background:transparent;position:absolute;top:0;left:0;height:100%;z-index:1;width:100%;border:0;margin:0;padding:0 2.4rem;color:transparent;color:hsla(0,0%,100%,0)}.styled-select.js-enabled select option{color:#54595d}.styled-select.js-enabled select:hover{background-color:transparent}.styled-select-active-helper{display:none}.styled-select.js-enabled select:focus+.styled-select-active-helper{display:block;position:absolute;top:0;left:0;z-index:0;width:100%;height:100%;outline:1px solid #36c}.search-container .js-langpicker-label{display:inline-block;margin:0;color:#54595d;font-size:13px;font-size:1.3rem;line-height:24px;line-height:2.4rem;text-transform:uppercase}.styled-select select:hover{background-color:#f8f9fa}.styled-select select::-ms-expand{display:none}.styled-select select:focus{outline:0;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none}@-moz-document url-prefix(){.styled-select select{width:110%}}.other-projects{display:inline-block;width:65%}.other-project{float:left;position:relative;width:33%;height:90px;height:9rem}.other-project-link{display:inline-block;min-height:50px;width:90%;padding:1em;white-space:nowrap}.other-project-link:hover{background-color:#eaecf0}a.other-project-link{text-decoration:none}.other-project-icon{display:inline-block;width:50px;text-align:center}.svg-Wikinews-logo_sister{background-image:url(portal/wikipedia.org/assets/img/Wikinews-logo_sister.png);background-position:0 0;-webkit-background-size:47px 26px;-moz-background-size:47px 26px;background-size:47px 26px;width:47px;height:26px}@media (-o-min-device-pixel-ratio:5/4),(-webkit-min-device-pixel-ratio:1.25),(min-resolution:120dpi){.svg-Wikinews-logo_sister{background-image:url(portal/wikipedia.org/assets/img/Wikinews-logo_sister@2x.png)}}.other-project-text,.other-project .sprite-project-logos{display:inline-block}.other-project-text{max-width:65%;font-size:14px;font-size:1.4rem;vertical-align:middle;white-space:normal}.other-project-tagline,.other-project-title{display:block}.other-project-tagline{color:#54595d;font-size:13px;font-size:1.3rem}@media screen and (max-width:768px){.other-projects{width:100%}.other-project{width:33%}}@media screen and (max-width:480px){.other-project{width:50%}.other-project-tagline{-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}}@media screen and (max-width:320px){.other-project-text{margin-right:5px;font-size:13px;font-size:1.3rem}}.lang-list-container{background-color:#f8f9fa;overflow:hidden;position:relative;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;max-height:0;width:80%;margin:-1.6rem auto 4.8rem;-webkit-transition:max-height .5s ease-out .16s,visibility .5s ease-in 1s;-o-transition:max-height .5s ease-out .16s,visibility .5s ease-in 1s;-moz-transition:max-height .5s ease-out .16s,visibility .5s ease-in 1s;transition:max-height .5s ease-out .16s,visibility .5s ease-in 1s}.js-enabled .lang-list-container{visibility:hidden}.lang-list-active .lang-list-container,.no-js .lang-list-container{visibility:visible;max-height:10000px;-webkit-transition:max-height 1s ease-in .2s,visibility 1000s ease-in 0ms;-o-transition:max-height 1s ease-in .2s,visibility 1000s ease-in 0ms;-moz-transition:max-height 1s ease-in .2s,visibility 1000s ease-in 0ms;transition:max-height 1s ease-in .2s,visibility 1000s ease-in 0ms}.no-js .lang-list-button{display:none}.lang-list-button-wrapper{text-align:center}.lang-list-button{background-color:#f8f9fa;display:inline;position:relative;z-index:1;margin:0 auto;padding:.6rem 1.2rem;outline:16px solid #fff;outline:1.6rem solid #fff;border:1px solid #a2a9b1;-moz-border-radius:2px;border-radius:2px;color:#36c;font-size:14px;font-size:1.4rem;font-weight:700;line-height:1;-webkit-transition:outline-width .1s ease-in .5s;-o-transition:outline-width .1s ease-in .5s;-moz-transition:outline-width .1s ease-in .5s;transition:outline-width .1s ease-in .5s}.lang-list-button:hover{background-color:#fff;border-color:#a2a9b1}.lang-list-button:focus{border-color:#36c;-webkit-box-shadow:inset 0 0 0 1px #36c;-moz-box-shadow:inset 0 0 0 1px #36c;box-shadow:inset 0 0 0 1px #36c}.lang-list-active .lang-list-button{background-color:#fff;outline:1px solid #fff;border-color:#72777d;-webkit-transition-delay:0s;-moz-transition-delay:0s;-o-transition-delay:0s;transition-delay:0s}.lang-list-button-text{padding:0 .64rem;vertical-align:middle}.lang-list-button i{display:inline-block;vertical-align:middle}.no-js .lang-list-border,.no-js .lang-list-button{display:none}.lang-list-border{background-color:#c8ccd1;display:block;position:relative;max-width:460px;width:80%;margin:-1.6rem auto 1.6rem;height:1px;-webkit-transition:max-width .2s ease-out .4s;-o-transition:max-width .2s ease-out .4s;-moz-transition:max-width .2s ease-out .4s;transition:max-width .2s ease-out .4s}.lang-list-active .lang-list-border{max-width:85%;-webkit-transition-delay:0s;-moz-transition-delay:0s;-o-transition-delay:0s;transition-delay:0s}.no-js .lang-list-content{padding:0}.lang-list-content{position:relative;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;width:100%;padding:1.6rem 1.6rem 0}.svg-arrow-down-blue{-webkit-transition:-webkit-transform .2s ease-out;transition:-webkit-transform .2s ease-out;-o-transition:transform .2s ease-out;-moz-transition:transform .2s ease-out,-moz-transform .2s ease-out;transition:transform .2s ease-out;transition:transform .2s ease-out,-webkit-transform .2s ease-out,-moz-transform .2s ease-out}.lang-list-active .svg-arrow-down-blue{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);transform:rotate(180deg)}.langlist{width:auto;margin:1.6rem 0;text-align:left}.langlist-others{font-weight:700;text-align:center}.hlist ul{margin:0;padding:0}.hlist li,.hlist ul ul{display:inline}.hlist li:before{content:\" Â· \";font-weight:700}.hlist li:first-child:before{content:none}.hlist li>ul:before{content:\"\\00a0(\"}.hlist li>ul:after{content:\") \"}.langlist>ul{-webkit-column-width:11.2rem;-moz-column-width:11.2rem;column-width:11.2rem}.langlist>ul>li{display:block;line-height:1.7;-webkit-column-break-inside:avoid;page-break-inside:avoid;break-inside:avoid}.no-js .langlist>ul{text-align:center;list-style-type:circle}.no-js .langlist>ul>li{display:inline-block;padding:0 .8rem}.langlist>ul>li:before{content:none}.langlist>ul>li a{white-space:normal}@media (max-width:480px){.langlist{font-size:inherit}.langlist a{word-wrap:break-word;white-space:normal}.lang-list-container{width:auto;margin-left:.8rem;margin-right:.8rem}.bookshelf{overflow:visible}}.bookshelf{display:block;border-top:1px solid #c8ccd1;-webkit-box-shadow:0 -1px 0 #fff;-moz-box-shadow:0 -1px 0 #fff;box-shadow:0 -1px 0 #fff;text-align:center;white-space:nowrap}.bookshelf .text{background-color:#f8f9fa;position:relative;top:-11.2px;top:-1.12rem;font-weight:400;padding:0 .8rem}.bookshelf-container{display:block;overflow:visible;width:100%;height:1px;margin:2.4rem 0 1.6rem;font-size:13px;font-size:1.3rem;font-weight:700;line-height:1.5}@media (max-width:480px){.bookshelf{width:auto;left:auto}.bookshelf-container{text-align:left;width:auto}}.app-badges .footer-sidebar-content{background-color:#f8f9fa}.app-badges .footer-sidebar-text{padding-top:.8rem;padding-bottom:.8rem}.app-badges .sprite.footer-sidebar-icon{top:8px;top:.8rem}.app-badges ul{margin:0;padding:0;list-style-type:none}.app-badge{display:inline-block}.app-badge a{color:transparent}@media screen and (max-width:768px){.app-badges .footer-sidebar-content{text-align:center}.app-badges .sprite.footer-sidebar-icon{display:inline-block;position:relative;margin:0;top:-3px;left:0;vertical-align:middle;-webkit-transform:scale(.7);-moz-transform:scale(.7);-ms-transform:scale(.7);transform:scale(.7)}}.footer{overflow:hidden;max-width:100%;margin:0 auto;padding:4.16rem 1.28rem 1.28rem;font-size:13px;font-size:1.3rem}.footer:after,.footer:before{content:\" \";display:table}.footer:after{clear:both}.footer-sidebar{width:35%;float:left;clear:left;margin-bottom:3.2rem;vertical-align:top}.footer-sidebar-content{position:relative;max-width:350px;margin:0 auto}.sprite.footer-sidebar-icon{position:absolute;top:0;left:8px;left:.8rem}.footer-sidebar-text{position:relative;margin:0;padding-left:6rem;padding-right:2rem;color:#54595d}.site-license{color:#54595d;text-align:center}.site-license small:after{content:\"\\2022\";display:inline-block;font-size:13px;font-size:1.3rem;line-height:inherit;margin-left:.8rem;margin-right:.5rem}.site-license small:last-child:after{display:none}@media screen and (max-width:768px){.footer{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;-moz-box-orient:vertical;-moz-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding-top:1.28rem}.footer .footer-sidebar{-webkit-box-ordinal-group:1;-moz-box-ordinal-group:1;-webkit-order:1;-ms-flex-order:1;order:1}.footer .other-projects{-webkit-box-ordinal-group:2;-moz-box-ordinal-group:2;-webkit-order:2;-ms-flex-order:2;order:2}.footer .app-badges{-webkit-box-ordinal-group:3;-moz-box-ordinal-group:3;-webkit-order:3;-ms-flex-order:3;order:3}.footer-sidebar{width:100%}.sprite.footer-sidebar-icon{display:block;position:relative;left:0;margin:0 auto 1.28rem}.footer-sidebar-content{max-width:none}.footer-sidebar-text{margin:0;padding:0;text-align:center}}@media screen and (max-width:480px){.footer{padding:.96rem .64rem 1.28rem}}@media (max-width:480px){.search-container{margin-top:0;height:78px;height:7.8rem;position:absolute;top:96px;top:9.6rem;left:0;right:0;max-width:100%;width:auto;padding:0;text-align:left}.search-container label{display:none}.search-form #searchInput{max-width:40%;vertical-align:middle}.search-form .formBtn{max-width:25%;vertical-align:middle}form fieldset{margin:0;border-left:0;border-right:0}hr{margin-top:.65rem}}@media (-o-min-device-pixel-ratio:2/1),(-webkit-min-device-pixel-ratio:2),(min--moz-device-pixel-ratio:2),(min-resolution:2dppx),(min-resolution:192dpi){hr{border-bottom-width:.5px}}@supports (-webkit-marquee-style:slide){hr{border-bottom-width:1px}}.js-enabled .central-featured,.js-enabled .jsl10n{visibility:hidden}.jsl10n-visible .central-featured,.jsl10n-visible .jsl10n{visibility:visible}@media print{body{background-color:transparent}a{color:#000!important;background:none!important;padding:0!important}a:link,a:visited{color:#520;background:transparent}img{border:0}}\n",
      "</style>\n",
      "<link rel=\"preconnect\" href=\"//upload.wikimedia.org\">\n",
      "</head>\n",
      "<body id=\"www-wikipedia-org\">\n",
      "<div class=\"central-textlogo\">\n",
      "<img class=\"central-featured-logo\" src=\"portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png\" srcset=\"portal/wikipedia.org/assets/img/Wikipedia-logo-v2@1.5x.png 1.5x, portal/wikipedia.org/assets/img/Wikipedia-logo-v2@2x.png 2x\" width=\"200\" height=\"183\" alt=\"Wikipedia\">\n",
      "<h1 class=\"central-textlogo-wrapper\">\n",
      "<span class=\"central-textlogo__image sprite svg-Wikipedia_wordmark\">\n",
      "Wikipedia\n",
      "</span>\n",
      "<strong class=\"jsl10n localized-slogan\" data-jsl10n=\"portal.slogan\">The Free Encyclopedia</strong>\n",
      "</h1>\n",
      "</div>\n",
      "<div class=\"central-featured\" data-el-section=\"primary links\">\n",
      "<!-- #1. en.wikipedia.org - 1 602 819 000 views/day -->\n",
      "<div class=\"central-featured-lang lang1\" lang=\"en\" dir=\"ltr\">\n",
      "<a id=\"js-link-box-en\" href=\"//en.wikipedia.org/\" title=\"English â Wikipedia â The Free Encyclopedia\" class=\"link-box\" data-slogan=\"The Free Encyclopedia\">\n",
      "<strong>English</strong>\n",
      "<small><bdi dir=\"ltr\">6&nbsp;458&nbsp;000+</bdi> <span>articles</span></small>\n",
      "</a>\n",
      "</div>\n",
      "<!-- #2. ja.wikipedia.org - 232 786 000 views/day -->\n",
      "<div class=\"central-featured-lang lang2\" lang=\"ja\" dir=\"ltr\">\n",
      "<a id=\"js-link-box-ja\" href=\"//ja.wikipedia.org/\" title=\"Nihongo â ã¦ã£ã­ããã£ã¢ â ããªã¼ç¾ç§äºå¸\" class=\"link-box\" data-slogan=\"ããªã¼ç¾ç§äºå¸\">\n",
      "<strong>æ¥æ¬èª</strong>\n",
      "<small><bdi dir=\"ltr\">1&nbsp;314&nbsp;000+</bdi> <span>è¨äº</span></small>\n",
      "</a>\n",
      "</div>\n",
      "<!-- #3. ru.wikipedia.org - 179 012 000 views/day -->\n",
      "<div class=\"central-featured-lang lang3\" lang=\"ru\" dir=\"ltr\">\n",
      "<a id=\"js-link-box-ru\" href=\"//ru.wikipedia.org/\" title=\"Russkiy â ÐÐ¸ÐºÐ¸Ð¿ÐµÐ´Ð¸Ñ â Ð¡Ð²Ð¾Ð±Ð¾Ð´Ð½Ð°Ñ ÑÐ½ÑÐ¸ÐºÐ»Ð¾Ð¿ÐµÐ´Ð¸Ñ\" class=\"link-box\" data-slogan=\"Ð¡Ð²Ð¾Ð±Ð¾Ð´Ð½Ð°Ñ ÑÐ½ÑÐ¸ÐºÐ»Ð¾Ð¿ÐµÐ´Ð¸Ñ\">\n",
      "<strong>Ð ÑÑÑÐºÐ¸Ð¹</strong>\n",
      "<small><bdi dir=\"ltr\">1&nbsp;798&nbsp;000+</bdi> <span>ÑÑÐ°ÑÐµÐ¹</span></small>\n",
      "</a>\n",
      "</div>\n",
      "<!-- #4. de.wikipedia.org - 176 185 000 views/day -->\n",
      "<div class=\"central-featured-lang lang4\" lang=\"de\" dir=\"ltr\">\n",
      "<a id=\"js-link-box-de\" href=\"//de.wikipedia.org/\" title=\"Deutsch â Wikipedia â Die freie EnzyklopÃ¤die\" class=\"link-box\" data-slogan=\"Die freie EnzyklopÃ¤die\">\n",
      "<strong>Deutsch</strong>\n",
      "<small><bdi dir=\"ltr\">2&nbsp;667&nbsp;000+</bdi> <span>Artikel</span></small>\n",
      "</a>\n",
      "</div>\n",
      "<!-- #5. es.wikipedia.org - 172 935 000 views/day -->\n",
      "<div class=\"central-featured-lang lang5\" lang=\"es\" dir=\"ltr\">\n",
      "<a id=\"js-link-box-es\" href=\"//es.wikipedia.org/\" title=\"EspaÃ±ol â Wikipedia â La enciclopedia libre\" class=\"link-box\" data-slogan=\"La enciclopedia libre\">\n",
      "<strong>EspaÃ±ol</strong>\n",
      "<small><bdi dir=\"ltr\">1&nbsp;755&nbsp;000+</bdi> <span>artÃ­culos</span></small>\n",
      "</a>\n",
      "</div>\n",
      "<!-- #6. fr.wikipedia.org - 162 045 000 views/day -->\n",
      "<div class=\"central-featured-lang lang6\" lang=\"fr\" dir=\"ltr\">\n",
      "<a id=\"js-link-box-fr\" href=\"//fr.wikipedia.org/\" title=\"franÃ§ais â WikipÃ©dia â LâencyclopÃ©die libre\" class=\"link-box\" data-slogan=\"LâencyclopÃ©die libre\">\n",
      "<strong>FranÃ§ais</strong>\n",
      "<small><bdi dir=\"ltr\">2&nbsp;400&nbsp;000+</bdi> <span>articles</span></small>\n",
      "</a>\n",
      "</div>\n",
      "<!-- #7. it.wikipedia.org - 95 412 000 views/day -->\n",
      "<div class=\"central-featured-lang lang7\" lang=\"it\" dir=\"ltr\">\n",
      "<a id=\"js-link-box-it\" href=\"//it.wikipedia.org/\" title=\"Italiano â Wikipedia â L&#x27;enciclopedia libera\" class=\"link-box\" data-slogan=\"L&#x27;enciclopedia libera\">\n",
      "<strong>Italiano</strong>\n",
      "<small><bdi dir=\"ltr\">1&nbsp;742&nbsp;000+</bdi> <span>voci</span></small>\n",
      "</a>\n",
      "</div>\n",
      "<!-- #8. zh.wikipedia.org - 87 143 000 views/day -->\n",
      "<div class=\"central-featured-lang lang8\" lang=\"zh\" dir=\"ltr\">\n",
      "<a id=\"js-link-box-zh\" href=\"//zh.wikipedia.org/\" title=\"ZhÅngwÃ©n â ç»´åºç¾ç§ / ç¶­åºç¾ç§ â èªç±çç¾ç§å¨ä¹¦ / èªç±çç¾ç§å¨æ¸\" class=\"link-box jscnconv\" data-title-hans=\"ZhÅngwÃ©n â ç»´åºç¾ç§ â èªç±çç¾ç§å¨ä¹¦\" data-title-hant=\"ZhÅngwÃ©n â ç¶­åºç¾ç§ â èªç±çç¾ç§å¨æ¸\" data-slogan=\"èªç±çç¾ç§å¨ä¹¦ / èªç±çç¾ç§å¨æ¸\">\n",
      "<strong>ä¸­æ</strong>\n",
      "<small><bdi dir=\"ltr\">1&nbsp;256&nbsp;000+</bdi> <span data-hans=\"æ¡ç®\" data-hant=\"æ¢ç®\" class=\"jscnconv\">æ¡ç® / æ¢ç®</span></small>\n",
      "</a>\n",
      "</div>\n",
      "<!-- #9. pt.wikipedia.org - 51 172 000 views/day -->\n",
      "<div class=\"central-featured-lang lang9\" lang=\"pt\" dir=\"ltr\">\n",
      "<a id=\"js-link-box-pt\" href=\"//pt.wikipedia.org/\" title=\"PortuguÃªs â WikipÃ©dia â A enciclopÃ©dia livre\" class=\"link-box\" data-slogan=\"A enciclopÃ©dia livre\">\n",
      "<strong>PortuguÃªs</strong>\n",
      "<small><bdi dir=\"ltr\">1&nbsp;085&nbsp;000+</bdi> <span>artigos</span></small>\n",
      "</a>\n",
      "</div>\n",
      "<!-- #10. pl.wikipedia.org - 42 752 000 views/day -->\n",
      "<div class=\"central-featured-lang lang10\" lang=\"pl\" dir=\"ltr\">\n",
      "<a id=\"js-link-box-pl\" href=\"//pl.wikipedia.org/\" title=\"Polski â Wikipedia â Wolna encyklopedia\" class=\"link-box\" data-slogan=\"Wolna encyklopedia\">\n",
      "<strong>Polski</strong>\n",
      "<small><bdi dir=\"ltr\">1&nbsp;512&nbsp;000+</bdi> <span>haseÅ</span></small>\n",
      "</a>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"search-container\">\n",
      "<form class=\"pure-form\" id=\"search-form\" action=\"//www.wikipedia.org/search-redirect.php\" data-el-section=\"search\">\n",
      "<fieldset>\n",
      "<input type=\"hidden\" name=\"family\" value=\"Wikipedia\">\n",
      "<input type=\"hidden\" id=\"hiddenLanguageInput\" name=\"language\" value=\"en\">\n",
      "<div class=\"search-input\" id=\"search-input\">\n",
      "<label for=\"searchInput\" class=\"screen-reader-text\" data-jsl10n=\"portal.search-input-label\">Search Wikipedia</label>\n",
      "<input id=\"searchInput\" name=\"search\" type=\"search\" size=\"20\" autofocus=\"autofocus\" accesskey=\"F\" dir=\"auto\" autocomplete=\"off\">\n",
      "<div class=\"styled-select no-js\">\n",
      "<div class=\"hide-arrow\">\n",
      "<select id=\"searchLanguage\" name=\"language\">\n",
      "<option value=\"af\" lang=\"af\">Afrikaans</option>\n",
      "<option value=\"pl\" lang=\"pl\">Polski</option>\n",
      "<option value=\"ar\" lang=\"ar\">Ø§ÙØ¹Ø±Ø¨ÙØ©</option><!-- Al-Ê¿ArabÄ«yah -->\n",
      "<option value=\"ast\" lang=\"ast\">Asturianu</option>\n",
      "<option value=\"az\" lang=\"az\">AzÉrbaycanca</option>\n",
      "<option value=\"bg\" lang=\"bg\">ÐÑÐ»Ð³Ð°ÑÑÐºÐ¸</option><!-- BÇlgarski -->\n",
      "<option value=\"nan\" lang=\"nan\">BÃ¢n-lÃ¢m-gÃº / HÅ-lÃ³-oÄ</option>\n",
      "<option value=\"bn\" lang=\"bn\">à¦¬à¦¾à¦à¦²à¦¾</option><!-- Bangla -->\n",
      "<option value=\"be\" lang=\"be\">ÐÐµÐ»Ð°ÑÑÑÐºÐ°Ñ</option><!-- Belaruskaya -->\n",
      "<option value=\"ca\" lang=\"ca\">CatalÃ </option>\n",
      "<option value=\"cs\" lang=\"cs\">ÄeÅ¡tina</option><!-- ÄeÅ¡tina -->\n",
      "<option value=\"cy\" lang=\"cy\">Cymraeg</option><!-- Cymraeg -->\n",
      "<option value=\"da\" lang=\"da\">Dansk</option>\n",
      "<option value=\"de\" lang=\"de\">Deutsch</option>\n",
      "<option value=\"et\" lang=\"et\">Eesti</option>\n",
      "<option value=\"el\" lang=\"el\">ÎÎ»Î»Î·Î½Î¹ÎºÎ¬</option><!-- EllÄ«nikÃ¡ -->\n",
      "<option value=\"en\" lang=\"en\" selected=selected>English</option><!-- English -->\n",
      "<option value=\"es\" lang=\"es\">EspaÃ±ol</option>\n",
      "<option value=\"eo\" lang=\"eo\">Esperanto</option>\n",
      "<option value=\"eu\" lang=\"eu\">Euskara</option>\n",
      "<option value=\"fa\" lang=\"fa\">ÙØ§Ø±Ø³Û</option><!-- FÄrsi -->\n",
      "<option value=\"fr\" lang=\"fr\">FranÃ§ais</option><!-- franÃ§ais -->\n",
      "<option value=\"gl\" lang=\"gl\">Galego</option>\n",
      "<option value=\"ko\" lang=\"ko\">íêµ­ì´</option><!-- Hangugeo -->\n",
      "<option value=\"hy\" lang=\"hy\">ÕÕ¡ÕµÕ¥ÖÕ¥Õ¶</option><!-- Hayeren -->\n",
      "<option value=\"hi\" lang=\"hi\">à¤¹à¤¿à¤¨à¥à¤¦à¥</option><!-- HindÄ« -->\n",
      "<option value=\"hr\" lang=\"hr\">Hrvatski</option>\n",
      "<option value=\"id\" lang=\"id\">Bahasa Indonesia</option>\n",
      "<option value=\"it\" lang=\"it\">Italiano</option>\n",
      "<option value=\"he\" lang=\"he\">×¢××¨××ª</option><!-- Ivrit -->\n",
      "<option value=\"ka\" lang=\"ka\">á¥áá áá£áá</option><!-- Kartuli -->\n",
      "<option value=\"la\" lang=\"la\">Latina</option>\n",
      "<option value=\"lv\" lang=\"lv\">LatvieÅ¡u</option>\n",
      "<option value=\"lt\" lang=\"lt\">LietuviÅ³</option>\n",
      "<option value=\"hu\" lang=\"hu\">Magyar</option>\n",
      "<option value=\"mk\" lang=\"mk\">ÐÐ°ÐºÐµÐ´Ð¾Ð½ÑÐºÐ¸</option><!-- Makedonski -->\n",
      "<option value=\"arz\" lang=\"arz\">ÙØµØ±Ù</option><!-- Maá¹£rÄ« -->\n",
      "<option value=\"ms\" lang=\"ms\">Bahasa Melayu</option>\n",
      "<option value=\"min\" lang=\"min\">Bahaso Minangkabau</option>\n",
      "<option value=\"my\" lang=\"my\">áá¼ááºáá¬áá¬áá¬</option><!-- Myanmarsar -->\n",
      "<option value=\"nl\" lang=\"nl\">Nederlands</option>\n",
      "<option value=\"ja\" lang=\"ja\">æ¥æ¬èª</option><!-- Nihongo -->\n",
      "<option value=\"no\" lang=\"nb\">Norsk (bokmÃ¥l)</option>\n",
      "<option value=\"nn\" lang=\"nn\">Norsk (nynorsk)</option>\n",
      "<option value=\"ce\" lang=\"ce\">ÐÐ¾ÑÑÐ¸Ð¹Ð½</option><!-- NoxÃ§iyn -->\n",
      "<option value=\"uz\" lang=\"uz\">OÊ»zbekcha / ÐÐ·Ð±ÐµÐºÑÐ°</option><!-- OÊ»zbekcha -->\n",
      "<option value=\"pt\" lang=\"pt\">PortuguÃªs</option>\n",
      "<option value=\"kk\" lang=\"kk\">ÒÐ°Ð·Ð°ÒÑÐ° / QazaqÅa / ÙØ§Ø²Ø§ÙØ´Ø§</option>\n",
      "<option value=\"ro\" lang=\"ro\">RomÃ¢nÄ</option><!-- RomÃ¢nÄ -->\n",
      "<option value=\"ru\" lang=\"ru\">Ð ÑÑÑÐºÐ¸Ð¹</option><!-- Russkiy -->\n",
      "<option value=\"simple\" lang=\"en\">Simple English</option>\n",
      "<option value=\"ceb\" lang=\"ceb\">Sinugboanong Binisaya</option>\n",
      "<option value=\"sk\" lang=\"sk\">SlovenÄina</option>\n",
      "<option value=\"sl\" lang=\"sl\">SlovenÅ¡Äina</option>\n",
      "<option value=\"sr\" lang=\"sr\">Ð¡ÑÐ¿ÑÐºÐ¸ / Srpski</option>\n",
      "<option value=\"sh\" lang=\"sh\">Srpskohrvatski / Ð¡ÑÐ¿ÑÐºÐ¾ÑÑÐ²Ð°ÑÑÐºÐ¸</option>\n",
      "<option value=\"fi\" lang=\"fi\">Suomi</option><!-- suomi -->\n",
      "<option value=\"sv\" lang=\"sv\">Svenska</option>\n",
      "<option value=\"ta\" lang=\"ta\">à®¤à®®à®¿à®´à¯</option><!-- Tamiá¸» -->\n",
      "<option value=\"tt\" lang=\"tt\">Ð¢Ð°ÑÐ°ÑÑÐ° / TatarÃ§a</option>\n",
      "<option value=\"th\" lang=\"th\">à¸ à¸²à¸©à¸²à¹à¸à¸¢</option><!-- Phasa Thai -->\n",
      "<option value=\"tg\" lang=\"tg\">Ð¢Ð¾Ò·Ð¸ÐºÓ£</option><!-- TojikÄ« -->\n",
      "<option value=\"azb\" lang=\"azb\">ØªÛØ±Ú©Ø¬Ù</option><!-- TÃ¼rkce -->\n",
      "<option value=\"tr\" lang=\"tr\">TÃ¼rkÃ§e</option><!-- TÃ¼rkÃ§e -->\n",
      "<option value=\"uk\" lang=\"uk\">Ð£ÐºÑÐ°ÑÐ½ÑÑÐºÐ°</option><!-- Ukrayinsâka -->\n",
      "<option value=\"ur\" lang=\"ur\">Ø§Ø±Ø¯Ù</option><!-- Urdu -->\n",
      "<option value=\"vi\" lang=\"vi\">Tiáº¿ng Viá»t</option>\n",
      "<option value=\"vo\" lang=\"vo\">VolapÃ¼k</option>\n",
      "<option value=\"war\" lang=\"war\">Winaray</option>\n",
      "<option value=\"zh-yue\" lang=\"yue\">ç²µèª</option><!-- Yuht YÃºh / Jyut6 jyu5 -->\n",
      "<option value=\"zh\" lang=\"zh\">ä¸­æ</option><!-- ZhÅngwÃ©n -->\n",
      "</select>\n",
      "<div class=\"styled-select-active-helper\"></div>\n",
      "</div>\n",
      "<i class=\"sprite svg-arrow-down\"></i>\n",
      "</div>\n",
      "</div>\n",
      "<button class=\"pure-button pure-button-primary-progressive\" type=\"submit\">\n",
      "<i class=\"sprite svg-search-icon\" data-jsl10n=\"search-input-button\">Search</i>\n",
      "</button>\n",
      "<input type=\"hidden\" value=\"Go\" name=\"go\">\n",
      "</fieldset>\n",
      "</form>\n",
      "</div>\n",
      "<div class=\"lang-list-button-wrapper\">\n",
      "<button id=\"js-lang-list-button\" class=\"lang-list-button\">\n",
      "<i class=\"sprite svg-language-icon\"></i>\n",
      "<span class=\"lang-list-button-text jsl10n\" data-jsl10n=\"portal.language-button-text\">Read Wikipedia in your language </span>\n",
      "<i class=\"sprite svg-arrow-down-blue\"></i>\n",
      "</button>\n",
      "</div>\n",
      "<div class=\"lang-list-border\"></div>\n",
      "<div class=\"lang-list-container\">\n",
      "<div id=\"js-lang-lists\" class=\"lang-list-content\">\n",
      "<h2 class=\"bookshelf-container\">\n",
      "<span class=\"bookshelf\">\n",
      "<span class=\"text\">\n",
      "<bdi dir=\"ltr\">\n",
      "1&nbsp;000&nbsp;000+\n",
      "</bdi>\n",
      "<span class=\"jsl10n\" data-jsl10n=\"entries\">\n",
      "articles\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</h2>\n",
      "<div class=\"langlist langlist-large hlist\" data-el-section=\"secondary links\">\n",
      "<ul>\n",
      "<li><a href=\"//pl.wikipedia.org/\" lang=\"pl\">Polski</a></li>\n",
      "<li><a href=\"//ar.wikipedia.org/\" lang=\"ar\" title=\"Al-Ê¿ArabÄ«yah\"><bdi dir=\"rtl\">Ø§ÙØ¹Ø±Ø¨ÙØ©</bdi></a></li>\n",
      "<li><a href=\"//de.wikipedia.org/\" lang=\"de\">Deutsch</a></li>\n",
      "<li><a href=\"//en.wikipedia.org/\" lang=\"en\" title=\"English\">English</a></li>\n",
      "<li><a href=\"//es.wikipedia.org/\" lang=\"es\">EspaÃ±ol</a></li>\n",
      "<li><a href=\"//fr.wikipedia.org/\" lang=\"fr\" title=\"franÃ§ais\">FranÃ§ais</a></li>\n",
      "<li><a href=\"//it.wikipedia.org/\" lang=\"it\">Italiano</a></li>\n",
      "<li><a href=\"//arz.wikipedia.org/\" lang=\"arz\" title=\"Maá¹£rÄ«\"><bdi dir=\"rtl\">ÙØµØ±Ù</bdi></a></li>\n",
      "<li><a href=\"//nl.wikipedia.org/\" lang=\"nl\">Nederlands</a></li>\n",
      "<li><a href=\"//ja.wikipedia.org/\" lang=\"ja\" title=\"Nihongo\">æ¥æ¬èª</a></li>\n",
      "<li><a href=\"//pt.wikipedia.org/\" lang=\"pt\">PortuguÃªs</a></li>\n",
      "<li><a href=\"//ru.wikipedia.org/\" lang=\"ru\" title=\"Russkiy\">Ð ÑÑÑÐºÐ¸Ð¹</a></li>\n",
      "<li><a href=\"//ceb.wikipedia.org/\" lang=\"ceb\">Sinugboanong Binisaya</a></li>\n",
      "<li><a href=\"//sv.wikipedia.org/\" lang=\"sv\">Svenska</a></li>\n",
      "<li><a href=\"//uk.wikipedia.org/\" lang=\"uk\" title=\"Ukrayinsâka\">Ð£ÐºÑÐ°ÑÐ½ÑÑÐºÐ°</a></li>\n",
      "<li><a href=\"//vi.wikipedia.org/\" lang=\"vi\">Tiáº¿ng Viá»t</a></li>\n",
      "<li><a href=\"//war.wikipedia.org/\" lang=\"war\">Winaray</a></li>\n",
      "<li><a href=\"//zh.wikipedia.org/\" lang=\"zh\" title=\"ZhÅngwÃ©n\">ä¸­æ</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "<h2 class=\"bookshelf-container\">\n",
      "<span class=\"bookshelf\">\n",
      "<span class=\"text\">\n",
      "<bdi dir=\"ltr\">\n",
      "100&nbsp;000+\n",
      "</bdi>\n",
      "<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\n",
      "articles\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</h2>\n",
      "<div class=\"langlist langlist-large hlist\" data-el-section=\"secondary links\">\n",
      "<ul>\n",
      "<li><a href=\"//af.wikipedia.org/\" lang=\"af\">Afrikaans</a></li>\n",
      "<li><a href=\"//ast.wikipedia.org/\" lang=\"ast\">Asturianu</a></li>\n",
      "<li><a href=\"//az.wikipedia.org/\" lang=\"az\">AzÉrbaycanca</a></li>\n",
      "<li><a href=\"//bg.wikipedia.org/\" lang=\"bg\" title=\"BÇlgarski\">ÐÑÐ»Ð³Ð°ÑÑÐºÐ¸</a></li>\n",
      "<li><a href=\"//zh-min-nan.wikipedia.org/\" lang=\"nan\">BÃ¢n-lÃ¢m-gÃº / HÅ-lÃ³-oÄ</a></li>\n",
      "<li><a href=\"//bn.wikipedia.org/\" lang=\"bn\" title=\"Bangla\">à¦¬à¦¾à¦à¦²à¦¾</a></li>\n",
      "<li><a href=\"//be.wikipedia.org/\" lang=\"be\" title=\"Belaruskaya\">ÐÐµÐ»Ð°ÑÑÑÐºÐ°Ñ</a></li>\n",
      "<li><a href=\"//ca.wikipedia.org/\" lang=\"ca\">CatalÃ </a></li>\n",
      "<li><a href=\"//cs.wikipedia.org/\" lang=\"cs\" title=\"ÄeÅ¡tina\">ÄeÅ¡tina</a></li>\n",
      "<li><a href=\"//cy.wikipedia.org/\" lang=\"cy\" title=\"Cymraeg\">Cymraeg</a></li>\n",
      "<li><a href=\"//da.wikipedia.org/\" lang=\"da\">Dansk</a></li>\n",
      "<li><a href=\"//et.wikipedia.org/\" lang=\"et\">Eesti</a></li>\n",
      "<li><a href=\"//el.wikipedia.org/\" lang=\"el\" title=\"EllÄ«nikÃ¡\">ÎÎ»Î»Î·Î½Î¹ÎºÎ¬</a></li>\n",
      "<li><a href=\"//eo.wikipedia.org/\" lang=\"eo\">Esperanto</a></li>\n",
      "<li><a href=\"//eu.wikipedia.org/\" lang=\"eu\">Euskara</a></li>\n",
      "<li><a href=\"//fa.wikipedia.org/\" lang=\"fa\" title=\"FÄrsi\"><bdi dir=\"rtl\">ÙØ§Ø±Ø³Û</bdi></a></li>\n",
      "<li><a href=\"//gl.wikipedia.org/\" lang=\"gl\">Galego</a></li>\n",
      "<li><a href=\"//ko.wikipedia.org/\" lang=\"ko\" title=\"Hangugeo\">íêµ­ì´</a></li>\n",
      "<li><a href=\"//hy.wikipedia.org/\" lang=\"hy\" title=\"Hayeren\">ÕÕ¡ÕµÕ¥ÖÕ¥Õ¶</a></li>\n",
      "<li><a href=\"//hi.wikipedia.org/\" lang=\"hi\" title=\"HindÄ«\">à¤¹à¤¿à¤¨à¥à¤¦à¥</a></li>\n",
      "<li><a href=\"//hr.wikipedia.org/\" lang=\"hr\">Hrvatski</a></li>\n",
      "<li><a href=\"//id.wikipedia.org/\" lang=\"id\">Bahasa Indonesia</a></li>\n",
      "<li><a href=\"//he.wikipedia.org/\" lang=\"he\" title=\"Ivrit\"><bdi dir=\"rtl\">×¢××¨××ª</bdi></a></li>\n",
      "<li><a href=\"//ka.wikipedia.org/\" lang=\"ka\" title=\"Kartuli\">á¥áá áá£áá</a></li>\n",
      "<li><a href=\"//la.wikipedia.org/\" lang=\"la\">Latina</a></li>\n",
      "<li><a href=\"//lv.wikipedia.org/\" lang=\"lv\">LatvieÅ¡u</a></li>\n",
      "<li><a href=\"//lt.wikipedia.org/\" lang=\"lt\">LietuviÅ³</a></li>\n",
      "<li><a href=\"//hu.wikipedia.org/\" lang=\"hu\">Magyar</a></li>\n",
      "<li><a href=\"//mk.wikipedia.org/\" lang=\"mk\" title=\"Makedonski\">ÐÐ°ÐºÐµÐ´Ð¾Ð½ÑÐºÐ¸</a></li>\n",
      "<li><a href=\"//ms.wikipedia.org/\" lang=\"ms\">Bahasa Melayu</a></li>\n",
      "<li><a href=\"//min.wikipedia.org/\" lang=\"min\">Bahaso Minangkabau</a></li>\n",
      "<li><a href=\"//my.wikipedia.org/\" lang=\"my\" title=\"Myanmarsar\">áá¼ááºáá¬áá¬áá¬</a></li>\n",
      "<li lang=\"no\">Norsk<ul><li><a href=\"//no.wikipedia.org/\" lang=\"nb\">bokmÃ¥l</a></li><li><a href=\"//nn.wikipedia.org/\" lang=\"nn\">nynorsk</a></li></ul></li>\n",
      "<li><a href=\"//ce.wikipedia.org/\" lang=\"ce\" title=\"NoxÃ§iyn\">ÐÐ¾ÑÑÐ¸Ð¹Ð½</a></li>\n",
      "<li><a href=\"//uz.wikipedia.org/\" lang=\"uz\" title=\"OÊ»zbekcha\">OÊ»zbekcha / ÐÐ·Ð±ÐµÐºÑÐ°</a></li>\n",
      "<li><a href=\"//kk.wikipedia.org/\" lang=\"kk\"><span lang=\"kk-Cyrl\">ÒÐ°Ð·Ð°ÒÑÐ°</span> / <span lang=\"kk-Latn\">QazaqÅa</span> / <bdi lang=\"kk-Arab\" dir=\"rtl\">ÙØ§Ø²Ø§ÙØ´Ø§</bdi></a></li>\n",
      "<li><a href=\"//ro.wikipedia.org/\" lang=\"ro\" title=\"RomÃ¢nÄ\">RomÃ¢nÄ</a></li>\n",
      "<li><a href=\"//simple.wikipedia.org/\" lang=\"en\">Simple English</a></li>\n",
      "<li><a href=\"//sk.wikipedia.org/\" lang=\"sk\">SlovenÄina</a></li>\n",
      "<li><a href=\"//sl.wikipedia.org/\" lang=\"sl\">SlovenÅ¡Äina</a></li>\n",
      "<li><a href=\"//sr.wikipedia.org/\" lang=\"sr\">Ð¡ÑÐ¿ÑÐºÐ¸ / Srpski</a></li>\n",
      "<li><a href=\"//sh.wikipedia.org/\" lang=\"sh\">Srpskohrvatski / Ð¡ÑÐ¿ÑÐºÐ¾ÑÑÐ²Ð°ÑÑÐºÐ¸</a></li>\n",
      "<li><a href=\"//fi.wikipedia.org/\" lang=\"fi\" title=\"suomi\">Suomi</a></li>\n",
      "<li><a href=\"//ta.wikipedia.org/\" lang=\"ta\" title=\"Tamiá¸»\">à®¤à®®à®¿à®´à¯</a></li>\n",
      "<li><a href=\"//tt.wikipedia.org/\" lang=\"tt\">Ð¢Ð°ÑÐ°ÑÑÐ° / TatarÃ§a</a></li>\n",
      "<li><a href=\"//th.wikipedia.org/\" lang=\"th\" title=\"Phasa Thai\">à¸ à¸²à¸©à¸²à¹à¸à¸¢</a></li>\n",
      "<li><a href=\"//tg.wikipedia.org/\" lang=\"tg\" title=\"TojikÄ«\">Ð¢Ð¾Ò·Ð¸ÐºÓ£</a></li>\n",
      "<li><a href=\"//azb.wikipedia.org/\" lang=\"azb\" title=\"TÃ¼rkce\"><bdi dir=\"rtl\">ØªÛØ±Ú©Ø¬Ù</bdi></a></li>\n",
      "<li><a href=\"//tr.wikipedia.org/\" lang=\"tr\" title=\"TÃ¼rkÃ§e\">TÃ¼rkÃ§e</a></li>\n",
      "<li><a href=\"//ur.wikipedia.org/\" lang=\"ur\" title=\"Urdu\"><bdi dir=\"rtl\">Ø§Ø±Ø¯Ù</bdi></a></li>\n",
      "<li><a href=\"//vo.wikipedia.org/\" lang=\"vo\">VolapÃ¼k</a></li>\n",
      "<li><a href=\"//zh-yue.wikipedia.org/\" lang=\"yue\" title=\"Yuht YÃºh / Jyut6 jyu5\">ç²µèª</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "<h2 class=\"bookshelf-container\">\n",
      "<span class=\"bookshelf\">\n",
      "<span class=\"text\">\n",
      "<bdi dir=\"ltr\">\n",
      "10&nbsp;000+\n",
      "</bdi>\n",
      "<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\n",
      "articles\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</h2>\n",
      "<div class=\"langlist hlist\" data-el-section=\"secondary links\">\n",
      "<ul>\n",
      "<li><a href=\"//ace.wikipedia.org/\" lang=\"ace\">Bahsa AcÃ¨h</a></li>\n",
      "<li><a href=\"//als.wikipedia.org/\" lang=\"gsw\">Alemannisch</a></li>\n",
      "<li><a href=\"//am.wikipedia.org/\" lang=\"am\" title=\"ÄmariÃ±Ã±Ä\">á áá­á</a></li>\n",
      "<li><a href=\"//an.wikipedia.org/\" lang=\"an\">AragonÃ©s</a></li>\n",
      "<li><a href=\"//ban.wikipedia.org/\" lang=\"ban\" title=\"Basa Bali\">Basa Bali</a></li>\n",
      "<li><a href=\"//map-bms.wikipedia.org/\" lang=\"map-x-bms\">Basa Banyumasan</a></li>\n",
      "<li><a href=\"//ba.wikipedia.org/\" lang=\"ba\" title=\"BaÅqortsa\">ÐÐ°ÑÒ¡Ð¾ÑÑÑÐ°</a></li>\n",
      "<li><a href=\"//be-tarask.wikipedia.org/\" lang=\"be\" title=\"Belaruskaya (TaraÅ¡kievica)\">ÐÐµÐ»Ð°ÑÑÑÐºÐ°Ñ (Ð¢Ð°ÑÐ°ÑÐºÐµÐ²ÑÑÐ°)</a></li>\n",
      "<li><a href=\"//bcl.wikipedia.org/\" lang=\"bcl\">Bikol Central</a></li>\n",
      "<li><a href=\"//bpy.wikipedia.org/\" lang=\"bpy\" title=\"Bishnupriya Manipuri\">à¦¬à¦¿à¦·à§à¦£à§à¦ªà§à¦°à¦¿à¦¯à¦¼à¦¾ à¦®à¦£à¦¿à¦ªà§à¦°à§</a></li>\n",
      "<li><a href=\"//bar.wikipedia.org/\" lang=\"bar\">Boarisch</a></li>\n",
      "<li><a href=\"//bs.wikipedia.org/\" lang=\"bs\">Bosanski</a></li>\n",
      "<li><a href=\"//br.wikipedia.org/\" lang=\"br\">Brezhoneg</a></li>\n",
      "<li><a href=\"//cv.wikipedia.org/\" lang=\"cv\" title=\"ÄÄvaÅ¡la\">Ð§ÓÐ²Ð°ÑÐ»Ð°</a></li>\n",
      "<li><a href=\"//nv.wikipedia.org/\" lang=\"nv\">DinÃ© Bizaad</a></li>\n",
      "<li><a href=\"//eml.wikipedia.org/\" lang=\"roa-x-eml\">EmigliÃ nâRumagnÃ²l</a></li>\n",
      "<li><a href=\"//hif.wikipedia.org/\" lang=\"hif\">Fiji Hindi</a></li>\n",
      "<li><a href=\"//fo.wikipedia.org/\" lang=\"fo\">FÃ¸royskt</a></li>\n",
      "<li><a href=\"//fy.wikipedia.org/\" lang=\"fy\">Frysk</a></li>\n",
      "<li><a href=\"//ga.wikipedia.org/\" lang=\"ga\">Gaeilge</a></li>\n",
      "<li><a href=\"//gd.wikipedia.org/\" lang=\"gd\">GÃ idhlig</a></li>\n",
      "<li><a href=\"//gu.wikipedia.org/\" lang=\"gu\" title=\"Gujarati\">àªà«àªàª°àª¾àª¤à«</a></li>\n",
      "<li><a href=\"//ha.wikipedia.org/\" lang=\"ha\" title=\"Hausa\">Hausa</a></li>\n",
      "<li><a href=\"//hsb.wikipedia.org/\" lang=\"hsb\">Hornjoserbsce</a></li>\n",
      "<li><a href=\"//io.wikipedia.org/\" lang=\"io\" title=\"Ido\">Ido</a></li>\n",
      "<li><a href=\"//ilo.wikipedia.org/\" lang=\"ilo\">Ilokano</a></li>\n",
      "<li><a href=\"//ia.wikipedia.org/\" lang=\"ia\">Interlingua</a></li>\n",
      "<li><a href=\"//os.wikipedia.org/\" lang=\"os\" title=\"Iron\">ÐÑÐ¾Ð½</a></li>\n",
      "<li><a href=\"//is.wikipedia.org/\" lang=\"is\">Ãslenska</a></li>\n",
      "<li><a href=\"//jv.wikipedia.org/\" lang=\"jv\" title=\"Jawa\">Jawa</a></li>\n",
      "<li><a href=\"//kn.wikipedia.org/\" lang=\"kn\" title=\"Kannada\">à²à²¨à³à²¨à²¡</a></li>\n",
      "<li><a href=\"//ht.wikipedia.org/\" lang=\"ht\">KreyÃ²l Ayisyen</a></li>\n",
      "<li><a href=\"//ku.wikipedia.org/\" lang=\"ku\"><span lang=\"ku-Latn\">KurdÃ®</span> / <bdi lang=\"ku-Arab\" dir=\"rtl\">ÙÙØ±Ø¯Û</bdi></a></li>\n",
      "<li><a href=\"//ckb.wikipedia.org/\" lang=\"ckb\" title=\"KurdÃ®y NawendÃ®\"><bdi dir=\"rtl\">Ú©ÙØ±Ø¯ÛÛ ÙØ§ÙÛÙØ¯Û</bdi></a></li>\n",
      "<li><a href=\"//ky.wikipedia.org/\" lang=\"ky\" title=\"KyrgyzÄa\">ÐÑÑÐ³ÑÐ·ÑÐ°</a></li>\n",
      "<li><a href=\"//mrj.wikipedia.org/\" lang=\"mjr\" title=\"Kyryk Mary\">ÐÑÑÑÐº Ð¼Ð°ÑÑ</a></li>\n",
      "<li><a href=\"//lb.wikipedia.org/\" lang=\"lb\">LÃ«tzebuergesch</a></li>\n",
      "<li><a href=\"//lij.wikipedia.org/\" lang=\"lij\">LÃ¬gure</a></li>\n",
      "<li><a href=\"//li.wikipedia.org/\" lang=\"li\">Limburgs</a></li>\n",
      "<li><a href=\"//lmo.wikipedia.org/\" lang=\"lmo\">Lombard</a></li>\n",
      "<li><a href=\"//mai.wikipedia.org/\" lang=\"mai\" title=\"MaithilÄ«\">à¤®à¥à¤¥à¤¿à¤²à¥</a></li>\n",
      "<li><a href=\"//mg.wikipedia.org/\" lang=\"mg\">Malagasy</a></li>\n",
      "<li><a href=\"//ml.wikipedia.org/\" lang=\"ml\" title=\"Malayalam\">à´®à´²à´¯à´¾à´³à´</a></li>\n",
      "<li><a href=\"//zh-classical.wikipedia.org/\" lang=\"lzh\" title=\"Man4jin4 / WÃ©nyÃ¡n\">æè¨</a></li>\n",
      "<li><a href=\"//mr.wikipedia.org/\" lang=\"mr\" title=\"Marathi\">à¤®à¤°à¤¾à¤ à¥</a></li>\n",
      "<li><a href=\"//xmf.wikipedia.org/\" lang=\"xmf\" title=\"Margaluri\">ááá áááá£á á</a></li>\n",
      "<li><a href=\"//mzn.wikipedia.org/\" lang=\"mzn\" title=\"MÃ¤zeruni\"><bdi dir=\"rtl\">ÙØ§Ø²ÙØ±ÙÙÛ</bdi></a></li>\n",
      "<li><a href=\"//cdo.wikipedia.org/\" lang=\"cdo\" title=\"Ming-deng-ngu\">MÃ¬ng-dÄÌ¤ng-ngá¹³Ì / é©æ±èª</a></li>\n",
      "<li><a href=\"//mn.wikipedia.org/\" lang=\"mn\" title=\"Mongol\">ÐÐ¾Ð½Ð³Ð¾Ð»</a></li>\n",
      "<li><a href=\"//nap.wikipedia.org/\" lang=\"nap\">Napulitano</a></li>\n",
      "<li><a href=\"//new.wikipedia.org/\" lang=\"new\" title=\"Nepal Bhasa\">à¤¨à¥à¤ªà¤¾à¤² à¤­à¤¾à¤·à¤¾</a></li>\n",
      "<li><a href=\"//ne.wikipedia.org/\" lang=\"ne\" title=\"NepÄlÄ«\">à¤¨à¥à¤ªà¤¾à¤²à¥</a></li>\n",
      "<li><a href=\"//frr.wikipedia.org/\" lang=\"frr\">Nordfriisk</a></li>\n",
      "<li><a href=\"//oc.wikipedia.org/\" lang=\"oc\">Occitan</a></li>\n",
      "<li><a href=\"//mhr.wikipedia.org/\" lang=\"mhr\" title=\"Olyk Marij\">ÐÐ»ÑÐº Ð¼Ð°ÑÐ¸Ð¹</a></li>\n",
      "<li><a href=\"//or.wikipedia.org/\" lang=\"or\" title=\"Oá¹iÄ\">à¬à¬¡à¬¿à¬¼à¬</a></li>\n",
      "<li><a href=\"//as.wikipedia.org/\" lang=\"as\" title=\"ÃxÃ´miya\">à¦à¦¸à¦®à§à¦¯à¦¾à¦¼</a></li>\n",
      "<li><a href=\"//pa.wikipedia.org/\" lang=\"pa\" title=\"PaÃ±jÄbÄ« (GurmukhÄ«)\">à¨ªà©°à¨à¨¾à¨¬à© (à¨à©à¨°à¨®à©à¨à©)</a></li>\n",
      "<li><a href=\"//pnb.wikipedia.org/\" lang=\"pnb\" title=\"PaÃ±jÄbÄ« (ShÄhmukhÄ«)\"><bdi dir=\"rtl\">Ù¾ÙØ¬Ø§Ø¨Û (Ø´Ø§Û ÙÚ©Ú¾Û)</bdi></a></li>\n",
      "<li><a href=\"//ps.wikipedia.org/\" lang=\"ps\" title=\"PaÊto\"><bdi dir=\"rtl\">Ù¾ÚØªÙ</bdi></a></li>\n",
      "<li><a href=\"//pms.wikipedia.org/\" lang=\"pms\">PiemontÃ¨is</a></li>\n",
      "<li><a href=\"//nds.wikipedia.org/\" lang=\"nds\">PlattdÃ¼Ã¼tsch</a></li>\n",
      "<li><a href=\"//crh.wikipedia.org/\" lang=\"crh\">QÄ±rÄ±mtatarca</a></li>\n",
      "<li><a href=\"//qu.wikipedia.org/\" lang=\"qu\">Runa Simi</a></li>\n",
      "<li><a href=\"//sa.wikipedia.org/\" lang=\"sa\" title=\"Saá¹ská¹tam\">à¤¸à¤à¤¸à¥à¤à¥à¤¤à¤®à¥</a></li>\n",
      "<li><a href=\"//sah.wikipedia.org/\" lang=\"sah\" title=\"Saxa Tyla\">Ð¡Ð°ÑÐ° Ð¢ÑÐ»Ð°</a></li>\n",
      "<li><a href=\"//sco.wikipedia.org/\" lang=\"sco\">Scots</a></li>\n",
      "<li><a href=\"//sq.wikipedia.org/\" lang=\"sq\">Shqip</a></li>\n",
      "<li><a href=\"//scn.wikipedia.org/\" lang=\"scn\">Sicilianu</a></li>\n",
      "<li><a href=\"//si.wikipedia.org/\" lang=\"si\" title=\"Siá¹hala\">à·à·à¶à·à¶½</a></li>\n",
      "<li><a href=\"//sd.wikipedia.org/\" lang=\"sd\" title=\"SindhÄ«\"><bdi dir=\"rtl\">Ø³ÙÚÙ</bdi></a></li>\n",
      "<li><a href=\"//szl.wikipedia.org/\" lang=\"szl\">ÅlÅ¯nski</a></li>\n",
      "<li><a href=\"//su.wikipedia.org/\" lang=\"su\">Basa Sunda</a></li>\n",
      "<li><a href=\"//sw.wikipedia.org/\" lang=\"sw\">Kiswahili</a></li>\n",
      "<li><a href=\"//tl.wikipedia.org/\" lang=\"tl\">Tagalog</a></li>\n",
      "<li><a href=\"//shn.wikipedia.org/\" lang=\"shn\">á½áááááááá¸</a></li>\n",
      "<li><a href=\"//te.wikipedia.org/\" lang=\"te\" title=\"Telugu\">à°¤à±à°²à±à°à±</a></li>\n",
      "<li><a href=\"//bug.wikipedia.org/\" lang=\"bug\">á¨á¨ á¨á¨á¨á¨ / Basa Ugi</a></li>\n",
      "<li><a href=\"//vec.wikipedia.org/\" lang=\"vec\">VÃ¨neto</a></li>\n",
      "<li><a href=\"//wa.wikipedia.org/\" lang=\"wa\">Walon</a></li>\n",
      "<li><a href=\"//wuu.wikipedia.org/\" lang=\"wuu\" title=\"WÃºyÇ\">å´è¯­</a></li>\n",
      "<li><a href=\"//yi.wikipedia.org/\" lang=\"yi\" title=\"YidiÅ¡\"><bdi dir=\"rtl\">××Ö´×××©</bdi></a></li>\n",
      "<li><a href=\"//yo.wikipedia.org/\" lang=\"yo\">YorÃ¹bÃ¡</a></li>\n",
      "<li><a href=\"//diq.wikipedia.org/\" lang=\"diq\" title=\"Zazaki\">Zazaki</a></li>\n",
      "<li><a href=\"//bat-smg.wikipedia.org/\" lang=\"sgs\">Å½emaitÄÅ¡ka</a></li>\n",
      "<li><a href=\"//zu.wikipedia.org/\" lang=\"zu\">isiZulu</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "<h2 class=\"bookshelf-container\">\n",
      "<span class=\"bookshelf\">\n",
      "<span class=\"text\">\n",
      "<bdi dir=\"ltr\">\n",
      "1&nbsp;000+\n",
      "</bdi>\n",
      "<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\n",
      "articles\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</h2>\n",
      "<div class=\"langlist hlist\" data-el-section=\"secondary links\">\n",
      "<ul>\n",
      "<li><a href=\"//lad.wikipedia.org/\" lang=\"lad\"><span lang=\"lad-Latn\">Dzhudezmo</span> / <bdi lang=\"lad-Hebr\" dir=\"rtl\">××××× ×</bdi></a></li>\n",
      "<li><a href=\"//kbd.wikipedia.org/\" lang=\"kbd\" title=\"Adighabze\">ÐÐ´ÑÐ³ÑÐ±Ð·Ñ</a></li>\n",
      "<li><a href=\"//ang.wikipedia.org/\" lang=\"ang\">Ãnglisc</a></li>\n",
      "<li><a href=\"//smn.wikipedia.org/\" lang=\"smn\" title=\"anarÃ¢Å¡kielÃ¢\">AnarÃ¢Å¡kielÃ¢</a></li>\n",
      "<li><a href=\"//ab.wikipedia.org/\" lang=\"ab\" title=\"aá¹sshwa\">Ð°Ô¥ÑÑÓÐ°</a></li>\n",
      "<li><a href=\"//hyw.wikipedia.org/\" lang=\"hyw\" title=\"Arevmdahayeren\">Ô±ÖÕ¥ÖÕ´Õ¿Õ¡Õ°Õ¡ÕµÕ¥ÖÕ§Õ¶</a></li>\n",
      "<li><a href=\"//roa-rup.wikipedia.org/\" lang=\"roa-rup\">ArmÃ£neashce</a></li>\n",
      "<li><a href=\"//frp.wikipedia.org/\" lang=\"frp\">Arpitan</a></li>\n",
      "<li><a href=\"//arc.wikipedia.org/\" lang=\"arc\" title=\"ÄtÃ»rÄyÃ¢\"><bdi dir=\"rtl\">ÜÜ¬ÜÜªÜÜ</bdi></a></li>\n",
      "<li><a href=\"//gn.wikipedia.org/\" lang=\"gn\">AvaÃ±eâáº½</a></li>\n",
      "<li><a href=\"//av.wikipedia.org/\" lang=\"av\" title=\"Avar\">ÐÐ²Ð°Ñ</a></li>\n",
      "<li><a href=\"//ay.wikipedia.org/\" lang=\"ay\">Aymar</a></li>\n",
      "<li><a href=\"//bjn.wikipedia.org/\" lang=\"bjn\">Bahasa Banjar</a></li>\n",
      "<li><a href=\"//bh.wikipedia.org/\" lang=\"bh\" title=\"BhÅjapurÄ«\">à¤­à¥à¤à¤ªà¥à¤°à¥</a></li>\n",
      "<li><a href=\"//bi.wikipedia.org/\" lang=\"bi\">Bislama</a></li>\n",
      "<li><a href=\"//bo.wikipedia.org/\" lang=\"bo\" title=\"Bod Skad\">à½à½¼à½à¼à½¡à½²à½</a></li>\n",
      "<li><a href=\"//bxr.wikipedia.org/\" lang=\"bxr\" title=\"Buryad\">ÐÑÑÑÐ°Ð´</a></li>\n",
      "<li><a href=\"//cbk-zam.wikipedia.org/\" lang=\"cbk-x-zam\">Chavacano de Zamboanga</a></li>\n",
      "<li><a href=\"//co.wikipedia.org/\" lang=\"co\">Corsu</a></li>\n",
      "<li><a href=\"//za.wikipedia.org/\" lang=\"za\">Vahcuengh / è©±å®</a></li>\n",
      "<li><a href=\"//dag.wikipedia.org/\" lang=\"dag\">Dagbanli</a></li>\n",
      "<li><a href=\"//ary.wikipedia.org/\" lang=\"ary\" title=\"Darija\"><bdi dir=\"rtl\">Ø§ÙØ¯Ø§Ø±Ø¬Ø©</bdi></a></li>\n",
      "<li><a href=\"//se.wikipedia.org/\" lang=\"se\">DavvisÃ¡megiella</a></li>\n",
      "<li><a href=\"//pdc.wikipedia.org/\" lang=\"pdc\">Deitsch</a></li>\n",
      "<li><a href=\"//dv.wikipedia.org/\" lang=\"dv\" title=\"Divehi\"><bdi dir=\"rtl\">ÞÞ¨ÞÞ¬ÞÞ¨ÞÞ¦ÞÞ°</bdi></a></li>\n",
      "<li><a href=\"//dsb.wikipedia.org/\" lang=\"dsb\">Dolnoserbski</a></li>\n",
      "<li><a href=\"//myv.wikipedia.org/\" lang=\"myv\" title=\"Erzjanj\">Ð­ÑÐ·ÑÐ½Ñ</a></li>\n",
      "<li><a href=\"//ext.wikipedia.org/\" lang=\"ext\">EstremeÃ±u</a></li>\n",
      "<li><a href=\"//fur.wikipedia.org/\" lang=\"fur\">Furlan</a></li>\n",
      "<li><a href=\"//gv.wikipedia.org/\" lang=\"gv\">Gaelg</a></li>\n",
      "<li><a href=\"//gag.wikipedia.org/\" lang=\"gag\">Gagauz</a></li>\n",
      "<li><a href=\"//inh.wikipedia.org/\" lang=\"inh\" title=\"Ghalghai\">ÐÓÐ°Ð»Ð³ÓÐ°Ð¹</a></li>\n",
      "<li><a href=\"//ki.wikipedia.org/\" lang=\"ki\">GÄ©kÅ©yÅ©</a></li>\n",
      "<li><a href=\"//glk.wikipedia.org/\" lang=\"glk\" title=\"GilÉki\"><bdi dir=\"rtl\">Ú¯ÛÙÚ©Û</bdi></a></li>\n",
      "<li><a href=\"//gan.wikipedia.org/\" lang=\"gan\" title=\"Gon ua\" data-hans=\"èµ£è¯­\" data-hant=\"è´èª\" class=\"jscnconv\">èµ£è¯­ / è´èª</a></li>\n",
      "<li><a href=\"//hak.wikipedia.org/\" lang=\"hak\">Hak-kÃ¢-ngÃ® / å®¢å®¶èª</a></li>\n",
      "<li><a href=\"//xal.wikipedia.org/\" lang=\"xal\" title=\"HalÊ¹mg\">Ð¥Ð°Ð»ÑÐ¼Ð³</a></li>\n",
      "<li><a href=\"//haw.wikipedia.org/\" lang=\"haw\">Ê»Ålelo HawaiÊ»i</a></li>\n",
      "<li><a href=\"//ig.wikipedia.org/\" lang=\"ig\">Igbo</a></li>\n",
      "<li><a href=\"//rw.wikipedia.org/\" lang=\"rw\">Ikinyarwanda</a></li>\n",
      "<li><a href=\"//ie.wikipedia.org/\" lang=\"ie\">Interlingue</a></li>\n",
      "<li><a href=\"//kbp.wikipedia.org/\" lang=\"kbp\">KabÉ©yÉ</a></li>\n",
      "<li><a href=\"//pam.wikipedia.org/\" lang=\"pam\">Kapampangan</a></li>\n",
      "<li><a href=\"//csb.wikipedia.org/\" lang=\"csb\">KaszÃ«bsczi</a></li>\n",
      "<li><a href=\"//kw.wikipedia.org/\" lang=\"kw\">Kernewek</a></li>\n",
      "<li><a href=\"//km.wikipedia.org/\" lang=\"km\" title=\"PhÃ©asa KhmÃ©r\">áá¶áá¶ááááá</a></li>\n",
      "<li><a href=\"//kv.wikipedia.org/\" lang=\"kv\" title=\"Komi\">ÐÐ¾Ð¼Ð¸</a></li>\n",
      "<li><a href=\"//koi.wikipedia.org/\" lang=\"koi\" title=\"Perem Komi\">ÐÐµÑÐµÐ¼ ÐºÐ¾Ð¼Ð¸</a></li>\n",
      "<li><a href=\"//kg.wikipedia.org/\" lang=\"kg\">Kongo</a></li>\n",
      "<li><a href=\"//gom.wikipedia.org/\" lang=\"gom\">à¤à¥à¤à¤à¤£à¥ / Konknni</a></li>\n",
      "<li><a href=\"//ks.wikipedia.org/\" lang=\"ks\" title=\"Koshur\"><bdi dir=\"rtl\">ÙÙ²Ø´ÙØ±</bdi></a></li>\n",
      "<li><a href=\"//gcr.wikipedia.org/\" lang=\"gcr\" title=\"KriyÃ²l Gwiyannen\">KriyÃ²l Gwiyannen</a></li>\n",
      "<li><a href=\"//lo.wikipedia.org/\" lang=\"lo\" title=\"Phaasaa Laao\">àºàº²àºªàº²àº¥àº²àº§</a></li>\n",
      "<li><a href=\"//lbe.wikipedia.org/\" lang=\"lbe\" title=\"Lakku\">ÐÐ°ÐºÐºÑ</a></li>\n",
      "<li><a href=\"//ltg.wikipedia.org/\" lang=\"ltg\">LatgaÄ¼u</a></li>\n",
      "<li><a href=\"//lez.wikipedia.org/\" lang=\"lez\" title=\"Lezgi\">ÐÐµÐ·Ð³Ð¸</a></li>\n",
      "<li><a href=\"//nia.wikipedia.org/\" lang=\"nia\">Li Niha</a></li>\n",
      "<li><a href=\"//ln.wikipedia.org/\" lang=\"ln\">LingÃ¡la</a></li>\n",
      "<li><a href=\"//jbo.wikipedia.org/\" lang=\"jbo\">lojban</a></li>\n",
      "<li><a href=\"//lg.wikipedia.org/\" lang=\"lg\">Luganda</a></li>\n",
      "<li><a href=\"//mt.wikipedia.org/\" lang=\"mt\">Malti</a></li>\n",
      "<li><a href=\"//mi.wikipedia.org/\" lang=\"mi\">MÄori</a></li>\n",
      "<li><a href=\"//tw.wikipedia.org/\" lang=\"tw\" title=\"Mfantse\">Twi</a></li>\n",
      "<li><a href=\"//mwl.wikipedia.org/\" lang=\"mwl\">MirandÃ©s</a></li>\n",
      "<li><a href=\"//mdf.wikipedia.org/\" lang=\"mdf\" title=\"MokÅ¡enj\">ÐÐ¾ÐºÑÐµÐ½Ñ</a></li>\n",
      "<li><a href=\"//mnw.wikipedia.org/\" lang=\"mnw\">áá¬áá¬ áááº</a></li>\n",
      "<li><a href=\"//nqo.wikipedia.org/\" lang=\"nqo\" title=\"N&#x27;Ko\">ßßß</a></li>\n",
      "<li><a href=\"//fj.wikipedia.org/\" lang=\"fj\">Na Vosa Vaka-Viti</a></li>\n",
      "<li><a href=\"//nah.wikipedia.org/\" lang=\"nah\">NÄhuatlahtÅlli</a></li>\n",
      "<li><a href=\"//na.wikipedia.org/\" lang=\"na\">Dorerin Naoero</a></li>\n",
      "<li><a href=\"//nds-nl.wikipedia.org/\" lang=\"nds-nl\">Nedersaksisch</a></li>\n",
      "<li><a href=\"//nrm.wikipedia.org/\" lang=\"roa-x-nrm\">Nouormand / Normaund</a></li>\n",
      "<li><a href=\"//nov.wikipedia.org/\" lang=\"nov\">Novial</a></li>\n",
      "<li><a href=\"//om.wikipedia.org/\" lang=\"om\" title=\"Ingiliffaa\">Afaan Oromoo</a></li>\n",
      "<li><a href=\"//pi.wikipedia.org/\" lang=\"pi\" title=\"PÄá¸·i\">à¤ªà¤¾à¤²à¤¿</a></li>\n",
      "<li><a href=\"//pag.wikipedia.org/\" lang=\"pag\">PangasinÃ¡n</a></li>\n",
      "<li><a href=\"//pap.wikipedia.org/\" lang=\"pap\">Papiamentu</a></li>\n",
      "<li><a href=\"//pfl.wikipedia.org/\" lang=\"pfl\">PfÃ¤lzisch</a></li>\n",
      "<li><a href=\"//pcd.wikipedia.org/\" lang=\"pcd\">Picard</a></li>\n",
      "<li><a href=\"//krc.wikipedia.org/\" lang=\"krc\" title=\"QaraÃ§ayâMalqar\">ÐÑÐ°ÑÐ°ÑÐ°Ð¹âÐ¼Ð°Ð»ÐºÑÐ°Ñ</a></li>\n",
      "<li><a href=\"//kaa.wikipedia.org/\" lang=\"kaa\">Qaraqalpaqsha</a></li>\n",
      "<li><a href=\"//ksh.wikipedia.org/\" lang=\"ksh\">Ripoarisch</a></li>\n",
      "<li><a href=\"//rm.wikipedia.org/\" lang=\"rm\">Rumantsch</a></li>\n",
      "<li><a href=\"//rue.wikipedia.org/\" lang=\"rue\" title=\"Rusinâskyj\">Ð ÑÑÐ¸Ð½ÑÑÐºÑÐ¹</a></li>\n",
      "<li><a href=\"//sm.wikipedia.org/\" lang=\"sm\">Gagana SÄmoa</a></li>\n",
      "<li><a href=\"//sat.wikipedia.org/\" lang=\"sat\" title=\"Santali\">á±¥á±á±±á±á±á±²á±¤</a></li>\n",
      "<li><a href=\"//sc.wikipedia.org/\" lang=\"sc\" title=\"Sardu\">Sardu</a></li>\n",
      "<li><a href=\"//trv.wikipedia.org/\" lang=\"trv\">Seediq</a></li>\n",
      "<li><a href=\"//stq.wikipedia.org/\" lang=\"stq\">Seeltersk</a></li>\n",
      "<li><a href=\"//nso.wikipedia.org/\" lang=\"nso\">Sesotho sa Leboa</a></li>\n",
      "<li><a href=\"//sn.wikipedia.org/\" lang=\"sn\">ChiShona</a></li>\n",
      "<li><a href=\"//cu.wikipedia.org/\" lang=\"cu\" title=\"SlovÄnÄ­skÅ­\">Ð¡Ð»Ð¾Ð²Ñ£ÌÐ½ÑÑÐºÑ / â°â°â°â°â°¡â°â° â°â°â°</a></li>\n",
      "<li><a href=\"//so.wikipedia.org/\" lang=\"so\">Soomaaliga</a></li>\n",
      "<li><a href=\"//srn.wikipedia.org/\" lang=\"srn\">Sranantongo</a></li>\n",
      "<li><a href=\"//kab.wikipedia.org/\" lang=\"kab\" title=\"Taqbaylit\">Taqbaylit</a></li>\n",
      "<li><a href=\"//roa-tara.wikipedia.org/\" lang=\"roa\">TarandÃ­ne</a></li>\n",
      "<li><a href=\"//tet.wikipedia.org/\" lang=\"tet\">Tetun</a></li>\n",
      "<li><a href=\"//tpi.wikipedia.org/\" lang=\"tpi\">Tok Pisin</a></li>\n",
      "<li><a href=\"//to.wikipedia.org/\" lang=\"to\">faka Tonga</a></li>\n",
      "<li><a href=\"//chr.wikipedia.org/\" lang=\"chr\" title=\"Tsalagi\">á£á³á©</a></li>\n",
      "<li><a href=\"//tum.wikipedia.org/\" lang=\"tum\">chiTumbuka</a></li>\n",
      "<li><a href=\"//tk.wikipedia.org/\" lang=\"tk\">TÃ¼rkmenÃ§e</a></li>\n",
      "<li><a href=\"//tyv.wikipedia.org/\" lang=\"tyv\" title=\"Tyva dyl\">Ð¢ÑÐ²Ð° Ð´ÑÐ»</a></li>\n",
      "<li><a href=\"//udm.wikipedia.org/\" lang=\"udm\" title=\"Udmurt\">Ð£Ð´Ð¼ÑÑÑ</a></li>\n",
      "<li><a href=\"//ug.wikipedia.org/\" lang=\"ug\"><bdi dir=\"rtl\">Ø¦ÛÙØºÛØ±ÚÙ</bdi></a></li>\n",
      "<li><a href=\"//vep.wikipedia.org/\" lang=\"vep\">VepsÃ¤n</a></li>\n",
      "<li><a href=\"//fiu-vro.wikipedia.org/\" lang=\"fiu-vro\">VÃµro</a></li>\n",
      "<li><a href=\"//vls.wikipedia.org/\" lang=\"vls\">West-Vlams</a></li>\n",
      "<li><a href=\"//wo.wikipedia.org/\" lang=\"wo\">Wolof</a></li>\n",
      "<li><a href=\"//xh.wikipedia.org/\" lang=\"xh\">isiXhosa</a></li>\n",
      "<li><a href=\"//zea.wikipedia.org/\" lang=\"zea\">ZeÃªuws</a></li>\n",
      "<li><a href=\"//ty.wikipedia.org/\" lang=\"ty\">Reo tahiti</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "<h2 class=\"bookshelf-container\">\n",
      "<span class=\"bookshelf\">\n",
      "<span class=\"text\">\n",
      "<bdi dir=\"ltr\">\n",
      "100+\n",
      "</bdi>\n",
      "<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\n",
      "articles\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</h2>\n",
      "<div class=\"langlist langlist-tiny hlist\" data-el-section=\"secondary links\">\n",
      "<ul>\n",
      "<li><a href=\"//ak.wikipedia.org/\" lang=\"ak\">Akan</a></li>\n",
      "<li><a href=\"//bm.wikipedia.org/\" lang=\"bm\">Bamanankan</a></li>\n",
      "<li><a href=\"//ch.wikipedia.org/\" lang=\"ch\">Chamoru</a></li>\n",
      "<li><a href=\"//ny.wikipedia.org/\" lang=\"ny\">Chichewa</a></li>\n",
      "<li><a href=\"//ee.wikipedia.org/\" lang=\"ee\">EÊegbe</a></li>\n",
      "<li><a href=\"//ff.wikipedia.org/\" lang=\"ff\">Fulfulde</a></li>\n",
      "<li><a href=\"//got.wikipedia.org/\" lang=\"got\" title=\"Gutisk\">ð²ð¿ðð¹ððº</a></li>\n",
      "<li><a href=\"//iu.wikipedia.org/\" lang=\"iu\">áááááá¦ / Inuktitut</a></li>\n",
      "<li><a href=\"//ik.wikipedia.org/\" lang=\"ik\">IÃ±upiak</a></li>\n",
      "<li><a href=\"//kl.wikipedia.org/\" lang=\"kl\">Kalaallisut</a></li>\n",
      "<li><a href=\"//mad.wikipedia.org/\" lang=\"mad\">MadhurÃ¢</a></li>\n",
      "<li><a href=\"//cr.wikipedia.org/\" lang=\"cr\">NÄhiyawÄwin / áá¦áá­ááá£</a></li>\n",
      "<li><a href=\"//pih.wikipedia.org/\" lang=\"pih\">Norfuk / Pitkern</a></li>\n",
      "<li><a href=\"//ami.wikipedia.org/\" lang=\"ami\">Pangcah</a></li>\n",
      "<li><a href=\"//pwn.wikipedia.org/\" lang=\"pwn\">pinayuanan</a></li>\n",
      "<li><a href=\"//pnt.wikipedia.org/\" lang=\"pnt\" title=\"PontiakÃ¡\">Î Î¿Î½ÏÎ¹Î±ÎºÎ¬</a></li>\n",
      "<li><a href=\"//dz.wikipedia.org/\" lang=\"dz\" title=\"Rdzong-Kha\">à½¢à¾«à½¼à½à¼à½</a></li>\n",
      "<li><a href=\"//rmy.wikipedia.org/\" lang=\"rmy\">romani Ähib</a></li>\n",
      "<li><a href=\"//rn.wikipedia.org/\" lang=\"rn\">Ikirundi</a></li>\n",
      "<li><a href=\"//sg.wikipedia.org/\" lang=\"sg\">SÃ¤ngÃ¶</a></li>\n",
      "<li><a href=\"//st.wikipedia.org/\" lang=\"st\">Sesotho</a></li>\n",
      "<li><a href=\"//tn.wikipedia.org/\" lang=\"tn\">Setswana</a></li>\n",
      "<li><a href=\"//ss.wikipedia.org/\" lang=\"ss\">SiSwati</a></li>\n",
      "<li><a href=\"//ti.wikipedia.org/\" lang=\"ti\" title=\"TÉgÉrÉÃ±a\">áµáá­á</a></li>\n",
      "<li><a href=\"//din.wikipedia.org/\" lang=\"din\">ThuÉÅjÃ¤Å</a></li>\n",
      "<li><a href=\"//chy.wikipedia.org/\" lang=\"chy\">TsÄhesenÄstsestotse</a></li>\n",
      "<li><a href=\"//ts.wikipedia.org/\" lang=\"ts\">Xitsonga</a></li>\n",
      "<li><a href=\"//ve.wikipedia.org/\" lang=\"ve\">Tshivená¸a</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "<div class=\"langlist langlist-others hlist\" data-el-section=\"other languages\">\n",
      "<a class=\"jsl10n\" href=\"https://meta.wikimedia.org/wiki/Special:MyLanguage/List_of_Wikipedias\" lang data-jsl10n=\"other-languages-label\">Other languages</a>\n",
      "</div></div>\n",
      "</div>\n",
      "<hr>\n",
      "<div class=\"footer\" data-el-section=\"other projects\">\n",
      "<div class=\"footer-sidebar\">\n",
      "<div class=\"footer-sidebar-content\">\n",
      "<div class=\"footer-sidebar-icon sprite svg-Wikimedia-logo_black\">\n",
      "</div>\n",
      "<div class=\"footer-sidebar-text jsl10n\" data-jsl10n=\"portal.footer-description\">\n",
      "Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects.\n",
      "</div>\n",
      "<div class=\"footer-sidebar-text\">\n",
      "<a href=\"https://donate.wikimedia.org/?utm_medium=portal&utm_campaign=portalFooter&utm_source=portalFooter\" target=\"_blank\">\n",
      "<span class=\"jsl10n\" data-jsl10n=\"footer-donate\">You can support our work with a donation.</span>\n",
      "</a>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"footer-sidebar app-badges\">\n",
      "<div class=\"footer-sidebar-content\">\n",
      "<div class=\"footer-sidebar-text\">\n",
      "<div class=\"footer-sidebar-icon sprite svg-wikipedia_app_tile\"></div>\n",
      "<strong class=\"jsl10n\" data-jsl10n=\"portal.app-links.title\">\n",
      "<a class=\"jsl10n\" data-jsl10n=\"portal.app-links.url\" href=\"https://en.wikipedia.org/wiki/List_of_Wikipedia_mobile_applications\">\n",
      "Download Wikipedia for Android or iOS\n",
      "</a>\n",
      "</strong>\n",
      "<p class=\"jsl10n\" data-jsl10n=\"portal.app-links.description\">\n",
      "Save your favorite articles to read offline, sync your reading lists across devices and customize your reading experience with the official Wikipedia app.\n",
      "</p>\n",
      "<ul>\n",
      "<li class=\"app-badge app-badge-android\">\n",
      "<a target=\"_blank\" rel=\"noreferrer\" href=\"https://play.google.com/store/apps/details?id=org.wikipedia&referrer=utm_source%3Dportal%26utm_medium%3Dbutton%26anid%3Dadmob\">\n",
      "<span class=\"jsl10n sprite svg-badge_google_play_store\" data-jsl10n=\"portal.app-links.google-store\">Google Play Store</span>\n",
      "</a>\n",
      "</li>\n",
      "<li class=\"app-badge app-badge-ios\">\n",
      "<a target=\"_blank\" rel=\"noreferrer\" href=\"https://itunes.apple.com/app/apple-store/id324715238?pt=208305&ct=portal&mt=8\">\n",
      "<span class=\"jsl10n sprite svg-badge_ios_app_store\" data-jsl10n=\"portal.app-links.apple-store\">Apple App Store</span>\n",
      "</a>\n",
      "</li>\n",
      "</ul>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"other-projects\">\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//commons.wikimedia.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-Commons-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"commons.name\">Commons</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"commons.slogan\">Freely usable photos &amp; more</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//www.wikivoyage.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-Wikivoyage-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikivoyage.name\">Wikivoyage</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikivoyage.slogan\">Free travel guide</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//www.wiktionary.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-Wiktionary-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"wiktionary.name\">Wiktionary</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wiktionary.slogan\">Free dictionary</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//www.wikibooks.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-Wikibooks-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikibooks.name\">Wikibooks</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikibooks.slogan\">Free textbooks</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//www.wikinews.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-Wikinews-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikinews.name\">Wikinews</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikinews.slogan\">Free news source</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//www.wikidata.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-Wikidata-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikidata.name\">Wikidata</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikidata.slogan\">Free knowledge base</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//www.wikiversity.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-Wikiversity-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikiversity.name\">Wikiversity</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikiversity.slogan\">Free course materials</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//www.wikiquote.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-Wikiquote-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikiquote.name\">Wikiquote</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikiquote.slogan\">Free quote compendium</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//www.mediawiki.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-MediaWiki-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"mediawiki.name\">MediaWiki</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"mediawiki.slogan\">Free &amp; open wiki application</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//www.wikisource.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-Wikisource-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikisource.name\">Wikisource</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikisource.slogan\">Free library</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//species.wikimedia.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-Wikispecies-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"wikispecies.name\">Wikispecies</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"wikispecies.slogan\">Free species directory</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"other-project\">\n",
      "<a class=\"other-project-link\" href=\"//meta.wikimedia.org/\">\n",
      "<div class=\"other-project-icon\">\n",
      "<div class=\"sprite svg-Meta-Wiki-logo_sister\"></div>\n",
      "</div>\n",
      "<div class=\"other-project-text\">\n",
      "<span class=\"other-project-title jsl10n\" data-jsl10n=\"metawiki.name\">Meta-Wiki</span>\n",
      "<span class=\"other-project-tagline jsl10n\" data-jsl10n=\"metawiki.slogan\">Community coordination &amp; documentation</span>\n",
      "</div>\n",
      "</a>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<hr>\n",
      "<p class=\"site-license\">\n",
      "<small class=\"jsl10n\" data-jsl10n=\"license\">This page is available under the <a href=\"https://creativecommons.org/licenses/by-sa/3.0/\">Creative Commons Attribution-ShareAlike License</a></small>\n",
      "<small class=\"jsl10n\" data-jsl10n=\"terms\"><a href=\"https://meta.wikimedia.org/wiki/Terms_of_use\">Terms of Use</a></small>\n",
      "<small class=\"jsl10n\" data-jsl10n=\"privacy-policy\"><a href=\"https://meta.wikimedia.org/wiki/Privacy_policy\">Privacy Policy</a></small>\n",
      "</p>\n",
      "<script>\n",
      "var rtlLangs = ['ar','arc','ary','arz','bcc','bgn','bqi','ckb','dv','fa','glk','he','kk-cn','kk-arab','khw','ks','ku-arab','lki','luz','mzn','nqo','pnb','ps','sd','sdh','skr','ug','ur','yi'],\n",
      "    translationsHash = '88c7e1eb',\n",
      "    /**\n",
      "     * This variable is used to convert the generic \"portal\" keyword in the data-jsl10n attributes\n",
      "     * e.g. 'data-jsl10n=\"portal.footer-description\"' into a portal-specific key, e.g. \"wiki\"\n",
      "     * for the Wikipedia portal.\n",
      "     */\n",
      "    translationsPortalKey = 'wiki';\n",
      "    /**\n",
      "     * The wm-typeahead.js feature is used for search,and it uses domain name for searching. We want domain\n",
      "     * name to be portal Specific (different for every portal).So by declaring variable 'portalSearchDomain'\n",
      "     * in index.handlebars we will make this portal Specific.\n",
      "    **/\n",
      "    portalSearchDomain = 'wikipedia.org'\n",
      "    /*\n",
      "     This object is used by l10n scripts (page-localized.js, topten-localized.js)\n",
      "     to reveal the page content after l10n json is loaded.\n",
      "     A timer is also set to prevent JS from hiding page content indefinitelty.\n",
      "     This script is inlined to safeguard againt script loading errors and placed\n",
      "     at the top of the page to safeguard against any HTML loading/parsing errors.\n",
      "    */\n",
      "    wmL10nVisible = {\n",
      "        ready: false,\n",
      "        makeVisible: function(){\n",
      "            if ( !wmL10nVisible.ready ) {\n",
      "                wmL10nVisible.ready = true;\n",
      "                document.body.className += ' jsl10n-visible';\n",
      "            }\n",
      "        }\n",
      "    };\n",
      "    window.setTimeout( wmL10nVisible.makeVisible, 1000 )\n",
      "</script>\n",
      "<script src=\"portal/wikipedia.org/assets/js/index-f1d77ed19b.js\"></script>\n",
      "<!--[if gt IE 9]><!-->\n",
      "<script src=\"portal/wikipedia.org/assets/js/gt-ie9-ce3fe8e88d.js\"></script>\n",
      "<!--<![endif]-->\n",
      "<!--[if lte IE 9]><!-->\n",
      "<style>\n",
      ".styled-select {\n",
      "        display: block;\n",
      "    }\n",
      "</style>\n",
      "<!--<![endif]-->\n",
      "<!--[if lte IE 9]>\n",
      "<style>\n",
      "    .langlist > ul {\n",
      "        text-align: center;\n",
      "    }\n",
      "    .langlist > ul > li {\n",
      "        display: inline;\n",
      "        padding: 0 0.5em;\n",
      "    }\n",
      "</style>\n",
      "<![endif]-->\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "print(html)"
   ],
   "execution_count": 23,
   "cell_type": "code"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 7. Let's practice!\n",
    "\n",
    "That's enough out of me for the time being. Let's get you hacking away at pulling down some HTML from the web using GET requests! GET coding!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Performing HTTP requests in Python using urllib\n",
    "\n",
    "<p>Now that you know the basics behind HTTP GET requests, it's time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, <code>\"https://campus.datacamp.com/courses/1606/4135?ex=2\"</code>.</p>\n",
    "<p>In the next exercise, you'll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen,Request"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Import the functions <code>urlopen</code> and <code>Request</code> from the subpackage <code>urllib.request</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Package the request to the url <code>\"https://campus.datacamp.com/courses/1606/4135?ex=2\"</code> using the function <code>Request()</code> and assign it to <code>request</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Send the request and catch the response in the variable <code>response</code> with  the function <code>urlopen()</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Run the rest of the code to see the datatype of <code>response</code> and to close the connection!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Printing HTTP request results in Python using urllib\n",
    "\n",
    "<p>You have just packaged and sent a GET request to <code>\"https://campus.datacamp.com/courses/1606/4135?ex=2\"</code> and then caught the response. You saw that such a response is a <code>http.client.HTTPResponse</code> object. The question remains: what can you do with this response?</p>\n",
    "<p>Well, as it came from an HTML page, you could <em>read</em> it to extract the HTML and, in fact, such a <code>http.client.HTTPResponse</code> object has an associated <code>read()</code> method. In this exercise, you'll build on your previous great work to extract the response and print the HTML.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Send the request and catch the response in the variable <code>response</code> with the function <code>urlopen()</code>, as in the previous exercise."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "\n",
    "# This packages the request\n",
    "request = Request(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract the response using the <code>read()</code> method and store the result in the variable <code>html</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the string <code>html</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "\n",
    "# Extract the response: html\n",
    "html = response.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hit submit to perform all of the above and to close the response: be tidy!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html><html lang=\"en\"><head><link rel=\"apple-touch-icon-precomposed\" sizes=\"57x57\" href=\"/campus/apple-touch-icon-57x57.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"114x114\" href=\"/campus/apple-touch-icon-114x114.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"72x72\" href=\"/campus/apple-touch-icon-72x72.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"144x144\" href=\"/campus/apple-touch-icon-144x144.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"60x60\" href=\"/campus/apple-touch-icon-60x60.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"120x120\" href=\"/campus/apple-touch-icon-120x120.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"76x76\" href=\"/campus/apple-touch-icon-76x76.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"152x152\" href=\"/campus/apple-touch-icon-152x152.png\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon.ico\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-196x196.png\" sizes=\"196x196\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-96x96.png\" sizes=\"96x96\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-32x32.png\" sizes=\"32x32\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-16x16.png\" sizes=\"16x16\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-128.png\" sizes=\"128x128\"><meta name=\"application-name\" content=\"DataCamp\"><meta name=\"msapplication-TileColor\" content=\"#FFFFFF\"><meta name=\"msapplication-TileImage\" content=\"/campus/mstile-144x144.png\"><meta name=\"msapplication-square70x70logo\" content=\"/campus/mstile-70x70.png\"><meta name=\"msapplication-square150x150logo\" content=\"/campus/mstile-150x150.png\"><meta name=\"msapplication-wide310x150logo\" content=\"/campus/mstile-310x150.png\"><meta name=\"msapplication-square310x310logo\" content=\"/campus/mstile-310x310.png\"><link href=\"/campus/static/css/16.c5650823.chunk.css\" rel=\"stylesheet\"><link href=\"/campus/static/css/main.88385ec5.chunk.css\" rel=\"stylesheet\"><title data-react-helmet=\"true\">Importing flat files from the web: your turn! | Python</title><link data-react-helmet=\"true\" rel=\"canonical\" href=\"https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2\"><meta data-react-helmet=\"true\" charset=\"utf-8\"><meta data-react-helmet=\"true\" http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"><meta data-react-helmet=\"true\" name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1\"><meta data-react-helmet=\"true\" name=\"fragment\" content=\"!\"><meta data-react-helmet=\"true\" name=\"keywords\" content=\"R, Python, Data analysis, interactive, learning\"><meta data-react-helmet=\"true\" name=\"description\" content=\"Here is an example of Importing flat files from the web: your turn!: You are about to import your first file from the web! The flat file you will import will be &apos;winequality-red.\"><meta data-react-helmet=\"true\" name=\"twitter:card\" content=\"summary\"><meta data-react-helmet=\"true\" name=\"twitter:site\" content=\"@DataCamp\"><meta data-react-helmet=\"true\" name=\"twitter:title\" content=\"Importing flat files from the web: your turn! | Python\"><meta data-react-helmet=\"true\" name=\"twitter:description\" content=\"Here is an example of Importing flat files from the web: your turn!: You are about to import your first file from the web! The flat file you will import will be &apos;winequality-red.\"><meta data-react-helmet=\"true\" name=\"twitter:creator\" content=\"@DataCamp\"><meta data-react-helmet=\"true\" name=\"twitter:image:src\" content=\"/public/assets/images/var/twitter_share.png\"><meta data-react-helmet=\"true\" name=\"twitter:domain\" content=\"www.datacamp.com\"><meta data-react-helmet=\"true\" property=\"og:title\" content=\"Importing flat files from the web: your turn! | Python\"><meta data-react-helmet=\"true\" property=\"og:image\" content=\"/public/assets/images/var/linkedin_share.png\"><meta data-react-helmet=\"true\" name=\"google-signin-clientid\" content=\"892114885437-01a7plbsu1b2vobuhvnckmmanhb58h3a.apps.googleusercontent.com\"><meta data-react-helmet=\"true\" name=\"google-signin-scope\" content=\"email profile\"><meta data-react-helmet=\"true\" name=\"google-signin-cookiepolicy\" content=\"single_host_origin\"><script async src=\\'/cdn-cgi/bm/cv/669835187/api.js\\'></script></head><body><script>window.PRELOADED_STATE = \"[&quot;~#iR&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;StateRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;backendSession&quot;,[&quot;~#iOM&quot;,[&quot;status&quot;,[&quot;^2&quot;,[&quot;code&quot;,&quot;none&quot;,&quot;text&quot;,&quot;&quot;]],&quot;isInitSession&quot;,false,&quot;message&quot;,null]],&quot;boot&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;BootStateRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;bootState&quot;,&quot;PRE_BOOTED&quot;,&quot;error&quot;,null]]],&quot;chapter&quot;,[&quot;^2&quot;,[&quot;current&quot;,[&quot;^2&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,1,&quot;number_of_videos&quot;,3,&quot;slug&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;last_updated_on&quot;,&quot;07/04/2022&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,12,&quot;free_preview&quot;,true,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;title&quot;,&quot;Importing data from the Internet&quot;,&quot;xp&quot;,1050,&quot;id&quot;,4135,&quot;exercises&quot;,[&quot;~#iL&quot;,[[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Scraping the web in Python&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]]],&quot;description&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;contentAuthorization&quot;,[&quot;^ &quot;],&quot;course&quot;,[&quot;^2&quot;,[&quot;difficulty_level&quot;,1,&quot;reduced_outline&quot;,null,&quot;marketing_video&quot;,&quot;&quot;,&quot;active_image&quot;,&quot;course-1606-master:cb59605c00ed73a970165be3564ff450-20220407091433345&quot;,&quot;mobile_enabled&quot;,true,&quot;author_field&quot;,null,&quot;chapters&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,1,&quot;number_of_videos&quot;,3,&quot;slug&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;last_updated_on&quot;,&quot;07/04/2022&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,12,&quot;free_preview&quot;,true,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;title&quot;,&quot;Importing data from the Internet&quot;,&quot;xp&quot;,1050,&quot;id&quot;,4135,&quot;exercises&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Scraping the web in Python&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]]],&quot;description&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^2&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,2,&quot;number_of_videos&quot;,2,&quot;slug&quot;,&quot;interacting-with-apis-to-import-data-from-the-web-2&quot;,&quot;last_updated_on&quot;,&quot;07/04/2022&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,9,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter2.pdf&quot;,&quot;title&quot;,&quot;Interacting with APIs to import data from the web&quot;,&quot;xp&quot;,650,&quot;id&quot;,4136,&quot;exercises&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Introduction to APIs and JSONs&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=1&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: What exactly is a JSON?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=2&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Loading and exploring a JSON&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=3&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: Exploring your JSON&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=4&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;APIs and interacting with the world wide web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=5&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: What&#39;s an API?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=6&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;API requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=7&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;JSON\\xe2\\x80\\x93from the web to Python&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=8&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Checking out the Wikipedia API&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=9&quot;]]]],&quot;description&quot;,&quot;In this chapter, you will gain a deeper understanding of how to import data from the web. You will learn the basics of extracting data from APIs, gain insight on the importance of APIs, and practice extracting data by diving into the OMDB and Library of Congress APIs.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^2&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,3,&quot;number_of_videos&quot;,2,&quot;slug&quot;,&quot;diving-deep-into-the-twitter-api&quot;,&quot;last_updated_on&quot;,&quot;07/04/2022&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,7,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter3.pdf&quot;,&quot;title&quot;,&quot;Diving  deep into the Twitter API&quot;,&quot;xp&quot;,600,&quot;id&quot;,4140,&quot;exercises&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;The Twitter API and Authentication&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=1&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Streaming tweets&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=2&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Load and explore your Twitter data&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=3&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Twitter data to DataFrame&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=4&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;A little bit of Twitter text analysis&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=5&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Plotting your Twitter data&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=6&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Final Thoughts&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=7&quot;]]]],&quot;description&quot;,&quot;In this chapter, you will consolidate your knowledge of interacting with APIs in a deep dive into the Twitter streaming API. You&#39;ll learn how to stream real-time Twitter data, and how to analyze and visualize it.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;time_needed&quot;,null,&quot;author_image&quot;,&quot;https://assets.datacamp.com/production/course_1606/author_images/author_image_course_1606_20200310-1-lgdj4c?1583853939&quot;,&quot;tracks&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;path&quot;,&quot;/tracks/data-scientist-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Data Scientist  with Python&quot;]],[&quot;^2&quot;,[&quot;path&quot;,&quot;/tracks/importing-cleaning-data-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Importing &amp; Cleaning Data  with Python&quot;]]]],&quot;runtime_config&quot;,null,&quot;lti_only&quot;,false,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;topic_id&quot;,8,&quot;slug&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;last_updated_on&quot;,&quot;09/08/2022&quot;,&quot;paid&quot;,true,&quot;collaborators&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/382/294/square/francis-photo.jpg?1471980001&quot;,&quot;full_name&quot;,&quot;Francisco Castro&quot;]]]],&quot;time_needed_in_hours&quot;,2,&quot;technology_id&quot;,2,&quot;university&quot;,null,&quot;archived_at&quot;,null,&quot;state&quot;,&quot;live&quot;,&quot;author_bio&quot;,null,&quot;should_cache&quot;,true,&quot;sharing_links&quot;,[&quot;^2&quot;,[&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;]],&quot;instructors&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;id&quot;,301837,&quot;marketing_biography&quot;,&quot;Data Scientist at DataCamp&quot;,&quot;biography&quot;,&quot;Hugo is a data scientist, educator, writer and podcaster at DataCamp. His main interests are promoting data &amp; AI literacy, helping to spread data skills through organizations and society and doing amateur stand up comedy in NYC. If you want to know what he likes to talk about, definitely check out DataFramed, the DataCamp podcast, which he hosts and produces: https://www.datacamp.com/community/podcast&quot;,&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/301/837/square/hugoaboutpic.jpg?1493154678&quot;,&quot;full_name&quot;,&quot;Hugo Bowne-Anderson&quot;,&quot;instructor_path&quot;,&quot;/instructors/hugobowne&quot;]]]],&quot;seo_title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;xp&quot;,2300,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb_home/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;short_description&quot;,&quot;Improve your Python data importing skills and learn to work with web and API data.&quot;,&quot;nb_of_subscriptions&quot;,140572,&quot;seo_description&quot;,&quot;Learn how to import data into Python from sources like the web and by pulling data from APIs, such as the Twitter streaming API to stream real-time tweets.&quot;,&quot;type&quot;,&quot;datacamp&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/intermediate-importing-data-in-python&quot;,&quot;case_study&quot;,null,&quot;id&quot;,1606,&quot;datasets&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/b422ace2fceada7b569e0ba3e8d833fddc684c4d/latitude.xls&quot;,&quot;name&quot;,&quot;Latitudes (XLS)&quot;]],[&quot;^2&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/3ef452f83a91556ea4284624b969392c0506fb33/tweets3.txt&quot;,&quot;name&quot;,&quot;Tweets&quot;]],[&quot;^2&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/013936d2700e2d00207ec42100d448c23692eb6f/winequality-red.csv&quot;,&quot;name&quot;,&quot;Red wine quality&quot;]]]],&quot;description&quot;,&quot;As a data scientist, you will need to clean data, wrangle and munge it, visualize it, build predictive models and interpret these models. Before you can do so, however, you will need to know how to get data into Python. In the prequel to this course, you learned many ways to import data into Python: from flat files such as .txt and .csv; from files native to other software such as Excel spreadsheets, Stata, SAS, and MATLAB files; and from relational databases such as SQLite and PostgreSQL. In this course, you&#39;ll extend this knowledge base by learning to import data from the web and by pulling data from Application Programming Interfaces\\xe2\\x80\\x94 APIs\\xe2\\x80\\x94such as the Twitter streaming API, which allows us to stream real-time tweets.&quot;,&quot;prerequisites&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;path&quot;,&quot;/courses/introduction-to-importing-data-in-python&quot;,&quot;title&quot;,&quot;Introduction to Importing Data in Python&quot;]]]],&quot;original_image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/original/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;programming_language&quot;,&quot;python&quot;,&quot;external_slug&quot;,&quot;intermediate-importing-data-in-python&quot;]],&quot;exercises&quot;,[&quot;^2&quot;,[&quot;current&quot;,1,&quot;all&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990668,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,1,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.36777536598343596,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;Importing flat files from the web&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990668,&quot;projector_key&quot;,&quot;course_1606_59604c018a6e132016cd26144a12fee0&quot;,&quot;video_link&quot;,null,&quot;key&quot;,&quot;e36457c7ed&quot;,&quot;course_id&quot;,1606]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;).multi(\\\\\\\\n  check_args(0).has_equal_value(),\\\\\\\\n  check_args(1).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;urlretrieve&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the function &lt;code&gt;urlretrieve()&lt;/code&gt; to save the file locally as &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the remaining code to load &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; in a pandas DataFrame and to print its head to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42707,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a subpackage &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;This one&#39;s a long URL. Make sure you typed it in correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; to import (in the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;filename&lt;/em&gt; for saving the file locally as the second argument to &lt;code&gt;urlretrieve()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change the code for loading &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; and printing its head.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,2,&quot;user&quot;,[&quot;^2&quot;,[&quot;isHintShown&quot;,false,&quot;editorTabs&quot;,[&quot;^2&quot;,[&quot;files/script.py&quot;,[&quot;^2&quot;,[&quot;title&quot;,&quot;script.py&quot;,&quot;isSolution&quot;,false,&quot;props&quot;,[&quot;^2&quot;,[&quot;active&quot;,true,&quot;isClosable&quot;,false,&quot;code&quot;,null,&quot;extra&quot;,[&quot;^2&quot;,[]]]]]]]],&quot;outputMarkdownTabs&quot;,[&quot;^2&quot;,[]],&quot;markdown&quot;,[&quot;^2&quot;,[&quot;titles&quot;,[&quot;^7&quot;,[&quot;Knit PDF&quot;,&quot;Knit HTML&quot;]],&quot;activeTitle&quot;,&quot;Knit HTML&quot;]],&quot;currentXp&quot;,100,&quot;graphicalTabs&quot;,[&quot;^2&quot;,[&quot;plot&quot;,[&quot;^2&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;Plots&quot;,&quot;props&quot;,[&quot;^2&quot;,[&quot;sources&quot;,[&quot;^7&quot;,[]],&quot;currentIndex&quot;,0]],&quot;dimension&quot;,[&quot;^2&quot;,[&quot;isRealSize&quot;,false,&quot;width&quot;,1,&quot;height&quot;,1]]]],&quot;html&quot;,[&quot;^2&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;HTML Viewer&quot;,&quot;props&quot;,[&quot;^2&quot;,[&quot;sources&quot;,[&quot;^7&quot;,[]],&quot;currentIndex&quot;,0]]]]]],&quot;feedbackMessages&quot;,[&quot;^7&quot;,[]],&quot;lastSubmittedCode&quot;,null,&quot;ltiStatus&quot;,[&quot;^2&quot;,[]],&quot;lastSubmitActiveEditorTab&quot;,null,&quot;consoleSqlTabs&quot;,[&quot;^2&quot;,[&quot;query_result&quot;,[&quot;^2&quot;,[&quot;extraClass&quot;,&quot;&quot;,&quot;title&quot;,&quot;query result&quot;,&quot;props&quot;,[&quot;^2&quot;,[&quot;active&quot;,true,&quot;isNotView&quot;,true,&quot;message&quot;,&quot;No query executed yet...&quot;]]]]]],&quot;consoleTabs&quot;,[&quot;^2&quot;,[&quot;console&quot;,[&quot;^2&quot;,[&quot;title&quot;,&quot;IPython Shell&quot;,&quot;props&quot;,[&quot;^2&quot;,[&quot;active&quot;,true]],&quot;dimension&quot;,[&quot;^2&quot;,[&quot;cols&quot;,400]]]],&quot;slides&quot;,[&quot;^2&quot;,[&quot;title&quot;,&quot;Slides&quot;,&quot;props&quot;,[&quot;^2&quot;,[&quot;active&quot;,false]]]]]],&quot;inputMarkdownTabs&quot;,[&quot;^2&quot;,[]],&quot;consoleObjectViewTabs&quot;,[&quot;^2&quot;,[]]]],&quot;randomNumber&quot;,0.5526523826264957,&quot;assignment&quot;,&quot;&lt;p&gt;You are about to import your first file from the web! The flat file you will import will be &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; from the University of California, Irvine&#39;s &lt;a href=\\\\\\\\&quot;https://archive.ics.uci.edu/ml/index.php\\\\\\\\&quot;&gt;Machine Learning repository&lt;/a&gt;. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;After you import it, you&#39;ll check your working directory to confirm that it is there and then you&#39;ll load it into a &lt;code&gt;pandas&lt;/code&gt; DataFrame.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nfrom urllib.request import urlretrieve\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\nurlretrieve(url, &#39;winequality-red.csv&#39;)\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42707]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(____)\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\ndf.iloc[:, 0].hist()\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;matplotlib.pyplot\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().has_equal_ast(code=\\\\\\\\&quot;df.iloc[:, 0].hist\\\\\\\\&quot;, incorrect_msg=\\\\\\\\&quot;Please do not change the code to plot the histogram.\\\\\\\\&quot;)\\\\\\\\nEx().check_function(\\\\\\\\&quot;matplotlib.pyplot.show\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read file into a DataFrame &lt;code&gt;df&lt;/code&gt; using &lt;code&gt;pd.read_csv()&lt;/code&gt;, recalling that the separator in the file is &lt;code&gt;&#39;;&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the rest of the code to plot histogram of the first feature in the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42708,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;separator&lt;/em&gt; as the second argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;head&lt;/em&gt; of a DataFrame can be accessed by using &lt;code&gt;head()&lt;/code&gt; on the DataFrame.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change any of the code for plotting the histograms.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,3,&quot;randomNumber&quot;,0.0722698875980019,&quot;assignment&quot;,&quot;&lt;p&gt;You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using &lt;code&gt;pandas&lt;/code&gt;. In particular, you can use the function &lt;code&gt;pd.read_csv()&lt;/code&gt; with the URL as the first argument and the separator &lt;code&gt;sep&lt;/code&gt; as the second argument.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file, once again, is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\ndf = pd.read_csv(url, sep=&#39;;&#39;)\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(df.head())\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\ndf.iloc[:, 0].hist()\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42708]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\n\\\\\\\\n&quot;,&quot;sct&quot;,&quot;Ex().has_import(&#39;pandas&#39;)\\\\\\\\nEx().check_correct(\\\\\\\\n    has_printout(0),\\\\\\\\n    multi(\\\\\\\\n        check_correct(\\\\\\\\n            check_object(&#39;xls&#39;).is_instance(dict),\\\\\\\\n            check_correct(\\\\\\\\n                check_function(&#39;pandas.read_excel&#39;).multi(\\\\\\\\n                    check_args(0).has_equal_value(),\\\\\\\\n                    check_args(&#39;sheet_name&#39;).has_equal_value()\\\\\\\\n                ),\\\\\\\\n                check_object(&#39;url&#39;).has_equal_value()\\\\\\\\n            )\\\\\\\\n        )\\\\\\\\n    )\\\\\\\\n)\\\\\\\\nEx().has_printout(1)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read the file in &lt;code&gt;url&lt;/code&gt; into a dictionary &lt;code&gt;xls&lt;/code&gt; using &lt;code&gt;pd.read_excel()&lt;/code&gt; recalling that, in order to import all sheets you need to pass &lt;code&gt;None&lt;/code&gt; to the argument &lt;code&gt;sheet_name&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary &lt;code&gt;xls&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the first sheet &lt;em&gt;using the sheet name, not the index of the sheet&lt;/em&gt;! The sheet name is &lt;code&gt;&#39;1700&#39;&lt;/code&gt;&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42709,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed in the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and &lt;code&gt;sheet_name&lt;/code&gt; with its corresponding value as the second argument to &lt;code&gt;pd.read_excel()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;keys&lt;/em&gt; of a dictionary can be accessed by using &lt;code&gt;keys()&lt;/code&gt; on the dictionary.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access a sheet using the format: &lt;em&gt;dictionary&lt;/em&gt;&lt;strong&gt;[&lt;/strong&gt;&lt;em&gt;sheet name or index&lt;/em&gt;&lt;strong&gt;]&lt;/strong&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,4,&quot;randomNumber&quot;,0.016732646010944707,&quot;assignment&quot;,&quot;&lt;p&gt;Congrats! You&#39;ve just loaded a flat file from the web into a DataFrame without first saving it locally using the &lt;code&gt;pandas&lt;/code&gt; function &lt;code&gt;pd.read_csv()&lt;/code&gt;. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you&#39;ll use &lt;code&gt;pd.read_excel()&lt;/code&gt; to import an Excel spreadsheet.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the spreadsheet is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;Your job is to use &lt;code&gt;pd.read_excel()&lt;/code&gt; to read in all of its sheets, print the sheet names and then print the head of the first sheet &lt;em&gt;using its name, not its index&lt;/em&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that the output of &lt;code&gt;pd.read_excel()&lt;/code&gt; is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\nxls = pd.read_excel(url, sheet_name=None)\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\nprint(xls.keys())\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\nprint(xls[&#39;1700&#39;].head())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42709]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990669,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,5,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.47060459046282,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990669,&quot;projector_key&quot;,&quot;course_1606_9d15ae176be1800b996f7869a82b8087&quot;,&quot;video_link&quot;,null,&quot;key&quot;,&quot;e480d1fdcf&quot;,&quot;course_id&quot;,1606]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\n\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import urlopen, Request\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;, missing_msg=predef_msg).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n  \\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;),\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the functions &lt;code&gt;urlopen&lt;/code&gt; and &lt;code&gt;Request&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the url &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; using the function &lt;code&gt;Request()&lt;/code&gt; and assign it to &lt;code&gt;request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with  the function &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Run the rest of the code to see the datatype of &lt;code&gt;response&lt;/code&gt; and to close the connection!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42711,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import two functions in one line, import the first function as usual and add a comma &lt;code&gt;,&lt;/code&gt; followed by the second function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (already in the &lt;code&gt;url&lt;/code&gt; object defined) as an argument to &lt;code&gt;Request()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the datatype of &lt;code&gt;response&lt;/code&gt; and closing the connection.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,6,&quot;randomNumber&quot;,0.6437039193567979,&quot;assignment&quot;,&quot;&lt;p&gt;Now that you know the basics behind HTTP GET requests, it&#39;s time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;In the next exercise, you&#39;ll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42711]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.read\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;html\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with the function &lt;code&gt;urlopen()&lt;/code&gt;, as in the previous exercise.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the response using the &lt;code&gt;read()&lt;/code&gt; method and store the result in the variable &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the string &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to perform all of the above and to close the response: be tidy!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42712,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Apply the method &lt;code&gt;read()&lt;/code&gt; to the response object &lt;code&gt;response&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Simply pass &lt;code&gt;html&lt;/code&gt; to the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for closing the response.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,7,&quot;randomNumber&quot;,0.21766911960920354,&quot;assignment&quot;,&quot;&lt;p&gt;You have just packaged and sent a GET request to &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; and then caught the response. You saw that such a response is a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object. The question remains: what can you do with this response?&lt;/p&gt;\\\\\\\\n&lt;p&gt;Well, as it came from an HTML page, you could &lt;em&gt;read&lt;/em&gt; it to extract the HTML and, in fact, such a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object has an associated &lt;code&gt;read()&lt;/code&gt; method. In this exercise, you&#39;ll build on your previous great work to extract the response and print the HTML.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\nhtml = response.read()\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(html)\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42712]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import requests\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;text&#39; variable\\\\\\\\nEx().has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `text`?\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;text\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the package &lt;code&gt;requests&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42713,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;import x&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Did you type in the URL correctly?&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,8,&quot;randomNumber&quot;,0.9047294207502945,&quot;assignment&quot;,&quot;&lt;p&gt;Now that you&#39;ve got your head and hands around making HTTP requests using the urllib package, you&#39;re going to figure out how to do the same using the higher-level requests library. You&#39;ll once again be pinging DataCamp servers for their &lt;code&gt;\\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;&lt;/code&gt; page.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that unlike in the previous exercises using urllib, you don&#39;t have to close the connection when using requests!&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nimport requests\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\nurl = \\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\ntext = r.text\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42713]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990670,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,9,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.7224866639917007,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;Scraping the web in Python&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990670,&quot;projector_key&quot;,&quot;course_1606_9d1f8a331d1200c7e1bdbfcaf3a7a491&quot;,&quot;video_link&quot;,null,&quot;key&quot;,&quot;da43858012&quot;,&quot;course_id&quot;,1606]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\n\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: call to prettify() and &#39;pretty_soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;pretty_soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.prettify\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;BeautifulSoup&lt;/code&gt; from the package &lt;code&gt;bs4&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;html_doc&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Create a BeautifulSoup object &lt;code&gt;soup&lt;/code&gt; from the resulting HTML using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;prettify()&lt;/code&gt; on &lt;code&gt;soup&lt;/code&gt; and assign the result to &lt;code&gt;pretty_soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print to prettified HTML to your shell!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42715,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Check the URL to make sure that you typed it in correctly.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the extracted &lt;em&gt;HTML&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;To use the &lt;code&gt;prettify()&lt;/code&gt; method on the BeautifulSoup object &lt;code&gt;soup&lt;/code&gt;, execute &lt;code&gt;soup.prettify()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the prettified HTML.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,10,&quot;randomNumber&quot;,0.6153930141061901,&quot;assignment&quot;,&quot;&lt;p&gt;In this interactive exercise, you&#39;ll learn how to use the BeautifulSoup package to &lt;em&gt;parse&lt;/em&gt;, &lt;em&gt;prettify&lt;/em&gt; and &lt;em&gt;extract&lt;/em&gt; information from HTML. You&#39;ll scrape the data from the webpage of Guido van Rossum, Python&#39;s very own &lt;a href=\\\\\\\\&quot;https://en.wikipedia.org/wiki/Benevolent_dictator_for_life\\\\\\\\&quot;&gt;Benevolent Dictator for Life&lt;/a&gt;. In the following exercises, you&#39;ll prettify the HTML and then extract the text and the hyperlinks.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of interest is &lt;code&gt;url = &#39;https://www.python.org/~guido/&#39;&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\npretty_soup = soup.prettify()\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42715]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: &#39;guido_title&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_title\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;soup.title\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `soup.title` to create `guido_title`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\n# Test: call to soup.get_text() and &#39;guido_text&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_text\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.get_text\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(1)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;In the sample code, the HTML response object &lt;code&gt;html_doc&lt;/code&gt; has already been created: your first task is to Soupify it using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt; and to assign the resulting soup to the variable &lt;code&gt;soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the title from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the attribute &lt;code&gt;title&lt;/code&gt; and assign the result to &lt;code&gt;guido_title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the title of Guido&#39;s webpage to the shell using the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the text from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the method &lt;code&gt;get_text()&lt;/code&gt; and assign to &lt;code&gt;guido_text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the text from Guido&#39;s webpage to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42716,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML response object&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;title&lt;/code&gt; attribute of the object &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The object that contains the title of Guido&#39;s webpage is &lt;code&gt;guido_title&lt;/code&gt;; pass this as an argument to &lt;code&gt;print()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;get_text()&lt;/code&gt; on the HTML soup &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.get_text()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the text from Guido&#39;s webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,11,&quot;randomNumber&quot;,0.8893605204424677,&quot;assignment&quot;,&quot;&lt;p&gt;As promised, in the following exercises, you&#39;ll learn the basics of extracting information from HTML soup. In this exercise, you&#39;ll figure out how to extract the text from the BDFL&#39;s webpage, along with printing the webpage&#39;s title.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\nguido_title = soup.title\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\nprint(guido_title)\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\nguido_text = soup.get_text()\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42716]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor ____ in ____:\\\\\\\\n    ____&quot;,&quot;sct&quot;,&quot;predef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nEx().check_correct(\\\\\\\\n    check_object(\\\\\\\\&quot;a_tags\\\\\\\\&quot;),\\\\\\\\n    check_function(\\\\\\\\&quot;soup.find_all\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_for_loop().multi(\\\\\\\\n        check_iter().has_equal_value(incorrect_msg = \\\\\\\\&quot;You have to iterate over `a_tags`\\\\\\\\&quot;),\\\\\\\\n        check_body().set_context(&#39;&lt;a href=\\\\\\\\&quot;pics.html\\\\\\\\&quot;&gt;&lt;img border=\\\\\\\\&quot;0\\\\\\\\&quot; src=\\\\\\\\&quot;images/IMG_2192.jpg\\\\\\\\&quot;/&gt;&lt;/a&gt;&#39;).check_function(\\\\\\\\&quot;print\\\\\\\\&quot;).check_args(0).check_function(\\\\\\\\&quot;link.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n    )\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;find_all()&lt;/code&gt; to find all hyperlinks in &lt;code&gt;soup&lt;/code&gt;, remembering that hyperlinks are defined by the HTML tag &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; but passed to &lt;code&gt;find_all()&lt;/code&gt; without angle brackets; store the result in the variable &lt;code&gt;a_tags&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The variable &lt;code&gt;a_tags&lt;/code&gt; is a results set: your job now is to enumerate over it, using a &lt;code&gt;for&lt;/code&gt; loop and to print the actual URLs of the hyperlinks; to do this, for every element &lt;code&gt;link&lt;/code&gt; in &lt;code&gt;a_tags&lt;/code&gt;, you want to &lt;code&gt;print()&lt;/code&gt; &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42717,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML tag&lt;/em&gt; to find (without the angle brackets &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;) as a string argument to &lt;code&gt;find_all()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Recall that the &lt;code&gt;for&lt;/code&gt; loop recipe is: &lt;code&gt;for&lt;/code&gt; &lt;em&gt;loop variable&lt;/em&gt; &lt;code&gt;in&lt;/code&gt; &lt;em&gt;results set&lt;/em&gt;&lt;code&gt;:&lt;/code&gt;. Don&#39;t forget to pass &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt; as an argument to &lt;code&gt;print()&lt;/code&gt; inside the &lt;code&gt;for&lt;/code&gt; loop body.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,12,&quot;randomNumber&quot;,0.6366817787546253,&quot;assignment&quot;,&quot;&lt;p&gt;In this exercise, you&#39;ll figure out how to extract the URLs of the hyperlinks from the BDFL&#39;s webpage. In the process, you&#39;ll become close friends with the soup method &lt;code&gt;find_all()&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\na_tags = soup.find_all(&#39;a&#39;)\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor link in a_tags:\\\\\\\\n    print(link.get(&#39;href&#39;))&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42717]]]],&quot;canRateChapter&quot;,false,&quot;isChapterCompleted&quot;,false]],&quot;learningMode&quot;,&quot;course&quot;,&quot;location&quot;,[&quot;^2&quot;,[&quot;current&quot;,[&quot;^2&quot;,[&quot;pathname&quot;,&quot;/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1&quot;,&quot;query&quot;,[&quot;^2&quot;,[&quot;ex&quot;,&quot;2&quot;]]]],&quot;canonical&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;,&quot;before&quot;,[&quot;^2&quot;,[&quot;pathname&quot;,&quot;/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1&quot;,&quot;query&quot;,[&quot;^2&quot;,[&quot;ex&quot;,&quot;2&quot;]]]]]],&quot;mobilePopup&quot;,[&quot;^2&quot;,[]],&quot;onboardingMilestones&quot;,[&quot;^ &quot;,&quot;isStarted&quot;,false,&quot;isActive&quot;,true,&quot;step&quot;,0],&quot;preFetchedData&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedDataStateRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^9&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[&quot;^ &quot;,&quot;id&quot;,1606,&quot;title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;description&quot;,&quot;As a data scientist, you will need to clean data, wrangle and munge it, visualize it, build predictive models and interpret these models. Before you can do so, however, you will need to know how to get data into Python. In the prequel to this course, you learned many ways to import data into Python: from flat files such as .txt and .csv; from files native to other software such as Excel spreadsheets, Stata, SAS, and MATLAB files; and from relational databases such as SQLite and PostgreSQL. In this course, you&#39;ll extend this knowledge base by learning to import data from the web and by pulling data from Application Programming Interfaces\\xe2\\x80\\x94 APIs\\xe2\\x80\\x94such as the Twitter streaming API, which allows us to stream real-time tweets.&quot;,&quot;short_description&quot;,&quot;Improve your Python data importing skills and learn to work with web and API data.&quot;,&quot;author_field&quot;,null,&quot;author_bio&quot;,null,&quot;author_image&quot;,&quot;https://assets.datacamp.com/production/course_1606/author_images/author_image_course_1606_20200310-1-lgdj4c?1583853939&quot;,&quot;nb_of_subscriptions&quot;,140572,&quot;slug&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb_home/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;last_updated_on&quot;,&quot;09/08/2022&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/intermediate-importing-data-in-python&quot;,&quot;should_cache&quot;,true,&quot;type&quot;,&quot;datacamp&quot;,&quot;difficulty_level&quot;,1,&quot;state&quot;,&quot;live&quot;,&quot;university&quot;,null,&quot;sharing_links&quot;,[&quot;^ &quot;,&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;],&quot;marketing_video&quot;,&quot;&quot;,&quot;programming_language&quot;,&quot;python&quot;,&quot;paid&quot;,true,&quot;time_needed&quot;,null,&quot;xp&quot;,2300,&quot;topic_id&quot;,8,&quot;technology_id&quot;,2,&quot;reduced_outline&quot;,null,&quot;runtime_config&quot;,null,&quot;lti_only&quot;,false,&quot;instructors&quot;,[[&quot;^ &quot;,&quot;id&quot;,301837,&quot;marketing_biography&quot;,&quot;Data Scientist at DataCamp&quot;,&quot;biography&quot;,&quot;Hugo is a data scientist, educator, writer and podcaster at DataCamp. His main interests are promoting data &amp; AI literacy, helping to spread data skills through organizations and society and doing amateur stand up comedy in NYC. If you want to know what he likes to talk about, definitely check out DataFramed, the DataCamp podcast, which he hosts and produces: https://www.datacamp.com/community/podcast&quot;,&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/301/837/square/hugoaboutpic.jpg?1493154678&quot;,&quot;full_name&quot;,&quot;Hugo Bowne-Anderson&quot;,&quot;instructor_path&quot;,&quot;/instructors/hugobowne&quot;]],&quot;collaborators&quot;,[[&quot;^ &quot;,&quot;^19&quot;,&quot;https://assets.datacamp.com/users/avatars/000/382/294/square/francis-photo.jpg?1471980001&quot;,&quot;^1:&quot;,&quot;Francisco Castro&quot;]],&quot;datasets&quot;,[[&quot;^ &quot;,&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/b422ace2fceada7b569e0ba3e8d833fddc684c4d/latitude.xls&quot;,&quot;name&quot;,&quot;Latitudes (XLS)&quot;],[&quot;^ &quot;,&quot;^1&gt;&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/3ef452f83a91556ea4284624b969392c0506fb33/tweets3.txt&quot;,&quot;^1?&quot;,&quot;Tweets&quot;],[&quot;^ &quot;,&quot;^1&gt;&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/013936d2700e2d00207ec42100d448c23692eb6f/winequality-red.csv&quot;,&quot;^1?&quot;,&quot;Red wine quality&quot;]],&quot;tracks&quot;,[[&quot;^ &quot;,&quot;path&quot;,&quot;/tracks/data-scientist-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Data Scientist  with Python&quot;],[&quot;^ &quot;,&quot;^1A&quot;,&quot;/tracks/importing-cleaning-data-with-python&quot;,&quot;^1B&quot;,&quot;Importing &amp; Cleaning Data  with Python&quot;]],&quot;prerequisites&quot;,[[&quot;^ &quot;,&quot;^1A&quot;,&quot;/courses/introduction-to-importing-data-in-python&quot;,&quot;^E&quot;,&quot;Introduction to Importing Data in Python&quot;]],&quot;time_needed_in_hours&quot;,2,&quot;seo_title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;seo_description&quot;,&quot;Learn how to import data into Python from sources like the web and by pulling data from APIs, such as the Twitter streaming API to stream real-time tweets.&quot;,&quot;archived_at&quot;,null,&quot;original_image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/original/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;external_slug&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;mobile_enabled&quot;,true,&quot;case_study&quot;,null,&quot;chapters&quot;,[[&quot;^ &quot;,&quot;id&quot;,4135,&quot;title_meta&quot;,null,&quot;^E&quot;,&quot;Importing data from the Internet&quot;,&quot;^F&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;number&quot;,1,&quot;^L&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;nb_exercises&quot;,12,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^O&quot;,&quot;07/04/2022&quot;,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;free_preview&quot;,true,&quot;xp&quot;,1050,&quot;number_of_videos&quot;,3,&quot;^:&quot;,[[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Importing flat files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;^1N&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Scraping the web in Python&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]],[&quot;^ &quot;,&quot;id&quot;,4136,&quot;^1M&quot;,null,&quot;^E&quot;,&quot;Interacting with APIs to import data from the web&quot;,&quot;^F&quot;,&quot;In this chapter, you will gain a deeper understanding of how to import data from the web. You will learn the basics of extracting data from APIs, gain insight on the importance of APIs, and practice extracting data by diving into the OMDB and Library of Congress APIs.&quot;,&quot;^1N&quot;,2,&quot;^L&quot;,&quot;interacting-with-apis-to-import-data-from-the-web-2&quot;,&quot;^1O&quot;,9,&quot;^1P&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1Q&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^O&quot;,&quot;07/04/2022&quot;,&quot;^1R&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter2.pdf&quot;,&quot;^1S&quot;,null,&quot;xp&quot;,650,&quot;^1T&quot;,2,&quot;^:&quot;,[[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Introduction to APIs and JSONs&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=1&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^E&quot;,&quot;Pop quiz: What exactly is a JSON?&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=2&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Loading and exploring a JSON&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=3&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^E&quot;,&quot;Pop quiz: Exploring your JSON&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=4&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;APIs and interacting with the world wide web&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=5&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^E&quot;,&quot;Pop quiz: What&#39;s an API?&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=6&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;API requests&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=7&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;JSON\\xe2\\x80\\x93from the web to Python&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=8&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Checking out the Wikipedia API&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=9&quot;]]],[&quot;^ &quot;,&quot;id&quot;,4140,&quot;^1M&quot;,null,&quot;^E&quot;,&quot;Diving  deep into the Twitter API&quot;,&quot;^F&quot;,&quot;In this chapter, you will consolidate your knowledge of interacting with APIs in a deep dive into the Twitter streaming API. You&#39;ll learn how to stream real-time Twitter data, and how to analyze and visualize it.&quot;,&quot;^1N&quot;,3,&quot;^L&quot;,&quot;diving-deep-into-the-twitter-api&quot;,&quot;^1O&quot;,7,&quot;^1P&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1Q&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^O&quot;,&quot;07/04/2022&quot;,&quot;^1R&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter3.pdf&quot;,&quot;^1S&quot;,null,&quot;xp&quot;,600,&quot;^1T&quot;,2,&quot;^:&quot;,[[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;The Twitter API and Authentication&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=1&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Streaming tweets&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=2&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Load and explore your Twitter data&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=3&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Twitter data to DataFrame&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=4&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;A little bit of Twitter text analysis&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=5&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Plotting your Twitter data&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=6&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Final Thoughts&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=7&quot;]]]]]]]],&quot;^6&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^C&quot;,&quot;SUCCESS&quot;,&quot;^D&quot;,[&quot;^ &quot;,&quot;id&quot;,4135,&quot;^1M&quot;,null,&quot;^E&quot;,&quot;Importing data from the Internet&quot;,&quot;^F&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;^1N&quot;,1,&quot;^L&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;^1O&quot;,12,&quot;^1P&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1Q&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^O&quot;,&quot;07/04/2022&quot;,&quot;^1R&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;^1S&quot;,true,&quot;xp&quot;,1050,&quot;^1T&quot;,3,&quot;^:&quot;,[[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Importing flat files from the web&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Scraping the web in Python&quot;,&quot;^1U&quot;,50,&quot;^1N&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^1U&quot;,100,&quot;^1N&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]]]]],&quot;^:&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^C&quot;,&quot;SUCCESS&quot;,&quot;^D&quot;,[[&quot;^ &quot;,&quot;id&quot;,990668,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;assignment&quot;,null,&quot;^E&quot;,&quot;Importing flat files from the web&quot;,&quot;sample_code&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;^1N&quot;,1,&quot;sct&quot;,&quot;&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;attachments&quot;,null,&quot;xp&quot;,50,&quot;possible_answers&quot;,[],&quot;feedbacks&quot;,[],&quot;question&quot;,&quot;&quot;,&quot;video_link&quot;,null,&quot;video_hls&quot;,null,&quot;aspect_ratio&quot;,56.25,&quot;projector_key&quot;,&quot;course_1606_59604c018a6e132016cd26144a12fee0&quot;,&quot;key&quot;,&quot;e36457c7ed&quot;,&quot;language&quot;,&quot;python&quot;,&quot;course_id&quot;,1606,&quot;chapter_id&quot;,4135,&quot;^14&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;randomNumber&quot;,0.36777536598343596,&quot;externalId&quot;,990668],[&quot;^ &quot;,&quot;id&quot;,42707,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1V&quot;,&quot;&lt;p&gt;You are about to import your first file from the web! The flat file you will import will be &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; from the University of California, Irvine&#39;s &lt;a href=\\\\\\\\&quot;https://archive.ics.uci.edu/ml/index.php\\\\\\\\&quot;&gt;Machine Learning repository&lt;/a&gt;. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;After you import it, you&#39;ll check your working directory to confirm that it is there and then you&#39;ll load it into a &lt;code&gt;pandas&lt;/code&gt; DataFrame.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^1W&quot;,&quot;# Import package\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;^1X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;urlretrieve&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the function &lt;code&gt;urlretrieve()&lt;/code&gt; to save the file locally as &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the remaining code to load &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; in a pandas DataFrame and to print its head to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1N&quot;,2,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;).multi(\\\\\\\\n  check_args(0).has_equal_value(),\\\\\\\\n  check_args(1).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1Y&quot;,&quot;&quot;,&quot;^1Z&quot;,&quot;# Import package\\\\\\\\nfrom urllib.request import urlretrieve\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\nurlretrieve(url, &#39;winequality-red.csv&#39;)\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;^1[&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a subpackage &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;This one&#39;s a long URL. Make sure you typed it in correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; to import (in the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;filename&lt;/em&gt; for saving the file locally as the second argument to &lt;code&gt;urlretrieve()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change the code for loading &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; and printing its head.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^20&quot;,null,&quot;xp&quot;,100,&quot;^21&quot;,[],&quot;^22&quot;,[],&quot;^23&quot;,&quot;&quot;,&quot;^28&quot;,&quot;python&quot;,&quot;^2&lt;&quot;,0.5526523826264957,&quot;^2=&quot;,42707],[&quot;^ &quot;,&quot;id&quot;,42708,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1V&quot;,&quot;&lt;p&gt;You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using &lt;code&gt;pandas&lt;/code&gt;. In particular, you can use the function &lt;code&gt;pd.read_csv()&lt;/code&gt; with the URL as the first argument and the separator &lt;code&gt;sep&lt;/code&gt; as the second argument.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file, once again, is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;&quot;,&quot;^E&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^1W&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(____)\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\ndf.iloc[:, 0].hist()\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;^1X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read file into a DataFrame &lt;code&gt;df&lt;/code&gt; using &lt;code&gt;pd.read_csv()&lt;/code&gt;, recalling that the separator in the file is &lt;code&gt;&#39;;&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the rest of the code to plot histogram of the first feature in the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1N&quot;,3,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;matplotlib.pyplot\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().has_equal_ast(code=\\\\\\\\&quot;df.iloc[:, 0].hist\\\\\\\\&quot;, incorrect_msg=\\\\\\\\&quot;Please do not change the code to plot the histogram.\\\\\\\\&quot;)\\\\\\\\nEx().check_function(\\\\\\\\&quot;matplotlib.pyplot.show\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1Y&quot;,&quot;&quot;,&quot;^1Z&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\ndf = pd.read_csv(url, sep=&#39;;&#39;)\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(df.head())\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\ndf.iloc[:, 0].hist()\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;^1[&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;separator&lt;/em&gt; as the second argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;head&lt;/em&gt; of a DataFrame can be accessed by using &lt;code&gt;head()&lt;/code&gt; on the DataFrame.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change any of the code for plotting the histograms.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^20&quot;,null,&quot;xp&quot;,100,&quot;^21&quot;,[],&quot;^22&quot;,[],&quot;^23&quot;,&quot;&quot;,&quot;^28&quot;,&quot;python&quot;,&quot;^2&lt;&quot;,0.0722698875980019,&quot;^2=&quot;,42708],[&quot;^ &quot;,&quot;id&quot;,42709,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1V&quot;,&quot;&lt;p&gt;Congrats! You&#39;ve just loaded a flat file from the web into a DataFrame without first saving it locally using the &lt;code&gt;pandas&lt;/code&gt; function &lt;code&gt;pd.read_csv()&lt;/code&gt;. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you&#39;ll use &lt;code&gt;pd.read_excel()&lt;/code&gt; to import an Excel spreadsheet.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the spreadsheet is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;Your job is to use &lt;code&gt;pd.read_excel()&lt;/code&gt; to read in all of its sheets, print the sheet names and then print the head of the first sheet &lt;em&gt;using its name, not its index&lt;/em&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that the output of &lt;code&gt;pd.read_excel()&lt;/code&gt; is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^1W&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\n\\\\\\\\n&quot;,&quot;^1X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read the file in &lt;code&gt;url&lt;/code&gt; into a dictionary &lt;code&gt;xls&lt;/code&gt; using &lt;code&gt;pd.read_excel()&lt;/code&gt; recalling that, in order to import all sheets you need to pass &lt;code&gt;None&lt;/code&gt; to the argument &lt;code&gt;sheet_name&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary &lt;code&gt;xls&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the first sheet &lt;em&gt;using the sheet name, not the index of the sheet&lt;/em&gt;! The sheet name is &lt;code&gt;&#39;1700&#39;&lt;/code&gt;&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1N&quot;,4,&quot;sct&quot;,&quot;Ex().has_import(&#39;pandas&#39;)\\\\\\\\nEx().check_correct(\\\\\\\\n    has_printout(0),\\\\\\\\n    multi(\\\\\\\\n        check_correct(\\\\\\\\n            check_object(&#39;xls&#39;).is_instance(dict),\\\\\\\\n            check_correct(\\\\\\\\n                check_function(&#39;pandas.read_excel&#39;).multi(\\\\\\\\n                    check_args(0).has_equal_value(),\\\\\\\\n                    check_args(&#39;sheet_name&#39;).has_equal_value()\\\\\\\\n                ),\\\\\\\\n                check_object(&#39;url&#39;).has_equal_value()\\\\\\\\n            )\\\\\\\\n        )\\\\\\\\n    )\\\\\\\\n)\\\\\\\\nEx().has_printout(1)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;^1Y&quot;,&quot;&quot;,&quot;^1Z&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\nxls = pd.read_excel(url, sheet_name=None)\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\nprint(xls.keys())\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\nprint(xls[&#39;1700&#39;].head())&quot;,&quot;^1[&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed in the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and &lt;code&gt;sheet_name&lt;/code&gt; with its corresponding value as the second argument to &lt;code&gt;pd.read_excel()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;keys&lt;/em&gt; of a dictionary can be accessed by using &lt;code&gt;keys()&lt;/code&gt; on the dictionary.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access a sheet using the format: &lt;em&gt;dictionary&lt;/em&gt;&lt;strong&gt;[&lt;/strong&gt;&lt;em&gt;sheet name or index&lt;/em&gt;&lt;strong&gt;]&lt;/strong&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^20&quot;,null,&quot;xp&quot;,100,&quot;^21&quot;,[],&quot;^22&quot;,[],&quot;^23&quot;,&quot;&quot;,&quot;^28&quot;,&quot;python&quot;,&quot;^2&lt;&quot;,0.016732646010944707,&quot;^2=&quot;,42709],[&quot;^ &quot;,&quot;id&quot;,990669,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^1V&quot;,null,&quot;^E&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,null,&quot;^1N&quot;,5,&quot;sct&quot;,&quot;&quot;,&quot;^1Y&quot;,&quot;&quot;,&quot;^1Z&quot;,&quot;&quot;,&quot;^1[&quot;,null,&quot;^20&quot;,null,&quot;xp&quot;,50,&quot;^21&quot;,[],&quot;^22&quot;,[],&quot;^23&quot;,&quot;&quot;,&quot;^24&quot;,null,&quot;^25&quot;,null,&quot;^26&quot;,56.25,&quot;^27&quot;,&quot;course_1606_9d15ae176be1800b996f7869a82b8087&quot;,&quot;key&quot;,&quot;e480d1fdcf&quot;,&quot;^28&quot;,&quot;python&quot;,&quot;^29&quot;,1606,&quot;^2:&quot;,4135,&quot;^14&quot;,null,&quot;^2;&quot;,&quot;v0&quot;,&quot;^2&lt;&quot;,0.47060459046282,&quot;^2=&quot;,990669],[&quot;^ &quot;,&quot;id&quot;,42711,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1V&quot;,&quot;&lt;p&gt;Now that you know the basics behind HTTP GET requests, it&#39;s time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;In the next exercise, you&#39;ll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^1W&quot;,&quot;# Import packages\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\n\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;^1X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the functions &lt;code&gt;urlopen&lt;/code&gt; and &lt;code&gt;Request&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the url &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; using the function &lt;code&gt;Request()&lt;/code&gt; and assign it to &lt;code&gt;request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with  the function &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Run the rest of the code to see the datatype of &lt;code&gt;response&lt;/code&gt; and to close the connection!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1N&quot;,6,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import urlopen, Request\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;, missing_msg=predef_msg).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n  \\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;),\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1Y&quot;,&quot;&quot;,&quot;^1Z&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;^1[&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import two functions in one line, import the first function as usual and add a comma &lt;code&gt;,&lt;/code&gt; followed by the second function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (already in the &lt;code&gt;url&lt;/code&gt; object defined) as an argument to &lt;code&gt;Request()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the datatype of &lt;code&gt;response&lt;/code&gt; and closing the connection.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^20&quot;,null,&quot;xp&quot;,100,&quot;^21&quot;,[],&quot;^22&quot;,[],&quot;^23&quot;,&quot;&quot;,&quot;^28&quot;,&quot;python&quot;,&quot;^2&lt;&quot;,0.6437039193567979,&quot;^2=&quot;,42711],[&quot;^ &quot;,&quot;id&quot;,42712,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1V&quot;,&quot;&lt;p&gt;You have just packaged and sent a GET request to &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; and then caught the response. You saw that such a response is a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object. The question remains: what can you do with this response?&lt;/p&gt;\\\\\\\\n&lt;p&gt;Well, as it came from an HTML page, you could &lt;em&gt;read&lt;/em&gt; it to extract the HTML and, in fact, such a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object has an associated &lt;code&gt;read()&lt;/code&gt; method. In this exercise, you&#39;ll build on your previous great work to extract the response and print the HTML.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^1W&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;^1X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with the function &lt;code&gt;urlopen()&lt;/code&gt;, as in the previous exercise.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the response using the &lt;code&gt;read()&lt;/code&gt; method and store the result in the variable &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the string &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to perform all of the above and to close the response: be tidy!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1N&quot;,7,&quot;sct&quot;,&quot;\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.read\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;html\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1Y&quot;,&quot;&quot;,&quot;^1Z&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\nhtml = response.read()\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(html)\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;^1[&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Apply the method &lt;code&gt;read()&lt;/code&gt; to the response object &lt;code&gt;response&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Simply pass &lt;code&gt;html&lt;/code&gt; to the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for closing the response.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^20&quot;,null,&quot;xp&quot;,100,&quot;^21&quot;,[],&quot;^22&quot;,[],&quot;^23&quot;,&quot;&quot;,&quot;^28&quot;,&quot;python&quot;,&quot;^2&lt;&quot;,0.21766911960920354,&quot;^2=&quot;,42712],[&quot;^ &quot;,&quot;id&quot;,42713,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1V&quot;,&quot;&lt;p&gt;Now that you&#39;ve got your head and hands around making HTTP requests using the urllib package, you&#39;re going to figure out how to do the same using the higher-level requests library. You&#39;ll once again be pinging DataCamp servers for their &lt;code&gt;\\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;&lt;/code&gt; page.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that unlike in the previous exercises using urllib, you don&#39;t have to close the connection when using requests!&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^1W&quot;,&quot;# Import package\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;^1X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the package &lt;code&gt;requests&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1N&quot;,8,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import requests\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;text&#39; variable\\\\\\\\nEx().has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `text`?\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;text\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1Y&quot;,&quot;&quot;,&quot;^1Z&quot;,&quot;# Import package\\\\\\\\nimport requests\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\nurl = \\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\ntext = r.text\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;^1[&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;import x&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Did you type in the URL correctly?&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^20&quot;,null,&quot;xp&quot;,100,&quot;^21&quot;,[],&quot;^22&quot;,[],&quot;^23&quot;,&quot;&quot;,&quot;^28&quot;,&quot;python&quot;,&quot;^2&lt;&quot;,0.9047294207502945,&quot;^2=&quot;,42713],[&quot;^ &quot;,&quot;id&quot;,990670,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^1V&quot;,null,&quot;^E&quot;,&quot;Scraping the web in Python&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,null,&quot;^1N&quot;,9,&quot;sct&quot;,&quot;&quot;,&quot;^1Y&quot;,&quot;&quot;,&quot;^1Z&quot;,&quot;&quot;,&quot;^1[&quot;,null,&quot;^20&quot;,null,&quot;xp&quot;,50,&quot;^21&quot;,[],&quot;^22&quot;,[],&quot;^23&quot;,&quot;&quot;,&quot;^24&quot;,null,&quot;^25&quot;,null,&quot;^26&quot;,56.25,&quot;^27&quot;,&quot;course_1606_9d1f8a331d1200c7e1bdbfcaf3a7a491&quot;,&quot;key&quot;,&quot;da43858012&quot;,&quot;^28&quot;,&quot;python&quot;,&quot;^29&quot;,1606,&quot;^2:&quot;,4135,&quot;^14&quot;,null,&quot;^2;&quot;,&quot;v0&quot;,&quot;^2&lt;&quot;,0.7224866639917007,&quot;^2=&quot;,990670],[&quot;^ &quot;,&quot;id&quot;,42715,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1V&quot;,&quot;&lt;p&gt;In this interactive exercise, you&#39;ll learn how to use the BeautifulSoup package to &lt;em&gt;parse&lt;/em&gt;, &lt;em&gt;prettify&lt;/em&gt; and &lt;em&gt;extract&lt;/em&gt; information from HTML. You&#39;ll scrape the data from the webpage of Guido van Rossum, Python&#39;s very own &lt;a href=\\\\\\\\&quot;https://en.wikipedia.org/wiki/Benevolent_dictator_for_life\\\\\\\\&quot;&gt;Benevolent Dictator for Life&lt;/a&gt;. In the following exercises, you&#39;ll prettify the HTML and then extract the text and the hyperlinks.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of interest is &lt;code&gt;url = &#39;https://www.python.org/~guido/&#39;&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^1W&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\n\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;^1X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;BeautifulSoup&lt;/code&gt; from the package &lt;code&gt;bs4&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;html_doc&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Create a BeautifulSoup object &lt;code&gt;soup&lt;/code&gt; from the resulting HTML using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;prettify()&lt;/code&gt; on &lt;code&gt;soup&lt;/code&gt; and assign the result to &lt;code&gt;pretty_soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print to prettified HTML to your shell!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1N&quot;,10,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: call to prettify() and &#39;pretty_soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;pretty_soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.prettify\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1Y&quot;,&quot;&quot;,&quot;^1Z&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\npretty_soup = soup.prettify()\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;^1[&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Check the URL to make sure that you typed it in correctly.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the extracted &lt;em&gt;HTML&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;To use the &lt;code&gt;prettify()&lt;/code&gt; method on the BeautifulSoup object &lt;code&gt;soup&lt;/code&gt;, execute &lt;code&gt;soup.prettify()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the prettified HTML.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^20&quot;,null,&quot;xp&quot;,100,&quot;^21&quot;,[],&quot;^22&quot;,[],&quot;^23&quot;,&quot;&quot;,&quot;^28&quot;,&quot;python&quot;,&quot;^2&lt;&quot;,0.6153930141061901,&quot;^2=&quot;,42715],[&quot;^ &quot;,&quot;id&quot;,42716,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1V&quot;,&quot;&lt;p&gt;As promised, in the following exercises, you&#39;ll learn the basics of extracting information from HTML soup. In this exercise, you&#39;ll figure out how to extract the text from the BDFL&#39;s webpage, along with printing the webpage&#39;s title.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^1W&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;^1X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;In the sample code, the HTML response object &lt;code&gt;html_doc&lt;/code&gt; has already been created: your first task is to Soupify it using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt; and to assign the resulting soup to the variable &lt;code&gt;soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the title from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the attribute &lt;code&gt;title&lt;/code&gt; and assign the result to &lt;code&gt;guido_title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the title of Guido&#39;s webpage to the shell using the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the text from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the method &lt;code&gt;get_text()&lt;/code&gt; and assign to &lt;code&gt;guido_text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the text from Guido&#39;s webpage to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1N&quot;,11,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: &#39;guido_title&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_title\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;soup.title\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `soup.title` to create `guido_title`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\n# Test: call to soup.get_text() and &#39;guido_text&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_text\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.get_text\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(1)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1Y&quot;,&quot;&quot;,&quot;^1Z&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\nguido_title = soup.title\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\nprint(guido_title)\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\nguido_text = soup.get_text()\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;^1[&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML response object&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;title&lt;/code&gt; attribute of the object &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The object that contains the title of Guido&#39;s webpage is &lt;code&gt;guido_title&lt;/code&gt;; pass this as an argument to &lt;code&gt;print()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;get_text()&lt;/code&gt; on the HTML soup &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.get_text()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the text from Guido&#39;s webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^20&quot;,null,&quot;xp&quot;,100,&quot;^21&quot;,[],&quot;^22&quot;,[],&quot;^23&quot;,&quot;&quot;,&quot;^28&quot;,&quot;python&quot;,&quot;^2&lt;&quot;,0.8893605204424677,&quot;^2=&quot;,42716],[&quot;^ &quot;,&quot;id&quot;,42717,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1V&quot;,&quot;&lt;p&gt;In this exercise, you&#39;ll figure out how to extract the URLs of the hyperlinks from the BDFL&#39;s webpage. In the process, you&#39;ll become close friends with the soup method &lt;code&gt;find_all()&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^1W&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor ____ in ____:\\\\\\\\n    ____&quot;,&quot;^1X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;find_all()&lt;/code&gt; to find all hyperlinks in &lt;code&gt;soup&lt;/code&gt;, remembering that hyperlinks are defined by the HTML tag &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; but passed to &lt;code&gt;find_all()&lt;/code&gt; without angle brackets; store the result in the variable &lt;code&gt;a_tags&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The variable &lt;code&gt;a_tags&lt;/code&gt; is a results set: your job now is to enumerate over it, using a &lt;code&gt;for&lt;/code&gt; loop and to print the actual URLs of the hyperlinks; to do this, for every element &lt;code&gt;link&lt;/code&gt; in &lt;code&gt;a_tags&lt;/code&gt;, you want to &lt;code&gt;print()&lt;/code&gt; &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1N&quot;,12,&quot;sct&quot;,&quot;predef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nEx().check_correct(\\\\\\\\n    check_object(\\\\\\\\&quot;a_tags\\\\\\\\&quot;),\\\\\\\\n    check_function(\\\\\\\\&quot;soup.find_all\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_for_loop().multi(\\\\\\\\n        check_iter().has_equal_value(incorrect_msg = \\\\\\\\&quot;You have to iterate over `a_tags`\\\\\\\\&quot;),\\\\\\\\n        check_body().set_context(&#39;&lt;a href=\\\\\\\\&quot;pics.html\\\\\\\\&quot;&gt;&lt;img border=\\\\\\\\&quot;0\\\\\\\\&quot; src=\\\\\\\\&quot;images/IMG_2192.jpg\\\\\\\\&quot;/&gt;&lt;/a&gt;&#39;).check_function(\\\\\\\\&quot;print\\\\\\\\&quot;).check_args(0).check_function(\\\\\\\\&quot;link.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n    )\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;^1Y&quot;,&quot;&quot;,&quot;^1Z&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\na_tags = soup.find_all(&#39;a&#39;)\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor link in a_tags:\\\\\\\\n    print(link.get(&#39;href&#39;))&quot;,&quot;^1[&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML tag&lt;/em&gt; to find (without the angle brackets &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;) as a string argument to &lt;code&gt;find_all()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Recall that the &lt;code&gt;for&lt;/code&gt; loop recipe is: &lt;code&gt;for&lt;/code&gt; &lt;em&gt;loop variable&lt;/em&gt; &lt;code&gt;in&lt;/code&gt; &lt;em&gt;results set&lt;/em&gt;&lt;code&gt;:&lt;/code&gt;. Don&#39;t forget to pass &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt; as an argument to &lt;code&gt;print()&lt;/code&gt; inside the &lt;code&gt;for&lt;/code&gt; loop body.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^20&quot;,null,&quot;xp&quot;,100,&quot;^21&quot;,[],&quot;^22&quot;,[],&quot;^23&quot;,&quot;&quot;,&quot;^28&quot;,&quot;python&quot;,&quot;^2&lt;&quot;,0.6366817787546253,&quot;^2=&quot;,42717]]]]],&quot;activeImage&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^C&quot;,&quot;SUCCESS&quot;,&quot;^D&quot;,&quot;course-1606-master:cb59605c00ed73a970165be3564ff450-20220407091433345&quot;]]],&quot;sharedImage&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^C&quot;,&quot;NOT_FETCHED&quot;,&quot;^D&quot;,null]]]]]],&quot;settings&quot;,[&quot;^2&quot;,[&quot;uiTheme&quot;,&quot;DARK&quot;,&quot;feedbackRatingStatus&quot;,&quot;NONE&quot;]],&quot;streakInfo&quot;,[&quot;^ &quot;,&quot;^R&quot;,&quot;StreakUnknown&quot;],&quot;systemStatus&quot;,[&quot;^2&quot;,[&quot;indicator&quot;,&quot;none&quot;,&quot;description&quot;,&quot;No status has been fetched from the Status Page.&quot;]],&quot;user&quot;,[&quot;^2&quot;,[&quot;status&quot;,&quot;not_initiate&quot;,&quot;settings&quot;,[&quot;^2&quot;,[]]]]]]]\";</script><div id=\"root\"><div class=\"theme progress-indicator--visible\"><style data-emotion=\"css p4fmi8\">.css-p4fmi8{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#F7F3EB;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:50px;padding-left:10px;padding-right:10px;position:relative;z-index:15;}</style><header data-cy=\"alpa-navbar\" class=\"css-p4fmi8\"><style data-emotion=\"css yp9swi\">.css-yp9swi{-webkit-flex:1;-ms-flex:1;flex:1;}</style><div class=\"css-yp9swi\"><style data-emotion=\"css ew67gc\">.css-ew67gc{border:0;border-width:0;padding:6px;}</style><style data-emotion=\"css 1jcub2v\">.css-1jcub2v{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:8px;border:0;border-width:0;padding:6px;}.css-1jcub2v:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-1jcub2v:disabled,.css-1jcub2v:hover:disabled,.css-1jcub2v:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-1jcub2v:focus{outline:0;}.css-1jcub2v:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><a aria-label=\"landing\" class=\"css-1jcub2v\" href=\"https://www.datacamp.com\" data-cy=\"header-logo\" data-testid=\"alpa-navbar-logo\"><svg height=\"28\" viewbox=\"0 0 173 36\" width=\"134.55555555555554\" aria-hidden=\"true\" color=\"currentColor\" title=\"\"><g fill=\"none\" fill-rule=\"evenodd\"><path d=\"M42.56 27.1a5.694 5.694 0 110-11.39 5.694 5.694 0 010 11.39m5.704-20.623v8.853a8.334 8.334 0 100 12.148v1.836h2.632V6.477h-2.632zm73.28 20.622a5.694 5.694 0 110-11.389 5.694 5.694 0 010 11.39m8.333-5.695v-8.247h-2.63v2.172a8.334 8.334 0 100 12.148v1.836h2.631v-7.91h-.001zm20.987-7.634a1.296 1.296 0 011.109-.622h.507c1.075 0 1.947.872 1.947 1.947v14.218h-2.686V17.269c-1.239 2-5.674 9.25-7.003 11.424a1.296 1.296 0 01-1.108.62h-.548a1.298 1.298 0 01-1.298-1.297V17.238a1909.582 1909.582 0 00-7.31 11.954l-.074.122h-2.574v-16.16h2.684v.033l-.062 11.147 6.438-10.56a1.3 1.3 0 011.11-.622h.51c1.073 0 1.944.869 1.947 1.942 0 2.972.014 8.383.014 9.17l6.397-10.493zm-37.92 12.541a8.331 8.331 0 11.21-9.502l-2.524 1.312a5.533 5.533 0 10-.379 6.88l2.693 1.31zm51.542.8a5.693 5.693 0 01-5.68-5.352v-.682a5.694 5.694 0 115.684 6.036m0-14.028a8.298 8.298 0 00-5.684 2.24v-2.168h-2.632V35.91h2.632v-8.4a8.333 8.333 0 105.684-14.425M75.277 15.68v9.938c0 .589.478 1.067 1.067 1.067h3.064v2.629h-3.062a3.7 3.7 0 01-3.696-3.696l-.01-9.938h-2.838v-2.56h2.838V8.702h2.635v4.428h4.672v2.55h-4.67zm12.757 11.418a5.694 5.694 0 110-11.39 5.694 5.694 0 010 11.39m5.702-13.941v2.173a8.334 8.334 0 100 12.148v1.836h2.632v-16.16l-2.632.003zM60.285 27.099a5.694 5.694 0 110-11.389 5.694 5.694 0 010 11.39m5.702-13.942v2.172a8.334 8.334 0 100 12.148v1.836h2.63v-16.16l-2.63.004z\" fill=\"#05192D\"/><path d=\"M11.7 8.514v8.332L2.857 21.89V3.44l8.841 5.074zm2.86 17.507v-7.51l11.84-6.757-2.88-1.65-8.96 5.112V7.68a1.44 1.44 0 00-.718-1.242L3.056.256A2.066 2.066 0 000 2.07v21.184a2.067 2.067 0 002.971 1.866l.082-.042 8.64-4.932v6.72c.002.513.276.987.721 1.243L23.502 34.4l2.88-1.651L14.56 26.02z\" fill=\"#05192D\"/></g></svg></a></div><style data-emotion=\"css 1f915o0\">.css-1f915o0{-webkit-box-pack:\\'initial\\';-ms-flex-pack:\\'initial\\';-webkit-justify-content:\\'initial\\';justify-content:\\'initial\\';}</style><div class=\"css-1f915o0\"><div class=\"dc-nav-course__container\"><style data-emotion=\"css 1nxd4b6\">.css-1nxd4b6{border:0;height:36px;-webkit-box-pack:initial;-ms-flex-pack:initial;-webkit-justify-content:initial;justify-content:initial;}</style><nav class=\"dc-nav-course css-1nxd4b6\"><style data-emotion=\"css ftus1d\">.css-ftus1d{z-index:1;border-width:2px;border-radius:4px 0px 0px 4px;}</style><style data-emotion=\"css 1sdywd0\">.css-1sdywd0{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:8px;z-index:1;border-width:2px;border-radius:4px 0px 0px 4px;}.css-1sdywd0:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-1sdywd0:disabled,.css-1sdywd0:hover:disabled,.css-1sdywd0:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-1sdywd0:focus{outline:0;}.css-1sdywd0:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><a aria-label=\"Go to previous exercise\" class=\"css-1sdywd0\" href=\"/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1\" data-cy=\"header-previous\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M4.42 8L16 7.998a1 1 0 010 2L4.41 10l3.285 3.296a.998.998 0 11-1.417 1.41l-4.93-4.948A.998.998 0 011.36 8.23l4.933-4.938a1 1 0 011.414 0c.39.391.39 1.025 0 1.416L4.42 7.999z\" fill-rule=\"evenodd\"/></svg></a><style data-emotion=\"css 96nxkt\">.css-96nxkt{border-radius:0;margin:0 -2px;border-width:2px;}</style><style data-emotion=\"css b29ve4\">.css-b29ve4{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:0 15px;border-radius:0;margin:0 -2px;border-width:2px;}.css-b29ve4:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-b29ve4:disabled,.css-b29ve4:hover:disabled,.css-b29ve4:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-b29ve4:focus{outline:0;}.css-b29ve4:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><button class=\"css-b29ve4\" type=\"button\" data-cy=\"header-outline\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M4 6a1 1 0 110-2h10a1 1 0 010 2H4zm0 4a1 1 0 110-2h10a1 1 0 010 2H4zm0 4a1 1 0 010-2h10a1 1 0 010 2H4z\" fill-rule=\"evenodd\"/></svg><style data-emotion=\"css aib9ji\">.css-aib9ji{font-size:14px;line-height:32px;color:#05192D;font-weight:bold;margin-left:8px;}</style><style data-emotion=\"css vvk465\">.css-vvk465{-webkit-font-smoothing:antialiased;color:#05192D;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-style:normal;font-size:14px;font-weight:400;font-size:14px;line-height:32px;color:#05192D;font-weight:bold;margin-left:8px;}</style><span class=\"css-vvk465\">Course Outline</span></button><style data-emotion=\"css q5k7z8\">.css-q5k7z8{z-index:1;border-width:2px;border-radius:0px 4px 4px 0px;}</style><style data-emotion=\"css 11zm6tc\">.css-11zm6tc{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:8px;z-index:1;border-width:2px;border-radius:0px 4px 4px 0px;}.css-11zm6tc:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-11zm6tc:disabled,.css-11zm6tc:hover:disabled,.css-11zm6tc:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-11zm6tc:focus{outline:0;}.css-11zm6tc:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><a aria-label=\"Go to next exercise\" class=\"css-11zm6tc\" href=\"/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3\" data-cy=\"header-next\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M13.58 10L2 10.002a1 1 0 010-2L13.59 8l-3.285-3.296a.998.998 0 111.417-1.41l4.93 4.948a.998.998 0 01-.012 1.527l-4.933 4.938a1 1 0 01-1.414 0 1.002 1.002 0 010-1.416l3.287-3.29z\" fill-rule=\"evenodd\"/></svg></a></nav><style data-emotion=\"css zs9gal fywrg5 728dx5 1d9ftqx atcdtd 728dx5 d3v9zr\">.css-zs9gal{opacity:1!important;-webkit-transform:none!important;-moz-transform:none!important;-ms-transform:none!important;transform:none!important;}.css-fywrg5{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;min-width:0;opacity:0;outline:none;position:relative;-webkit-transform:scale(0.5);-moz-transform:scale(0.5);-ms-transform:scale(0.5);transform:scale(0.5);-webkit-transition:0.4s cubic-bezier(0.19, 1, 0.22, 1);transition:0.4s cubic-bezier(0.19, 1, 0.22, 1);box-sizing:border-box;max-height:100%;padding:8px;width:916px;}.css-728dx5{opacity:0!important;}.css-1d9ftqx{opacity:1!important;}.css-atcdtd{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(5, 25, 45, 0.8);bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;left:0;opacity:0;position:fixed;right:0;top:0;-webkit-transition:opacity 0.6s cubic-bezier(0.19, 1, 0.22, 1);transition:opacity 0.6s cubic-bezier(0.19, 1, 0.22, 1);z-index:900;}.css-d3v9zr{overflow:hidden;}</style></div></div><style data-emotion=\"css s01fge\">.css-s01fge{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-ms-flex-pack:end;-webkit-justify-content:flex-end;justify-content:flex-end;}</style><nav class=\"css-s01fge\"><style data-emotion=\"css 1dskn3o\">.css-1dskn3o{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-ms-flex-pack:end;-webkit-justify-content:flex-end;justify-content:flex-end;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><nav class=\"css-1dskn3o\"><div data-cy=\"header-session\" class=\"dc-u-fx dc-u-fx-aic dc-u-mr-8\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"false\" height=\"16\" role=\"img\" width=\"16\"><title>Session Ready</title><path fill=\"#03EF62\" d=\"M9 18A9 9 0 119 0a9 9 0 010 18z\" fill-rule=\"evenodd\"/></svg></div><style data-emotion=\"css 15xw4wa\">.css-15xw4wa{border:none;color:#05192D;}</style><style data-emotion=\"css 1rlvrrk\">.css-1rlvrrk{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:8px;border:none;color:#05192D;}.css-1rlvrrk:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-1rlvrrk:disabled,.css-1rlvrrk:hover:disabled,.css-1rlvrrk:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-1rlvrrk:focus{outline:0;}.css-1rlvrrk:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><button aria-label=\"Continue learning on mobile\" class=\"css-1rlvrrk\" type=\"button\" data-cy=\"header-mobile\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M5.5 2v14h7V2h-7zm-1-2h9a1 1 0 011 1v16a1 1 0 01-1 1h-9a1 1 0 01-1-1V1a1 1 0 011-1zm4 13h1a1 1 0 010 2h-1a1 1 0 010-2z\" fill-rule=\"evenodd\"/></svg></button><button aria-label=\"Show Video\" class=\"css-1rlvrrk\" type=\"button\" data-cy=\"header-video\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M13 6.3l3.331-2.998A1 1 0 0118 4.045v9.91a1 1 0 01-1.669.743L13 11.7V14c0 .552-.485 1-1.083 1H1.083C.485 15 0 14.552 0 14V4c0-.552.485-1 1.083-1h10.834C12.515 3 13 3.448 13 4v2.3zm0 2.69v.02l3 2.7V6.29l-3 2.7zM2 5v8h9V5H2z\" fill-rule=\"evenodd\"/></svg></button><button aria-label=\"Show Slides\" class=\"css-1rlvrrk\" type=\"button\" data-cy=\"header-slides\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M14 9.004H9.996a2 2 0 01-2-2V2H4v14h10V9.004zm1.828-2.815A1.938 1.938 0 0116 7v9a2 2 0 01-2 2H4a2 2 0 01-2-2V2a2 2 0 012-2h5.003a2 2 0 011.415.586l4.997 5a2 2 0 01.413.603zm-1.832.815l-4-4v4h4z\" fill-rule=\"evenodd\"/></svg></button><button aria-label=\"Provide Feedback\" class=\"css-1rlvrrk\" type=\"button\" data-cy=\"header-issue\" data-test-id=\"header-report-issue-button\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M9 16A7 7 0 109 2a7 7 0 000 14zm0 2A9 9 0 119 0a9 9 0 010 18zm0-4a1 1 0 110-2 1 1 0 010 2zM8 5a1 1 0 112 0v5a1 1 0 01-2 0V5z\" fill-rule=\"evenodd\"/></svg></button></nav></nav></header><main class=\"exercise-area\"><div data-cy=\"server-side-loader-placeholder\"><aside class=\"exercise--sidebar\" style=\"width:40%\"><div class=\"exercise--sidebar-content\"><div class=\"listview__outer\"><div class=\"listview__inner\"><div class=\"listview__section\"><div><div role=\"button\" class=\"listview__header\"><div class=\"exercise--sidebar-header\"><h5 class=\"dc-panel__title\"><svg aria-label=\"exercise icon\" class=\"dc-icon-exercise dc-u-color-navy dc-u-mr-8\" fill=\"currentColor\" height=\"12\" role=\"Img\" width=\"12\"><use xlink:href=\"/static/media/symbols.e369b265.svg#exercise\"/></svg>Exercise</h5></div></div></div><div class=\"listview__content\"><div class=\"exercise--assignment exercise--typography\"><h1 class=\"exercise--title\">Importing flat files from the web: your turn!</h1><div class><p>You are about to import your first file from the web! The flat file you will import will be <code>&apos;winequality-red.csv&apos;</code> from the University of California, Irvine&apos;s <a href=\"https://archive.ics.uci.edu/ml/index.php\">Machine Learning repository</a>. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.</p>\\n<p>The URL of the file is</p>\\n<pre><code>&apos;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&apos;\\n</code></pre>\\n<p>After you import it, you&apos;ll check your working directory to confirm that it is there and then you&apos;ll load it into a <code>pandas</code> DataFrame.</p></div></div></div></div><div class=\"listview__section\" style=\"min-height:calc(100% - 33px)\"><div><div role=\"button\" class=\"listview__header\"><div class=\"exercise--sidebar-header\"><h5 class=\"dc-panel__title\"><svg aria-label=\"checkmark_circle icon\" class=\"dc-icon-checkmark_circle dc-u-color-navy dc-u-mr-8\" fill=\"currentColor\" height=\"12\" role=\"Img\" width=\"12\"><use xlink:href=\"/static/media/symbols.e369b265.svg#checkmark_circle\"/></svg>Instructions</h5><style data-emotion=\"css 6996zu\">.css-6996zu{border-radius:4px;display:inline-block;text-transform:uppercase;background-color:#FCCE0D;color:#05192D;font-size:12px;line-height:18px;padding-left:4px;padding-right:4px;}</style><style data-emotion=\"css 1o8nzjk\">.css-1o8nzjk{-webkit-font-smoothing:antialiased;color:#05192D;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-style:normal;font-weight:800;line-height:1.5;border-radius:4px;display:inline-block;text-transform:uppercase;background-color:#FCCE0D;color:#05192D;font-size:12px;line-height:18px;padding-left:4px;padding-right:4px;}</style><strong class=\"css-1o8nzjk\">100 XP</strong></div></div></div><div class=\"listview__content\"><div><div class><div class=\"exercise--instructions exercise--typography\"><div class=\"exercise--instructions__content\"><ul>\\n<li>Import the function <code>urlretrieve</code> from the subpackage <code>urllib.request</code>.</li>\\n<li>Assign the URL of the file to the variable <code>url</code>.</li>\\n<li>Use the function <code>urlretrieve()</code> to save the file locally as <code>&apos;winequality-red.csv&apos;</code>.</li>\\n<li>Execute the remaining code to load <code>&apos;winequality-red.csv&apos;</code> in a pandas DataFrame and to print its head to the shell.</li>\\n</ul></div><div style=\"margin:16px -15px 0\"><section class=\"dc-sct-feedback\" tabindex=\"-1\"><div></div><nav class=\"dc-sct-feedback__nav\"><style data-emotion=\"css 6is1tf\">.css-6is1tf{padding-left:16px;}</style><div class=\"css-6is1tf\"><style data-emotion=\"css 12j1yck\">.css-12j1yck{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:0 15px;}.css-12j1yck:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-12j1yck:disabled,.css-12j1yck:hover:disabled,.css-12j1yck:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-12j1yck:focus{outline:0;}.css-12j1yck:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><button class=\"css-12j1yck\" type=\"button\" data-cy=\"exercise-show-hint\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M9 0a7 7 0 014.95 11.95l-.001-.001c-.794.795-.949 1.1-.949 2.051a1 1 0 01-2 0c0-1.548.396-2.325 1.535-3.467l.04-.037a5 5 0 10-7.11.037C6.605 11.675 7 12.453 7 14a1 1 0 01-2 0c0-.951-.155-1.256-.949-2.051A7 7 0 019 0zm0 7a1 1 0 011 1v6a1 1 0 01-2 0V8a1 1 0 011-1zm0 11c-1.657 0-3-.895-3-2h6c0 1.105-1.343 2-3 2z\" fill-rule=\"evenodd\"/></svg><style data-emotion=\"css aib9ji\">.css-aib9ji{font-size:14px;line-height:32px;color:#05192D;font-weight:bold;margin-left:8px;}</style><style data-emotion=\"css vvk465\">.css-vvk465{-webkit-font-smoothing:antialiased;color:#05192D;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-style:normal;font-size:14px;font-weight:400;font-size:14px;line-height:32px;color:#05192D;font-weight:bold;margin-left:8px;}</style><span class=\"css-vvk465\">Take Hint (-30 XP)</span></button></div></nav></section></div></div></div></div></div></div></div></div></div></aside><section class=\"exercise--content\" style=\"width:60%\"><div class=\"exercise-waiting\"><div class=\"global-spinner dc-u-fx-jcc dc-u-fx\"><style data-emotion=\"css 1f2mbny\">.css-1f2mbny{height:70px;width:70px;}</style><div class=\"css-1f2mbny\"><style data-emotion=\"css 1idrum8\">.css-1idrum8{-webkit-animation:animation-1pv1bkr cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;animation:animation-1pv1bkr cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;margin:auto;width:76.65%;will-change:clip-path;}@-webkit-keyframes animation-1pv1bkr{0%,6%{-webkit-clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);}100%{-webkit-clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);}}@keyframes animation-1pv1bkr{0%,6%{-webkit-clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);}100%{-webkit-clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);}}</style><div class=\"css-1idrum8\"><style data-emotion=\"css 1j8nxo animation-1h2cwi2\">.css-1j8nxo{-webkit-animation:animation-1h2cwi2 cubic-bezier(0, 0, 0.85, 1) 2s infinite alternate;animation:animation-1h2cwi2 cubic-bezier(0, 0, 0.85, 1) 2s infinite alternate;will-change:clip-path;}@-webkit-keyframes animation-1h2cwi2{0%,71%{-webkit-clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);}96%,100%{-webkit-clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);}}@keyframes animation-1h2cwi2{0%,71%{-webkit-clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);}96%,100%{-webkit-clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);}}</style><div class=\"css-1j8nxo\"><style data-emotion=\"css 5h5b50\">.css-5h5b50{-webkit-clip-path:polygon(-0.1% -10%, 169% 65%, -0.1% 139%);clip-path:polygon(-0.1% -10%, 169% 65%, -0.1% 139%);}</style><div class=\"css-5h5b50\"><style data-emotion=\"css 4zleql\">.css-4zleql{display:block;}</style><svg version=\"1.1\" viewbox=\"0 0 2640 3444\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" class=\"css-4zleql\"><title>Loading</title><style data-emotion=\"css jy99qt animation-co7x2c\">.css-jy99qt{-webkit-animation:animation-co7x2c cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;animation:animation-co7x2c cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;stroke-dasharray:9800;stroke-dashoffset:9800;will-change:stroke-dashoffset;}@-webkit-keyframes animation-co7x2c{100%{stroke-dashoffset:0;}}@keyframes animation-co7x2c{100%{stroke-dashoffset:0;}}</style><path d=\"M0 0 M2569 1056L143 2447V149l1175 673v1867l1248 715\" fill=\"none\" stroke=\"#05192d\" stroke-linejoin=\"round\" stroke-width=\"300\" class=\"css-jy99qt\"/></svg></div></div></div></div></div><noscript></noscript></div></section></div><style data-emotion=\"css dhfy3a 7y8jxc 14w24v3 1yuhvjn\">.css-dhfy3a{-webkit-animation-name:animation-2ijyvo;animation-name:animation-2ijyvo;-webkit-animation-timing-function:cubic-bezier(0.23, 1, 0.32, 1);animation-timing-function:cubic-bezier(0.23, 1, 0.32, 1);}@-webkit-keyframes animation-2ijyvo{50%{opacity:1;}from{opacity:0;-webkit-transform:scale3d(0.3, 0.3, 0.3);-moz-transform:scale3d(0.3, 0.3, 0.3);-ms-transform:scale3d(0.3, 0.3, 0.3);transform:scale3d(0.3, 0.3, 0.3);}}@keyframes animation-2ijyvo{50%{opacity:1;}from{opacity:0;-webkit-transform:scale3d(0.3, 0.3, 0.3);-moz-transform:scale3d(0.3, 0.3, 0.3);-ms-transform:scale3d(0.3, 0.3, 0.3);transform:scale3d(0.3, 0.3, 0.3);}}.css-7y8jxc{-webkit-animation-name:animation-1phn0oq;animation-name:animation-1phn0oq;-webkit-animation-timing-function:cubic-bezier(0.755, 0.05, 0.855, 0.06);animation-timing-function:cubic-bezier(0.755, 0.05, 0.855, 0.06);}@-webkit-keyframes animation-1phn0oq{50%{opacity:0;-webkit-transform:scale3d(0.3, 0.3, 0.3);-moz-transform:scale3d(0.3, 0.3, 0.3);-ms-transform:scale3d(0.3, 0.3, 0.3);transform:scale3d(0.3, 0.3, 0.3);}from{opacity:1;}to{opacity:0;}}@keyframes animation-1phn0oq{50%{opacity:0;-webkit-transform:scale3d(0.3, 0.3, 0.3);-moz-transform:scale3d(0.3, 0.3, 0.3);-ms-transform:scale3d(0.3, 0.3, 0.3);transform:scale3d(0.3, 0.3, 0.3);}from{opacity:1;}to{opacity:0;}}.css-14w24v3{left:50%;position:fixed;top:0;-webkit-transform:translateX(-50%);-moz-transform:translateX(-50%);-ms-transform:translateX(-50%);transform:translateX(-50%);z-index:999;}.css-14w24v3 .Toastify__progress-bar{-webkit-animation:animation-qqoh2i linear 1;animation:animation-qqoh2i linear 1;}@-webkit-keyframes animation-qqoh2i{0%{-webkit-transform:scaleX(1);-moz-transform:scaleX(1);-ms-transform:scaleX(1);transform:scaleX(1);}100%{-webkit-transform:scaleX(0);-moz-transform:scaleX(0);-ms-transform:scaleX(0);transform:scaleX(0);}}@keyframes animation-qqoh2i{0%{-webkit-transform:scaleX(1);-moz-transform:scaleX(1);-ms-transform:scaleX(1);transform:scaleX(1);}100%{-webkit-transform:scaleX(0);-moz-transform:scaleX(0);-ms-transform:scaleX(0);transform:scaleX(0);}}.css-1yuhvjn{margin-top:16px;}</style><div class=\"Toastify\"></div></main><div class=\"exercise-footer\"><ul data-cy=\"progress-container\" class=\"dc-progress-indicator\"><li class=\"dc-progress-indicator__item\"><a href=\"javascript:void(0)\" class=\"dc-progress-indicator__bar\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li><li class=\"dc-progress-indicator__item\"><a href=\"javascript:void(0)\" class=\"dc-progress-indicator__bar\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li><li class=\"dc-progress-indicator__item\"><a href=\"javascript:void(0)\" class=\"dc-progress-indicator__bar\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li></ul></div><style data-emotion=\"css zs9gal 13qqqtf 728dx5 1d9ftqx atcdtd 728dx5 d3v9zr\">.css-zs9gal{opacity:1!important;-webkit-transform:none!important;-moz-transform:none!important;-ms-transform:none!important;transform:none!important;}.css-13qqqtf{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;min-width:0;opacity:0;outline:none;position:relative;-webkit-transform:scale(0.5);-moz-transform:scale(0.5);-ms-transform:scale(0.5);transform:scale(0.5);-webkit-transition:0.4s cubic-bezier(0.19, 1, 0.22, 1);transition:0.4s cubic-bezier(0.19, 1, 0.22, 1);box-sizing:border-box;max-height:100%;padding:8px;width:496px;}.css-728dx5{opacity:0!important;}.css-1d9ftqx{opacity:1!important;}.css-atcdtd{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(5, 25, 45, 0.8);bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;left:0;opacity:0;position:fixed;right:0;top:0;-webkit-transition:opacity 0.6s cubic-bezier(0.19, 1, 0.22, 1);transition:opacity 0.6s cubic-bezier(0.19, 1, 0.22, 1);z-index:900;}.css-d3v9zr{overflow:hidden;}</style></div></div><script>window.MathJax={options:{ignoreHtmlClass:\"tex2jax_ignore\",processHtmlClass:\"tex2jax_process\"},tex:{autoload:{color:[],colorV2:[\"color\"]},packages:{\"[+]\":[\"noerrors\"]}},loader:{load:[\"[tex]/noerrors\"]}}</script><script src=\"/campus/mathjax@3/es5/tex-chtml.js\" id=\"MathJax-script\"></script><script>!function(e){function t(t){for(var n,o,f=t[0],u=t[1],d=t[2],i=0,s=[];i<f.length;i++)o=f[i],Object.prototype.hasOwnProperty.call(a,o)&&a[o]&&s.push(a[o][0]),a[o]=0;for(n in u)Object.prototype.hasOwnProperty.call(u,n)&&(e[n]=u[n]);for(l&&l(t);s.length;)s.shift()();return c.push.apply(c,d||[]),r()}function r(){for(var e,t=0;t<c.length;t++){for(var r=c[t],n=!0,o=1;o<r.length;o++){var u=r[o];0!==a[u]&&(n=!1)}n&&(c.splice(t--,1),e=f(f.s=r[0]))}return e}var n={},o={14:0},a={14:0},c=[];function f(t){if(n[t])return n[t].exports;var r=n[t]={i:t,l:!1,exports:{}};return e[t].call(r.exports,r,r.exports,f),r.l=!0,r.exports}f.e=function(e){var t=[];o[e]?t.push(o[e]):0!==o[e]&&{1:1,2:1,5:1,8:1,9:1,10:1,13:1,17:1,18:1,20:1,21:1}[e]&&t.push(o[e]=new Promise((function(t,r){for(var n=\"static/css/\"+({7:\"console-monaco\",8:\"dnde\",9:\"ee\",10:\"idee\",12:\"monaco\",13:\"rde\",15:\"xterm\"}[e]||e)+\".\"+{0:\"31d6cfe0\",1:\"e8c7adfe\",2:\"4d201c8f\",3:\"31d6cfe0\",4:\"31d6cfe0\",5:\"a014e8d0\",6:\"31d6cfe0\",7:\"31d6cfe0\",8:\"b5d0672e\",9:\"2bf50755\",10:\"f128b9fc\",12:\"31d6cfe0\",13:\"2299b9ab\",15:\"31d6cfe0\",17:\"4846b048\",18:\"3d94959c\",19:\"31d6cfe0\",20:\"8f55c4a5\",21:\"e16192f0\",22:\"31d6cfe0\",23:\"31d6cfe0\",24:\"31d6cfe0\",25:\"31d6cfe0\",26:\"31d6cfe0\",27:\"31d6cfe0\",28:\"31d6cfe0\"}[e]+\".chunk.css\",a=f.p+n,c=document.getElementsByTagName(\"link\"),u=0;u<c.length;u++){var d=(l=c[u]).getAttribute(\"data-href\")||l.getAttribute(\"href\");if(\"stylesheet\"===l.rel&&(d===n||d===a))return t()}var i=document.getElementsByTagName(\"style\");for(u=0;u<i.length;u++){var l;if((d=(l=i[u]).getAttribute(\"data-href\"))===n||d===a)return t()}var s=document.createElement(\"link\");s.rel=\"stylesheet\",s.type=\"text/css\",s.onload=t,s.onerror=function(t){var n=t&&t.target&&t.target.src||a,c=new Error(\"Loading CSS chunk \"+e+\" failed.\\\\n(\"+n+\")\");c.code=\"CSS_CHUNK_LOAD_FAILED\",c.request=n,delete o[e],s.parentNode.removeChild(s),r(c)},s.href=a,document.getElementsByTagName(\"head\")[0].appendChild(s)})).then((function(){o[e]=0})));var r=a[e];if(0!==r)if(r)t.push(r[2]);else{var n=new Promise((function(t,n){r=a[e]=[t,n]}));t.push(r[2]=n);var c,u=document.createElement(\"script\");u.charset=\"utf-8\",u.timeout=120,f.nc&&u.setAttribute(\"nonce\",f.nc),u.src=function(e){return f.p+\"static/js/\"+({7:\"console-monaco\",8:\"dnde\",9:\"ee\",10:\"idee\",12:\"monaco\",13:\"rde\",15:\"xterm\"}[e]||e)+\".\"+{0:\"6087a32d\",1:\"69268e53\",2:\"880022c3\",3:\"63bfc8e5\",4:\"a38eddbe\",5:\"a69b8abd\",6:\"a3493094\",7:\"f4cc1e3c\",8:\"23fc6ba2\",9:\"f6a72292\",10:\"544a9f9c\",12:\"84140e79\",13:\"ab7085a0\",15:\"2f7f220a\",17:\"695dbd66\",18:\"6531c83e\",19:\"b054c042\",20:\"dfee9247\",21:\"52fe4c73\",22:\"10883438\",23:\"5e529106\",24:\"e8147699\",25:\"256bb74b\",26:\"410157cf\",27:\"dc8b285a\",28:\"49c82727\"}[e]+\".chunk.js\"}(e);var d=new Error;c=function(t){u.onerror=u.onload=null,clearTimeout(i);var r=a[e];if(0!==r){if(r){var n=t&&(\"load\"===t.type?\"missing\":t.type),o=t&&t.target&&t.target.src;d.message=\"Loading chunk \"+e+\" failed.\\\\n(\"+n+\": \"+o+\")\",d.name=\"ChunkLoadError\",d.type=n,d.request=o,r[1](d)}a[e]=void 0}};var i=setTimeout((function(){c({type:\"timeout\",target:u})}),12e4);u.onerror=u.onload=c,document.head.appendChild(u)}return Promise.all(t)},f.m=e,f.c=n,f.d=function(e,t,r){f.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:r})},f.r=function(e){\"undefined\"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(e,\"__esModule\",{value:!0})},f.t=function(e,t){if(1&t&&(e=f(e)),8&t)return e;if(4&t&&\"object\"==typeof e&&e&&e.__esModule)return e;var r=Object.create(null);if(f.r(r),Object.defineProperty(r,\"default\",{enumerable:!0,value:e}),2&t&&\"string\"!=typeof e)for(var n in e)f.d(r,n,function(t){return e[t]}.bind(null,n));return r},f.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return f.d(t,\"a\",t),t},f.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},f.p=\"/campus/\",f.oe=function(e){throw console.error(e),e};var u=this[\"webpackJsonpcampus-app-v2\"]=this[\"webpackJsonpcampus-app-v2\"]||[],d=u.push.bind(u);u.push=t,u=u.slice();for(var i=0;i<u.length;i++)t(u[i]);var l=d;r()}([])</script><script src=\"/campus/static/js/16.e08a129a.chunk.js\"></script><script src=\"/campus/static/js/main.7756758e.chunk.js\"></script><script>(function(){window[\\'__CF$cv$params\\']={r:\\'738fe7ed0e5ab2b1\\',m:\\'KtvHhW8R4ryKTeKUwuxSRtn0OG6JpgW3OKEg2TOdvKE-1660209737-0-AXe7k0jMd/KeYad98+vBg+BPvuUqa5VLFSNs7fbkIG9CW8vYjFhWSCRUCYkxnGi1pqP+bsRqhtZCqLkyoZlO/X7gBfoDhkaN4mLJekFNBAgAxSDWS5vFmQcyqHqVa59no2RFtuTyCgtxJVpzEU/gE7w=\\',s:[0x24151d4b68,0x079880b5d3],}})();</script></body></html>'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performing HTTP requests in Python using requests\n",
    "\n",
    "<p>Now that you've got your head and hands around making HTTP requests using the urllib package, you're going to figure out how to do the same using the higher-level requests library. You'll once again be pinging DataCamp servers for their <code>\"http://www.datacamp.com/teach/documentation\"</code> page.</p>\n",
    "<p>Note that unlike in the previous exercises using urllib, you don't have to close the connection when using requests!</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Import the package <code>requests</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Import package\n",
    "import requests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assign the URL of interest to the variable <code>url</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "\n",
    "# Specify the url: url\n",
    "url = \"http://www.datacamp.com/teach/documentation\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Package the request to the URL, send the request and catch the response with a single function <code>requests.get()</code>, assigning the response to the variable <code>r</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "\n",
    "# Packages the request, send the request and catch the response: r\n",
    "r = requests.get(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the <code>text</code> attribute of the object <code>r</code> to return the HTML of the webpage as a string; store the result in a variable <code>text</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "\n",
    "# Extract the response: text\n",
    "text = r.text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hit submit to print the HTML of the webpage."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en-US\">\n",
      "<head>\n",
      "    <title>Just a moment...</title>\n",
      "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
      "    <meta name=\"robots\" content=\"noindex,nofollow\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
      "    <link href=\"/cdn-cgi/styles/cf-errors.css\" rel=\"stylesheet\" />\n",
      "    <meta http-equiv=\"refresh\" content=\"35\">\n",
      "<script>\n",
      "    (function(){\n",
      "        window._cf_chl_opt={\n",
      "            cvId: '2',\n",
      "            cType: 'non-interactive',\n",
      "            cNounce: '11951',\n",
      "            cRay: '738fea87b9e7a901',\n",
      "            cHash: '152f149d20ce34c',\n",
      "            cUPMDTk: \"\\/teach\\/documentation?__cf_chl_tk=0ks73Vl_5RrQjl8GCS7oBS.rY1wxqOjBbjPFbpgRZu8-1660209844-0-gaNycGzNCBE\",\n",
      "            cFPWv: 'g',\n",
      "            cTTimeMs: '1000',\n",
      "            cTplV: 3,\n",
      "            cRq: {\n",
      "                ru: 'aHR0cHM6Ly93d3cuZGF0YWNhbXAuY29tL3RlYWNoL2RvY3VtZW50YXRpb24=',\n",
      "                ra: 'cHl0aG9uLXJlcXVlc3RzLzIuMjguMA==',\n",
      "                rm: 'R0VU',\n",
      "                d: '4QGP1X0jbhpAu+3mugI/yBPu86HqHkfF7VJ76HTcGSBFQ6k7wT1TesTILQLcrzYdcNoLLXbvOCGrYocVb3CQKyHQjJ9q4UIUuU6mYRfZheh/7UuHKd7IdW0fPcZ3qekcn8yQ+UbO36H5EwPfD4oeoAIVW3L/EZmWDziIUbEi/0Ou/4bVwkvITO4ZFHkQFMCL7ApdEM/BpmCj7o3ZoISnEpoMWVwygtmVyqtSkhbFxHwT0G4xZWrICuguJgMVTLS0vOeo7l9O9VaYUE8ygAIlxxWy1h86cGIjhuZl5wM67cNbuh/UEZQ7ZDgjBhabjWmi27TtMkcgkTLydFOhbloET5UUH1wFBtSIqotaryMYBGZ5eOFjAn5Sf4I9frDBVTs2Xe8nCjysC3ejvVM/FW4eYMfnqfkeaHOkpBuel33y4XHgcAL6YtOqGD6AvZsoeWBgUSw8sgQElUE1A41uAxtnjnet5Dy8qWAsIWvuRwjJr21pAGMl52ivLM/m5zOwQHmbXaN/dWg8+0B4kTrs9sKz+MK5+lPu8WYYdlJc2clHBJkW4QfOE40Or+kCQ9SwwTTA6IhI3jaMWXWV7WE5BGYWsw==',\n",
      "                t: 'MTY2MDIwOTg0NC40NDUwMDA=',\n",
      "                m: 'eddk+oMrDg6LEl7PByWiaQpRKSeURYOL8SUEcrxzT0Y=',\n",
      "                i1: 'CUftF1U3//kWKycQQX3WTg==',\n",
      "                i2: 'xlqefVPnuF0zB6HVV6cjXg==',\n",
      "                zh: 'hzfiqo9hugT9sHeHQ1zy81NCL/S0295H0+GuRnkSV9o=',\n",
      "                uh: 'KXOU42hMUUNBCj8raB/cd2As9NbLMb3/GhNBEShv6ew=',\n",
      "                hh: 'rAZnIHiyrNuZ60h9aAZNML8izDilqmOSNuCtac1WqPs=',\n",
      "            }\n",
      "        }\n",
      "        window._cf_chl_enter = function(){window._cf_chl_opt.p=1};\n",
      "    })();\n",
      "</script>\n",
      "\n",
      "</head>\n",
      "<body class=\"no-js\">\n",
      "    <div class=\"main-wrapper\" role=\"main\">\n",
      "    <div class=\"main-content\">\n",
      "        <h1 class=\"zone-name-title h1\">\n",
      "            <img class=\"heading-favicon\" src=\"/favicon.ico\"\n",
      "                 onerror=\"this.onerror=null;this.parentNode.removeChild(this)\" />\n",
      "            www.datacamp.com\n",
      "        </h1>\n",
      "        <h2 class=\"h2\" id=\"cf-challenge-running\">\n",
      "            Checking if the site connection is secure\n",
      "        </h2>\n",
      "        <noscript>\n",
      "            <div id=\"cf-challenge-error-title\">\n",
      "                <div class=\"h2\">\n",
      "                  <span class=\"icon-wrapper\">\n",
      "                      <div class=\"heading-icon warning-icon\"></div>\n",
      "                  </span>\n",
      "                    <span id=\"cf-challenge-error-text\">\n",
      "                    Enable JavaScript and cookies to continue\n",
      "                </span>\n",
      "                </div>\n",
      "            </div>\n",
      "        </noscript>\n",
      "        <div id='trk_jschal_js' style=\"display:none;background-image:url('/cdn-cgi/images/trace/jschal/nojs/transparent.gif?ray=738fea87b9e7a901')\"></div>\n",
      "        <div id=\"cf-challenge-body-text\" class=\"core-msg spacer\">\n",
      "            www.datacamp.com needs to review the security of your connection before\n",
      "            proceeding.\n",
      "        </div>\n",
      "        <form id=\"challenge-form\" action=\"/teach/documentation?__cf_chl_f_tk=0ks73Vl_5RrQjl8GCS7oBS.rY1wxqOjBbjPFbpgRZu8-1660209844-0-gaNycGzNCBE\" method=\"POST\" enctype=\"application/x-www-form-urlencoded\">\n",
      "            <input type=\"hidden\" name=\"md\" value=\"kAPZX7Exg2YWBDeqYPg4UuJXm4uyUvWPmIUknNqWLJ4-1660209844-0-AaT-NnC-atRn1Vl0aMT4NRrs-UyO-NjF8noIxg7qWQHUCNn4BsfN1ElLzS5ReWv5RQqC-62Pgsw0lNAa2IwM-kIiFYdfH8ecRJ3VupH3hPJ-glfouK9d-Mu-pjzOhh99I6wDHvk7pnetpDYE9vxlau-cBJPjLYEizMsp7B4jfRooS5EeeBpGRIrqgNPtC82UBhAaPG2rYWSb0kMDfXpMPcV48vLYftTn11X8YmIEtNkfrRLYp-g8Qx36z-OzXFLcNcAXLCDdid_zBShrBgxaYabsQWtAu2mY0nq9_JbsOzN427Jj0bG5nFb2eaIgXg6VtBr_xO4uqHueY8fWm8_HIY-wc-AStIG9kvvPBINpWB8xO9fGjv2F_NRclDgN8N6IOODhxfk6KqFm-fvkcZzZnfRHbciNHG5nduhyDkvAv8u0HEFsGkMVvVHfvz-tcUQ0KZ3jqcqYQmpbvX6pHPbNtyR8B4dakvTSTYeVJfOxUpVtdkxHI4YhribkrBrY9y8adOoZiDyBjYoHBrg7D-ffaQg8FPo5UTpfyx3qYSgLYvYmPyPoKCceYtVRWZD01X141YNY15t9YhLzLJb2JY6FvgacykJlTH7ZH8e_73_Jw2fgoRG2cQ0rbmP3_Z7ghk7RImclX39OgRePv4AE_HRAzRk\" />\n",
      "            <input type=\"hidden\" name=\"r\" value=\"lbdsf9kruV7Lh.MzVKFgtK0KqUbeigyau0i2Ah6J66Q-1660209844-0-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\"/>\n",
      "            \n",
      "                <input type=\"hidden\" name=\"pass\" value=\"1660209845.445-7Yh3mFc4Y4\"/>\n",
      "            \n",
      "        </form>\n",
      "    </div>\n",
      "</div>\n",
      "<script>\n",
      "    (function(){\n",
      "        var trkjs = document.createElement('img');\n",
      "        trkjs.setAttribute('src', '/cdn-cgi/images/trace/jschal/js/transparent.gif?ray=738fea87b9e7a901');\n",
      "        trkjs.setAttribute('style', 'display: none');\n",
      "        document.body.appendChild(trkjs);\n",
      "        var cpo = document.createElement('script');\n",
      "        cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/jsch/v1?ray=738fea87b9e7a901';\n",
      "        window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;\n",
      "        window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, -window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;\n",
      "        if (window.history && window.history.replaceState) {\n",
      "            var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;\n",
      "            history.replaceState(null, null, \"\\/teach\\/documentation?__cf_chl_rt_tk=0ks73Vl_5RrQjl8GCS7oBS.rY1wxqOjBbjPFbpgRZu8-1660209844-0-gaNycGzNCBE\" + window._cf_chl_opt.cOgUHash);\n",
      "            cpo.onload = function() {\n",
      "                history.replaceState(null, null, ogU);\n",
      "            };\n",
      "        }\n",
      "        document.getElementsByTagName('head')[0].appendChild(cpo);\n",
      "    }());\n",
      "</script>\n",
      "\n",
      "    <div class=\"footer\" role=\"contentinfo\">\n",
      "        <div class=\"footer-inner\">\n",
      "            <div class=\"clearfix diagnostic-wrapper\">\n",
      "                <div class=\"ray-id\">Ray ID: <code>738fea87b9e7a901</code></div>\n",
      "            </div>\n",
      "            <div class=\"text-center\">\n",
      "                Performance &amp; security by\n",
      "                <a rel=\"noopener noreferrer\" href=\"https://www.cloudflare.com\" target=\"_blank\">Cloudflare</a>\n",
      "            </div>\n",
      "        </div>\n",
      "    </div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the html\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Scraping the web in Python"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 1. Scraping the web in Python\n",
    "\n",
    "Wow! you have just scraped HTML data from the web and you've done so using two different packages, urllib and requests. You also saw that requests provided a higher-level interface in that you needed to write less lines of to retrieve the relevant HTML as a string."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 2. HTML\n",
    "\n",
    "You've got the HTML of your page of interest but, generally HTML is a humble-jumble mix of both unstructured and structured data. A word on these terms: Structured data is data that has a pre-defined data model or that is organized in a defined manner. Unstructured data is data that does not possess either of these properties. HTML is interesting because, although much of it is unstructured text, it does contain tags that determine where, for examples, headings can be found, and hyperlinks."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 3. BeautifulSoup\n",
    "\n",
    "In general, to turn HTML that you have scraped from the world wide web into useful data, you'll need to parse it and extract structured data from it. In this video and the next few interactive exercises, we'll provide a brief introduction to how you can perform such tasks using the Python package BeautifulSoup. Lets check out the package's website. The first words at the top are: \"You didn't write that awful page. You're just trying to get some data out of it. Beautiful Soup is here to help. Since 2004, it's been saving programmers hours or days of work on quick-turnaround screen scraping projects.\" Firstly, a word on the name of the package: BeautifulSoup? In web development, the term \"tag soup\" refers to structurally or syntactically incorrect HTML code written for a web page. What Beautiful Soup does best is to make tag soup beautiful again and to extract information from it with ease! In fact, the main object created and queried when using this package is called BeautifulSoup and it has a very important associated method called prettify! Lets now see BeautifulSoup in Beautiful Action!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "url = 'https://www.crummy.com/software/BeautifulSoup/'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)"
   ],
   "execution_count": 44,
   "cell_type": "code"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 4. BeautifulSoup\n",
    "\n",
    "Once again, you use requests to scrape the HTML from the web. Then you create a BeautifulSoup object from the resulting HTML and prettify."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n",
      "<html>\n",
      " <head>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "  <title>\n",
      "   Beautiful Soup: We called him Tortoise because he taught us.\n",
      "  </title>\n",
      "  <link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>\n",
      "  <link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>\n",
      "  <meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>\n",
      "  <meta content=\"Leonard Richardson\" name=\"author\"/>\n",
      " </head>\n",
      " <body alink=\"red\" bgcolor=\"white\" link=\"blue\" text=\"black\" vlink=\"660066\">\n",
      "  <style>\n",
      "   #tidelift { }\n",
      "\n",
      "#tidelift a {\n",
      " border: 1px solid #666666;\n",
      " margin-left: auto;\n",
      " padding: 10px;\n",
      " text-decoration: none;\n",
      "}\n",
      "\n",
      "#tidelift .cta {\n",
      " background: url(\"tidelift.svg\") no-repeat;\n",
      " padding-left: 30px;\n",
      "}\n",
      "  </style>\n",
      "  <img align=\"right\" src=\"10.1.jpg\" width=\"250\"/>\n",
      "  <br/>\n",
      "  <p>\n",
      "   [\n",
      "   <a href=\"#Download\">\n",
      "    Download\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"bs4/doc/\">\n",
      "    Documentation\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"#HallOfFame\">\n",
      "    Hall of Fame\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"enterprise.html\">\n",
      "    For enterprise\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"https://code.launchpad.net/beautifulsoup\">\n",
      "    Source\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">\n",
      "    Changelog\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n",
      "    Discussion group\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"zine/\">\n",
      "    Zine\n",
      "   </a>\n",
      "   ]\n",
      "  </p>\n",
      "  <div align=\"center\">\n",
      "   <a href=\"bs4/download/\">\n",
      "    <h1>\n",
      "     Beautiful Soup\n",
      "    </h1>\n",
      "   </a>\n",
      "  </div>\n",
      "  <p>\n",
      "   You didn't write that awful page. You're just trying to get some\n",
      "data out of it. Beautiful Soup is here to help. Since 2004, it's been\n",
      "saving programmers hours or days of work on quick-turnaround\n",
      "screen scraping projects.\n",
      "  </p>\n",
      "  <p>\n",
      "   Beautiful Soup is a Python library designed for quick turnaround\n",
      "projects like screen-scraping. Three features make it powerful:\n",
      "  </p>\n",
      "  <ol>\n",
      "   <li>\n",
      "    Beautiful Soup provides a few simple methods and Pythonic idioms\n",
      "for navigating, searching, and modifying a parse tree: a toolkit for\n",
      "dissecting a document and extracting what you need. It doesn't take\n",
      "much code to write an application\n",
      "   </li>\n",
      "   <li>\n",
      "    Beautiful Soup automatically converts incoming documents to\n",
      "Unicode and outgoing documents to UTF-8. You don't have to think\n",
      "about encodings, unless the document doesn't specify an encoding and\n",
      "Beautiful Soup can't detect one. Then you just have to specify the\n",
      "original encoding.\n",
      "   </li>\n",
      "   <li>\n",
      "    Beautiful Soup sits on top of popular Python parsers like\n",
      "    <a href=\"http://lxml.de/\">\n",
      "     lxml\n",
      "    </a>\n",
      "    and\n",
      "    <a href=\"http://code.google.com/p/html5lib/\">\n",
      "     html5lib\n",
      "    </a>\n",
      "    , allowing you\n",
      "to try out different parsing strategies or trade speed for\n",
      "flexibility.\n",
      "   </li>\n",
      "  </ol>\n",
      "  <p>\n",
      "   Beautiful Soup parses anything you give it, and does the tree\n",
      "traversal stuff for you. You can tell it \"Find all the links\", or\n",
      "\"Find all the links of class\n",
      "   <tt>\n",
      "    externalLink\n",
      "   </tt>\n",
      "   \", or \"Find all the\n",
      "links whose urls match \"foo.com\", or \"Find the table heading that's\n",
      "got bold text, then give me that text.\"\n",
      "  </p>\n",
      "  <p>\n",
      "   Valuable data that was once locked up in poorly-designed websites\n",
      "is now within your reach. Projects that would have taken hours take\n",
      "only minutes with Beautiful Soup.\n",
      "  </p>\n",
      "  <p>\n",
      "   Interested?\n",
      "   <a href=\"bs4/doc/\">\n",
      "    Read more.\n",
      "   </a>\n",
      "  </p>\n",
      "  <h3>\n",
      "   Getting and giving support\n",
      "  </h3>\n",
      "  <div align=\"center\" id=\"tidelift\">\n",
      "   <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=enterprise\" target=\"_blank\">\n",
      "    <span class=\"cta\">\n",
      "     Beautiful Soup for enterprise available via Tidelift\n",
      "    </span>\n",
      "   </a>\n",
      "  </div>\n",
      "  <p>\n",
      "   If you have questions, send them to\n",
      "   <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n",
      "    the discussion\n",
      "group\n",
      "   </a>\n",
      "   . If you find a bug,\n",
      "   <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n",
      "    file it on Launchpad\n",
      "   </a>\n",
      "   . If it's a security vulnerability, report it confidentially through\n",
      "   <a href=\"https://tidelift.com/security\">\n",
      "    Tidelift\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      "  <p>\n",
      "   If you use Beautiful Soup as part of your work, please consider a\n",
      "   <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">\n",
      "    Tidelift subscription\n",
      "   </a>\n",
      "   . This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n",
      "  </p>\n",
      "  <p>\n",
      "   If Beautiful Soup is useful to you on a personal level, you might like to read\n",
      "   <a href=\"zine/\">\n",
      "    <i>\n",
      "     Tool Safety\n",
      "    </i>\n",
      "   </a>\n",
      "   , a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\n",
      "  </p>\n",
      "  <a name=\"Download\">\n",
      "   <h2>\n",
      "    Download Beautiful Soup\n",
      "   </h2>\n",
      "  </a>\n",
      "  <p>\n",
      "   The current release is\n",
      "   <a href=\"bs4/download/\">\n",
      "    Beautiful Soup\n",
      "4.11.1\n",
      "   </a>\n",
      "   (April 8, 2022). You can install Beautiful Soup 4 with\n",
      "   <code>\n",
      "    pip install beautifulsoup4\n",
      "   </code>\n",
      "   .\n",
      "  </p>\n",
      "  <p>\n",
      "   In Debian and Ubuntu, Beautiful Soup is available as the\n",
      "   <code>\n",
      "    python-bs4\n",
      "   </code>\n",
      "   package (for Python 2) or the\n",
      "   <code>\n",
      "    python3-bs4\n",
      "   </code>\n",
      "   package (for Python 3). In Fedora it's\n",
      "available as the\n",
      "   <code>\n",
      "    python-beautifulsoup4\n",
      "   </code>\n",
      "   package.\n",
      "  </p>\n",
      "  <p>\n",
      "   Beautiful Soup is licensed under the MIT license, so you can also\n",
      "download the tarball, drop the\n",
      "   <code>\n",
      "    bs4/\n",
      "   </code>\n",
      "   directory into almost\n",
      "any Python application (or into your library path) and start using it\n",
      "immediately. (If you want to do this under Python 3, you will need to\n",
      "manually convert the code using\n",
      "   <code>\n",
      "    2to3\n",
      "   </code>\n",
      "   .)\n",
      "  </p>\n",
      "  <p>\n",
      "   Beautiful Soup 4 works on Python 3.6 and up. Support for Python 2 was discontinued on January 1,\n",
      "2021—one year after the Python 2 sunsetting date.\n",
      "  </p>\n",
      "  <h3>\n",
      "   Beautiful Soup 3\n",
      "  </h3>\n",
      "  <p>\n",
      "   Beautiful Soup 3 was the official release line of Beautiful Soup\n",
      "from May 2006 to March 2012. It does not support Python 3 and was\n",
      "discontinued or January 1, 2021—one year after the Python 2\n",
      "sunsetting date. If you have any active projects using Beautiful Soup\n",
      "3, you should migrate to Beautiful Soup 4 as part of your Python 3\n",
      "conversion.\n",
      "  </p>\n",
      "  <p>\n",
      "   <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">\n",
      "    Here's\n",
      "the Beautiful Soup 3 documentation.\n",
      "   </a>\n",
      "  </p>\n",
      "  <p>\n",
      "   The current and hopefully final release of Beautiful Soup 3 is\n",
      "   <a href=\"download/3.x/BeautifulSoup-3.2.2.tar.gz\">\n",
      "    3.2.2\n",
      "   </a>\n",
      "   (October 5,\n",
      "2019). It's the\n",
      "   <code>\n",
      "    BeautifulSoup\n",
      "   </code>\n",
      "   package on pip. It's also\n",
      "available as\n",
      "   <code>\n",
      "    python-beautifulsoup\n",
      "   </code>\n",
      "   in Debian and Ubuntu,\n",
      "and as\n",
      "   <code>\n",
      "    python-BeautifulSoup\n",
      "   </code>\n",
      "   in Fedora.\n",
      "  </p>\n",
      "  <p>\n",
      "   Once Beautiful Soup 3 is discontinued, these package names will be available for use by a more recent version of Beautiful Soup.\n",
      "  </p>\n",
      "  <p>\n",
      "   Beautiful Soup 3, like Beautiful Soup 4, is\n",
      "   <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&amp;utm_medium=referral&amp;utm_campaign=website\">\n",
      "    supported through Tidelift\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      "  <a name=\"HallOfFame\">\n",
      "   <h2>\n",
      "    Hall of Fame\n",
      "   </h2>\n",
      "  </a>\n",
      "  <p>\n",
      "   Over the years, Beautiful Soup has been used in hundreds of\n",
      "different projects. There's no way I can list them all, but I want to\n",
      "highlight a few high-profile projects. Beautiful Soup isn't what makes\n",
      "these projects interesting, but it did make their completion easier:\n",
      "  </p>\n",
      "  <ul>\n",
      "   <li>\n",
      "    <a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\n",
      "     \"Movable\n",
      " Type\"\n",
      "    </a>\n",
      "    , a work of digital art on display in the lobby of the New\n",
      " York Times building, uses Beautiful Soup to scrape news feeds.\n",
      "   </li>\n",
      "   <li>\n",
      "    Jiabao Lin's\n",
      "    <a href=\"https://github.com/BlankerL/DXY-COVID-19-Crawler\">\n",
      "     DXY-COVID-19-Crawler\n",
      "    </a>\n",
      "    uses Beautiful Soup to scrape a Chinese medical site for information\n",
      "about COVID-19, making it easier for researchers to track the spread\n",
      "of the virus. (Source:\n",
      "    <a href=\"https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\">\n",
      "     \"How open source software is fighting COVID-19\"\n",
      "    </a>\n",
      "    )\n",
      "   </li>\n",
      "   <li>\n",
      "    Reddit uses Beautiful Soup to\n",
      "    <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">\n",
      "     parse\n",
      "a page that's been linked to and find a representative image\n",
      "    </a>\n",
      "    .\n",
      "   </li>\n",
      "   <li>\n",
      "    Alexander Harrowell uses Beautiful Soup to\n",
      "    <a href=\"http://www.harrowell.org.uk/viktormap.html\">\n",
      "     track the business\n",
      " activities\n",
      "    </a>\n",
      "    of an arms merchant.\n",
      "   </li>\n",
      "   <li>\n",
      "    The developers of Python itself used Beautiful Soup to\n",
      "    <a href=\"http://svn.python.org/view/tracker/importer/\">\n",
      "     migrate the Python\n",
      "bug tracker from Sourceforge to Roundup\n",
      "    </a>\n",
      "    .\n",
      "   </li>\n",
      "   <li>\n",
      "    The\n",
      "    <a href=\"http://www2.ljworld.com/\">\n",
      "     Lawrence Journal-World\n",
      "    </a>\n",
      "    uses Beautiful Soup to\n",
      "    <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">\n",
      "     gather\n",
      "statewide election results\n",
      "    </a>\n",
      "    .\n",
      "   </li>\n",
      "   <li>\n",
      "    The\n",
      "    <a href=\"http://esrl.noaa.gov/gsd/fab/\">\n",
      "     NOAA's Forecast\n",
      "Applications Branch\n",
      "    </a>\n",
      "    uses Beautiful Soup in\n",
      "    <a href=\"http://laps.noaa.gov/topograbber/\">\n",
      "     TopoGrabber\n",
      "    </a>\n",
      "    , a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "   </li>\n",
      "  </ul>\n",
      "  <p>\n",
      "   If you've used Beautiful Soup in a project you'd like me to know\n",
      "about, please do send email to me or\n",
      "   <a href=\"http://groups.google.com/group/beautifulsoup/\">\n",
      "    the discussion\n",
      "group\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      "  <h2>\n",
      "   Development\n",
      "  </h2>\n",
      "  <p>\n",
      "   Development happens at\n",
      "   <a href=\"https://launchpad.net/beautifulsoup\">\n",
      "    Launchpad\n",
      "   </a>\n",
      "   . You can\n",
      "   <a href=\"https://code.launchpad.net/beautifulsoup/\">\n",
      "    get the source\n",
      "code\n",
      "   </a>\n",
      "   or\n",
      "   <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n",
      "    file\n",
      "bugs\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      "  <hr/>\n",
      "  <table>\n",
      "   <tr>\n",
      "    <td valign=\"top\">\n",
      "     <p>\n",
      "      This document (\n",
      "      <a href=\"/source/software/BeautifulSoup/index.bhtml\">\n",
      "       source\n",
      "      </a>\n",
      "      ) is part of Crummy, the webspace of\n",
      "      <a href=\"/self/\">\n",
      "       Leonard Richardson\n",
      "      </a>\n",
      "      (\n",
      "      <a href=\"/self/contact.html\">\n",
      "       contact information\n",
      "      </a>\n",
      "      ). It was last modified on Monday, June 27 2022, 15:36:35 Nowhere Standard Time and last built on Thursday, August 11 2022, 09:00:01 Nowhere Standard Time.\n",
      "     </p>\n",
      "     <p>\n",
      "     </p>\n",
      "     <table class=\"licenseText\">\n",
      "      <tr>\n",
      "       <td>\n",
      "        <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n",
      "         <img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/>\n",
      "        </a>\n",
      "       </td>\n",
      "       <td valign=\"top\">\n",
      "        Crummy is © 1996-2022 Leonard Richardson. Unless otherwise noted, all text licensed under a\n",
      "        <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n",
      "         Creative Commons License\n",
      "        </a>\n",
      "        .\n",
      "       </td>\n",
      "      </tr>\n",
      "     </table>\n",
      "     <!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>-->\n",
      "    </td>\n",
      "    <td valign=\"top\">\n",
      "     <p>\n",
      "      <b>\n",
      "       Document tree:\n",
      "      </b>\n",
      "     </p>\n",
      "     <dl>\n",
      "      <dd>\n",
      "       <a href=\"http://www.crummy.com/\">\n",
      "        http://www.crummy.com/\n",
      "       </a>\n",
      "       <dl>\n",
      "        <dd>\n",
      "         <a href=\"http://www.crummy.com/software/\">\n",
      "          software/\n",
      "         </a>\n",
      "         <dl>\n",
      "          <dd>\n",
      "           <a href=\"http://www.crummy.com/software/BeautifulSoup/\">\n",
      "            BeautifulSoup/\n",
      "           </a>\n",
      "          </dd>\n",
      "         </dl>\n",
      "        </dd>\n",
      "       </dl>\n",
      "      </dd>\n",
      "     </dl>\n",
      "     Site Search:\n",
      "     <form action=\"/search/\" method=\"get\">\n",
      "      <input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\n",
      "     </form>\n",
      "    </td>\n",
      "   </tr>\n",
      "  </table>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ],
   "execution_count": 45,
   "cell_type": "code"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 5. Prettified Soup\n",
    "\n",
    "Printing the prettified Soup and the original HTML, you can see that for, example, the prettified Soup is indented in the way you would expect properly written HTML to be."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)"
   ],
   "execution_count": 46,
   "cell_type": "code"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Beautiful Soup: We called him Tortoise because he taught us.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[ Download | Documentation | Hall of Fame | For enterprise | Source | Changelog | Discussion group  | Zine ]\n",
      "\n",
      "Beautiful Soup\n",
      "\n",
      "You didn't write that awful page. You're just trying to get some\n",
      "data out of it. Beautiful Soup is here to help. Since 2004, it's been\n",
      "saving programmers hours or days of work on quick-turnaround\n",
      "screen scraping projects.\n",
      "Beautiful Soup is a Python library designed for quick turnaround\n",
      "projects like screen-scraping. Three features make it powerful:\n",
      "\n",
      "\n",
      "Beautiful Soup provides a few simple methods and Pythonic idioms\n",
      "for navigating, searching, and modifying a parse tree: a toolkit for\n",
      "dissecting a document and extracting what you need. It doesn't take\n",
      "much code to write an application\n",
      "\n",
      "Beautiful Soup automatically converts incoming documents to\n",
      "Unicode and outgoing documents to UTF-8. You don't have to think\n",
      "about encodings, unless the document doesn't specify an encoding and\n",
      "Beautiful Soup can't detect one. Then you just have to specify the\n",
      "original encoding.\n",
      "\n",
      "Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you\n",
      "to try out different parsing strategies or trade speed for\n",
      "flexibility.\n",
      "\n",
      "\n",
      "Beautiful Soup parses anything you give it, and does the tree\n",
      "traversal stuff for you. You can tell it \"Find all the links\", or\n",
      "\"Find all the links of class externalLink\", or \"Find all the\n",
      "links whose urls match \"foo.com\", or \"Find the table heading that's\n",
      "got bold text, then give me that text.\"\n",
      "\n",
      "Valuable data that was once locked up in poorly-designed websites\n",
      "is now within your reach. Projects that would have taken hours take\n",
      "only minutes with Beautiful Soup.\n",
      "\n",
      "Interested? Read more.\n",
      "Getting and giving support\n",
      "\n",
      "\n",
      "\n",
      "  Beautiful Soup for enterprise available via Tidelift\n",
      " \n",
      "\n",
      "\n",
      "If you have questions, send them to the discussion\n",
      "group. If you find a bug, file it on Launchpad. If it's a security vulnerability, report it confidentially through Tidelift.\n",
      "If you use Beautiful Soup as part of your work, please consider a Tidelift subscription. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n",
      "\n",
      "\n",
      "If Beautiful Soup is useful to you on a personal level, you might like to read Tool Safety, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\n",
      "Download Beautiful Soup\n",
      "The current release is Beautiful Soup\n",
      "4.11.1 (April 8, 2022). You can install Beautiful Soup 4 with\n",
      "pip install beautifulsoup4.\n",
      "\n",
      "In Debian and Ubuntu, Beautiful Soup is available as the\n",
      "python-bs4 package (for Python 2) or the\n",
      "python3-bs4 package (for Python 3). In Fedora it's\n",
      "available as the python-beautifulsoup4 package.\n",
      "\n",
      "Beautiful Soup is licensed under the MIT license, so you can also\n",
      "download the tarball, drop the bs4/ directory into almost\n",
      "any Python application (or into your library path) and start using it\n",
      "immediately. (If you want to do this under Python 3, you will need to\n",
      "manually convert the code using 2to3.)\n",
      "\n",
      "Beautiful Soup 4 works on Python 3.6 and up. Support for Python 2 was discontinued on January 1,\n",
      "2021—one year after the Python 2 sunsetting date.\n",
      "\n",
      "Beautiful Soup 3\n",
      "Beautiful Soup 3 was the official release line of Beautiful Soup\n",
      "from May 2006 to March 2012. It does not support Python 3 and was\n",
      "discontinued or January 1, 2021—one year after the Python 2\n",
      "sunsetting date. If you have any active projects using Beautiful Soup\n",
      "3, you should migrate to Beautiful Soup 4 as part of your Python 3\n",
      "conversion.\n",
      "\n",
      "Here's\n",
      "the Beautiful Soup 3 documentation.\n",
      "The current and hopefully final release of Beautiful Soup 3 is 3.2.2 (October 5,\n",
      "2019). It's the BeautifulSoup package on pip. It's also\n",
      "available as python-beautifulsoup in Debian and Ubuntu,\n",
      "and as python-BeautifulSoup in Fedora.\n",
      "\n",
      "Once Beautiful Soup 3 is discontinued, these package names will be available for use by a more recent version of Beautiful Soup.\n",
      "\n",
      "Beautiful Soup 3, like Beautiful Soup 4, is supported through Tidelift.\n",
      "Hall of Fame\n",
      "Over the years, Beautiful Soup has been used in hundreds of\n",
      "different projects. There's no way I can list them all, but I want to\n",
      "highlight a few high-profile projects. Beautiful Soup isn't what makes\n",
      "these projects interesting, but it did make their completion easier:\n",
      "\n",
      "\n",
      "\"Movable\n",
      " Type\", a work of digital art on display in the lobby of the New\n",
      " York Times building, uses Beautiful Soup to scrape news feeds.\n",
      "\n",
      "Jiabao Lin's DXY-COVID-19-Crawler\n",
      "uses Beautiful Soup to scrape a Chinese medical site for information\n",
      "about COVID-19, making it easier for researchers to track the spread\n",
      "of the virus. (Source: \"How open source software is fighting COVID-19\")\n",
      "\n",
      "Reddit uses Beautiful Soup to parse\n",
      "a page that's been linked to and find a representative image.\n",
      "\n",
      "Alexander Harrowell uses Beautiful Soup to track the business\n",
      " activities of an arms merchant.\n",
      "\n",
      "The developers of Python itself used Beautiful Soup to migrate the Python\n",
      "bug tracker from Sourceforge to Roundup.\n",
      "\n",
      "The Lawrence Journal-World\n",
      "uses Beautiful Soup to gather\n",
      "statewide election results.\n",
      "\n",
      "The NOAA's Forecast\n",
      "Applications Branch uses Beautiful Soup in TopoGrabber, a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "\n",
      "\n",
      "If you've used Beautiful Soup in a project you'd like me to know\n",
      "about, please do send email to me or the discussion\n",
      "group.\n",
      "\n",
      "Development\n",
      "Development happens at Launchpad. You can get the source\n",
      "code or file\n",
      "bugs.\n",
      "This document (source) is part of Crummy, the webspace of Leonard Richardson (contact information). It was last modified on Monday, June 27 2022, 15:36:35 Nowhere Standard Time and last built on Thursday, August 11 2022, 09:00:01 Nowhere Standard Time.Crummy is © 1996-2022 Leonard Richardson. Unless otherwise noted, all text licensed under a Creative Commons License.Document tree:\n",
      "http://www.crummy.com/software/BeautifulSoup/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Site Search:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.get_text())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 6. Exploring BeautifulSoup\n",
    "\n",
    "You'll explore a few of the methods that you can apply to your soupified HTML in the following exercises, such as title and get_text, which extract the title and text, respectively."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 7. Exploring BeautifulSoup\n",
    "\n",
    "You'll also work with the Soupy method find_all in order to extract the URLs of all of the hyperlinks in the HTML. These are merely a few of many methods existing in BeautifulSoup to extract data from HTML. If, after completing these exercises, you find yourself thirsting for more BeautifulSoup, there are plenty of great resources on their website."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Download\n",
      "bs4/doc/\n",
      "#HallOfFame\n",
      "enterprise.html\n",
      "https://code.launchpad.net/beautifulsoup\n",
      "https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\n",
      "https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\n",
      "zine/\n",
      "bs4/download/\n",
      "http://lxml.de/\n",
      "http://code.google.com/p/html5lib/\n",
      "bs4/doc/\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=enterprise\n",
      "https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\n",
      "https://bugs.launchpad.net/beautifulsoup/\n",
      "https://tidelift.com/security\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website\n",
      "zine/\n",
      "None\n",
      "bs4/download/\n",
      "http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\n",
      "download/3.x/BeautifulSoup-3.2.2.tar.gz\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&utm_medium=referral&utm_campaign=website\n",
      "None\n",
      "http://www.nytimes.com/2007/10/25/arts/design/25vide.html\n",
      "https://github.com/BlankerL/DXY-COVID-19-Crawler\n",
      "https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\n",
      "https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\n",
      "http://www.harrowell.org.uk/viktormap.html\n",
      "http://svn.python.org/view/tracker/importer/\n",
      "http://www2.ljworld.com/\n",
      "http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\n",
      "http://esrl.noaa.gov/gsd/fab/\n",
      "http://laps.noaa.gov/topograbber/\n",
      "http://groups.google.com/group/beautifulsoup/\n",
      "https://launchpad.net/beautifulsoup\n",
      "https://code.launchpad.net/beautifulsoup/\n",
      "https://bugs.launchpad.net/beautifulsoup/\n",
      "/source/software/BeautifulSoup/index.bhtml\n",
      "/self/\n",
      "/self/contact.html\n",
      "http://creativecommons.org/licenses/by-sa/2.0/\n",
      "http://creativecommons.org/licenses/by-sa/2.0/\n",
      "http://www.crummy.com/\n",
      "http://www.crummy.com/software/\n",
      "http://www.crummy.com/software/BeautifulSoup/\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all(\"a\"):\n",
    "    print(link.get(\"href\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 8. Let's practice!\n",
    "\n",
    "Okay, now it's your turn to jump into the deep end of the proverbial soup bowl! Happy hacking!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Parsing HTML with BeautifulSoup\n",
    "\n",
    "<p>In this interactive exercise, you'll learn how to use the BeautifulSoup package to <em>parse</em>, <em>prettify</em> and <em>extract</em> information from HTML. You'll scrape the data from the webpage of Guido van Rossum, Python's very own <a href=\"https://en.wikipedia.org/wiki/Benevolent_dictator_for_life\" target=\"_blank\" rel=\"noopener noreferrer\">Benevolent Dictator for Life</a>. In the following exercises, you'll prettify the HTML and then extract the text and the hyperlinks.</p>\n",
    "<p>The URL of interest is <code>url = 'https://www.python.org/~guido/'</code>.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Import the function <code>BeautifulSoup</code> from the package <code>bs4</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assign the URL of interest to the variable <code>url</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Package the request to the URL, send the request and catch the response with a single function <code>requests.get()</code>, assigning the response to the variable <code>r</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "\n",
    "r = requests.get(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the <code>text</code> attribute of the object <code>r</code> to return the HTML of the webpage as a string; store the result in a variable <code>html_doc</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a BeautifulSoup object <code>soup</code> from the resulting HTML using the function <code>BeautifulSoup()</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup =BeautifulSoup(html_doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the method <code>prettify()</code> on <code>soup</code> and assign the result to <code>pretty_soup</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hit submit to print to prettified HTML to your shell!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Guido's Personal Home Page\n",
      "  </title>\n",
      " </head>\n",
      " <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n",
      "  <!-- Built from main -->\n",
      "  <h1>\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" src=\"images/IMG_2192.jpg\"/>\n",
      "   </a>\n",
      "   Guido van Rossum - Personal Home Page\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" height=\"216\" src=\"images/guido-headshot-2019.jpg\" width=\"270\"/>\n",
      "   </a>\n",
      "  </h1>\n",
      "  <p>\n",
      "   <a href=\"http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\">\n",
      "    <i>\n",
      "     \"Gawky and proud of it.\"\n",
      "    </i>\n",
      "   </a>\n",
      "  </p>\n",
      "  <h3>\n",
      "   <a href=\"images/df20000406.jpg\">\n",
      "    Who I Am\n",
      "   </a>\n",
      "  </h3>\n",
      "  <p>\n",
      "   Read\n",
      "my\n",
      "   <a href=\"http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\">\n",
      "    \"King's\n",
      "Day Speech\"\n",
      "   </a>\n",
      "   for some inspiration.\n",
      "  </p>\n",
      "  <p>\n",
      "   I am the author of the\n",
      "   <a href=\"http://www.python.org\">\n",
      "    Python\n",
      "   </a>\n",
      "   programming language.  See also my\n",
      "   <a href=\"Resume.html\">\n",
      "    resume\n",
      "   </a>\n",
      "   and my\n",
      "   <a href=\"Publications.html\">\n",
      "    publications list\n",
      "   </a>\n",
      "   , a\n",
      "   <a href=\"bio.html\">\n",
      "    brief bio\n",
      "   </a>\n",
      "   , assorted\n",
      "   <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "    writings\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"http://legacy.python.org/doc/essays/ppt/\">\n",
      "    presentations\n",
      "   </a>\n",
      "   and\n",
      "   <a href=\"interviews.html\">\n",
      "    interviews\n",
      "   </a>\n",
      "   (all about Python), some\n",
      "   <a href=\"pics.html\">\n",
      "    pictures of me\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"http://neopythonic.blogspot.com\">\n",
      "    my new blog\n",
      "   </a>\n",
      "   , and\n",
      "my\n",
      "   <a href=\"http://www.artima.com/weblogs/index.jsp?blogger=12088\">\n",
      "    old\n",
      "blog\n",
      "   </a>\n",
      "   on Artima.com.  I am\n",
      "   <a href=\"https://twitter.com/gvanrossum\">\n",
      "    @gvanrossum\n",
      "   </a>\n",
      "   on Twitter.\n",
      "  </p>\n",
      "  <p>\n",
      "   I am currently a Distinguished Engineer at Microsoft.\n",
      "I have worked for Dropbox, Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my\n",
      "   <a href=\"Resume.html\">\n",
      "    resume\n",
      "   </a>\n",
      "   .)  I created Python while at CWI.\n",
      "  </p>\n",
      "  <h3>\n",
      "   How to Reach Me\n",
      "  </h3>\n",
      "  <p>\n",
      "   You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but I receive too much email to respond\n",
      "to everything.\n",
      "  </p>\n",
      "  <h3>\n",
      "   My Name\n",
      "  </h3>\n",
      "  <p>\n",
      "   My name often poses difficulties for Americans.\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Pronunciation:\n",
      "   </b>\n",
      "   in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "   <a href=\"guido.au\">\n",
      "    sound clip\n",
      "   </a>\n",
      "   .)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Spelling:\n",
      "   </b>\n",
      "   my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Alphabetization:\n",
      "   </b>\n",
      "   in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "  </p>\n",
      "  <h3>\n",
      "   More Hyperlinks\n",
      "  </h3>\n",
      "  <ul>\n",
      "   <li>\n",
      "    Here's a collection of\n",
      "    <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "     essays\n",
      "    </a>\n",
      "    relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "    <p>\n",
      "    </p>\n",
      "   </li>\n",
      "   <li>\n",
      "    I own the official\n",
      "    <a href=\"images/license.jpg\">\n",
      "     <img align=\"center\" border=\"0\" height=\"75\" src=\"images/license_thumb.jpg\" width=\"100\"/>\n",
      "     Python license.\n",
      "    </a>\n",
      "    <p>\n",
      "    </p>\n",
      "   </li>\n",
      "  </ul>\n",
      "  <h3>\n",
      "   The Audio File Formats FAQ\n",
      "  </h3>\n",
      "  <p>\n",
      "   I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It is now maintained by Chris Bagwell\n",
      "at\n",
      "   <a href=\"http://www.cnpbagwell.com/audio-faq\">\n",
      "    http://www.cnpbagwell.com/audio-faq\n",
      "   </a>\n",
      "   .  And here is a link to\n",
      "   <a href=\"http://sox.sourceforge.net/\">\n",
      "    SOX\n",
      "   </a>\n",
      "   , to which I contributed\n",
      "some early code.\n",
      "  </p>\n",
      "  <hr/>\n",
      "  <a href=\"images/internetdog.gif\">\n",
      "   \"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "  </a>\n",
      "  <hr/>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the response\n",
    "print(pretty_soup)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Turning a webpage into data using BeautifulSoup: getting the text\n",
    "\n",
    "<p>As promised, in the following exercises, you'll learn the basics of extracting information from HTML soup. In this exercise, you'll figure out how to extract the text from the BDFL's webpage, along with printing the webpage's title.</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the sample code, the HTML response object <code>html_doc</code> has already been created: your first task is to Soupify it using the function <code>BeautifulSoup()</code> and to assign the resulting soup to the variable <code>soup</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "\n",
    "soup = BeautifulSoup(html_doc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract the title from the HTML soup <code>soup</code> using the attribute <code>title</code> and assign the result to <code>guido_title</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the title of Guido's webpage to the shell using the <code>print()</code> function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(guido_title)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract the text from the HTML soup <code>soup</code> using the method <code>get_text()</code> and assign to <code>guido_text</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Guido's Personal Home Page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Guido van Rossum - Personal Home Page\n",
      "\n",
      "\n",
      "\"Gawky and proud of it.\"\n",
      "Who I Am\n",
      "Read\n",
      "my \"King's\n",
      "Day Speech\" for some inspiration.\n",
      "\n",
      "I am the author of the Python\n",
      "programming language.  See also my resume\n",
      "and my publications list, a brief bio, assorted writings, presentations and interviews (all about Python), some\n",
      "pictures of me,\n",
      "my new blog, and\n",
      "my old\n",
      "blog on Artima.com.  I am\n",
      "@gvanrossum on Twitter.\n",
      "\n",
      "I am currently a Distinguished Engineer at Microsoft.\n",
      "I have worked for Dropbox, Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my resume.)  I created Python while at CWI.\n",
      "\n",
      "How to Reach Me\n",
      "You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but I receive too much email to respond\n",
      "to everything.\n",
      "\n",
      "My Name\n",
      "My name often poses difficulties for Americans.\n",
      "\n",
      "Pronunciation: in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "sound clip.)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "\n",
      "Spelling: my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "\n",
      "Alphabetization: in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "\n",
      "\n",
      "More Hyperlinks\n",
      "\n",
      "Here's a collection of essays relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "I own the official \n",
      "Python license.\n",
      "\n",
      "The Audio File Formats FAQ\n",
      "I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It is now maintained by Chris Bagwell\n",
      "at http://www.cnpbagwell.com/audio-faq.  And here is a link to\n",
      "SOX, to which I contributed\n",
      "some early code.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hit submit to print the text from Guido's webpage to the shell."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Turning a webpage into data using BeautifulSoup: getting the hyperlinks\n",
    "\n",
    "<p>In this exercise, you'll figure out how to extract the URLs of the hyperlinks from the BDFL's webpage. In the process, you'll become close friends with the soup method <code>find_all()</code>.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Use the method <code>find_all()</code> to find all hyperlinks in <code>soup</code>, remembering that hyperlinks are defined by the HTML tag <code>&lt;a&gt;</code> but passed to <code>find_all()</code> without angle brackets; store the result in the variable <code>a_tags</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all(\"a\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The variable <code>a_tags</code> is a results set: your job now is to enumerate over it, using a <code>for</code> loop and to print the actual URLs of the hyperlinks; to do this, for every element <code>link</code> in <code>a_tags</code>, you want to <code>print()</code> <code>link.get('href')</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pics.html\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "images/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "Resume.html\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get(\"href\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "# Interacting with APIs to import data from the web"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Introduction to APIs and JSONs"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 1. Introduction to APIs and JSONs\n",
    "\n",
    "In this chapter, you'll explore pulling data from the web even further by"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 2. APIs\n",
    "\n",
    "learning how to interact with APIs, or Application Programming Interfaces. An API is a set of protocols and routines for building and interacting with software applications. In particular, you'll learn how to use the Open Movie Database API and, in the next chapter,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 3. APIs\n",
    "\n",
    "the Twitter API to pull data from both applications, while learning about API interaction best practices. A standard form for transferring data through APIs is"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 4. JSONs\n",
    "\n",
    "the JSON file format, so in this video, we'll focus our attention squarely on these. Then we'll move onto actually getting data from APIs. JSON is an acronym that is short for JavaScript Object Notation. It is a file format that arose out of a growing need for real-time server-to-browser communication that wouldn't necessarily rely on Flash or Java and was first specified and also popularized by Douglas Crockford, an American programmer and entrepreneur. One of the cool things about JSONs is that they're human readable, that is, they can naturally be read by humans unlike, for example, pickled files, as we saw in the previous course. As they're human readable, let's check one out!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 5. JSONs\n",
    "\n",
    "Here you see a JSON from the OMDB OR Open Movie Database API. In particular, this is JSON containing information about the movie Snakes on a Plane. First notice that the JSON consists of name-value pairs separated by commas. This will remind you of the key-value pairs in a Python dictionary! We'll see in a minute that, for this reason, when loading JSONs into Python, it is natural to store them in a dict. The keys in JSONs will always be strings enclosed in quotation marks. The values can be strings, integers, arrays or even objects. Such an object can even be a JSON and then you have nested JSONs but we won't go further into these here. In this case of the Snakes on a Plane JSON, all the values are strings and we can see this from the quotation marks."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 6. JSONs\n",
    "\n",
    "The value corresponding to the key 'Title' is the title of the movie as a string: Snakes on a Plane."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 7. JSONs\n",
    "\n",
    "The value corresponding to the key 'Year' is the year of release as a string: 2006 and so on. There's the rating, the runtime, director, writers, plot, language and much more! You'll soon learn how to use the OMDB API and Python to automate retrieval of such data, but first you'll figure out how to load JSONs from a local directory."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 8. Loading JSONs in Python\n",
    "\n",
    "Lets say that I had the JSON stored in my working directory as 'snakes dot json'. To load the JSON into my Python environment, I would first import the package json and then open a connection to the file and use the function json dot load to load the JSON. If I then check the datatype of json_data by executing type(json_data), I see that Python cleverly imported the JSON as a dictionary!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "dict"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/a_movie.json\",\"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "type(json_data)"
   ],
   "execution_count": 62,
   "cell_type": "code"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 9. Exploring JSONs in Python\n",
    "\n",
    "To print the key-value pairs to the console, I can then iterate over the key-value pairs using a for loop. Now it's your turn to test your JSON skills"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:The Social Network\n",
      "Year:2010\n",
      "Rated:PG-13\n",
      "Released:01 Oct 2010\n",
      "Runtime:120 min\n",
      "Genre:Biography, Drama\n",
      "Director:David Fincher\n",
      "Writer:Aaron Sorkin, Ben Mezrich\n",
      "Actors:Jesse Eisenberg, Andrew Garfield, Justin Timberlake\n",
      "Plot:As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea, and by the co-founder who was later squeezed out of the business.\n",
      "Language:English, French\n",
      "Country:United States\n",
      "Awards:Won 3 Oscars. 172 wins & 186 nominations total\n",
      "Poster:https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for key,values in json_data.items():\n",
    "        print(key + \":\" +values)\n",
    "except TypeError:\n",
    "    pass"
   ],
   "execution_count": 64,
   "cell_type": "code"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 10. Let's practice!\n",
    "\n",
    "so get coding!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Loading and exploring a JSON\n",
    "\n",
    "<p>Now that you know what a JSON is, you'll load one into your Python\n",
    "environment and explore it yourself. Here, you'll load the JSON\n",
    "<code>'a_movie.json'</code> into the variable <code>json_data</code>, which will be a\n",
    "dictionary. You'll then explore the JSON contents by printing the\n",
    "key-value pairs of <code>json_data</code> to the shell.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Load the JSON <code>'a_movie.json'</code> into the variable <code>json_data</code> <em>within the context</em> provided by the <code>with</code> statement. To do so, use the function <code>json.load()</code> <em>within the context manager</em>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "with open(\"data/a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Use a <code>for</code> loop to print all key-value pairs in the dictionary <code>json_data</code>. Recall that you can access a value in a dictionary using the syntax: <em>dictionary</em><code>[</code>key<code>]</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Social Network\n",
      "Year: 2010\n",
      "Rated: PG-13\n",
      "Released: 01 Oct 2010\n",
      "Runtime: 120 min\n",
      "Genre: Biography, Drama\n",
      "Director: David Fincher\n",
      "Writer: Aaron Sorkin, Ben Mezrich\n",
      "Actors: Jesse Eisenberg, Andrew Garfield, Justin Timberlake\n",
      "Plot: As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea, and by the co-founder who was later squeezed out of the business.\n",
      "Language: English, French\n",
      "Country: United States\n",
      "Awards: Won 3 Oscars. 172 wins & 186 nominations total\n",
      "Poster: https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\n",
      "Ratings: [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\n",
      "Metascore: 95\n",
      "imdbRating: 7.8\n",
      "imdbVotes: 690,655\n",
      "imdbID: tt1285016\n",
      "Type: movie\n",
      "DVD: 11 Jan 2011\n",
      "BoxOffice: $96,962,694\n",
      "Production: N/A\n",
      "Website: N/A\n",
      "Response: True\n"
     ]
    }
   ],
   "source": [
    "for k in json_data.keys():\n",
    "    print(k + \":\" , json_data[k])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## APIs and interacting with the world wide web"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 1. APIs and interacting with the world wide web\n",
    "\n",
    "Congrats on making it through your crash course in JSONs! JSONs are everywhere and one of the main motivating reasons for getting to know how to work with them as a Data Scientist is that much of the data that you'll get from APIs are packaged as JSONs."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 2. Herein, you’ll learn\n",
    "\n",
    "In this video, you'll learn what APIs are, why they are so important, and see a number of illustrative examples. In the subsequent interactive exercises, you'll gain valuable practice connecting to a variety of APIs, pulling and parsing data from them."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 3. What is an API?\n",
    "\n",
    "So what is an API and why are they so important? Simply put, an API is a set of protocols and routines for building and interacting with software applications. Another way to think of it is that an API is a bunch of code that allows two software programs to communicate with each other. For example, if you wanted to stream twitter data by writing some Python code, you would use the Twitter API. If you wanted to automate pulling and processing information"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 4. What is an API?\n",
    "\n",
    "from Wikipedia in your programming language of choice, you could do so using the Wikipedia API."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 5. APIs are everywhere\n",
    "\n",
    "Using such APIs have now become standard ways of interacting with such applications. Twitter has an API that is used by marketing companies and social scientists engaged in research concerning social networks."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 6. APIs are everywhere\n",
    "\n",
    "Uber,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 7. APIs are everywhere\n",
    "\n",
    "Facebook and"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 8. APIs are everywhere\n",
    "\n",
    "Instagram all have APIs. Now let's figure out how to connect to an API and how to pull data from it."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 9. Connecting to an API in Python\n",
    "\n",
    "In this example, we'll pull movie data from the Open Movie Database, or OMDB, API. Once again, you'll use the ever-elegant requests library. You import requests and assign the URL of interest to the variable url. You then package and send the request to the URL, which describes your API query, and catch the response in one line of code. Thanks again, requests package! Another really cool aspect of the requests package is that the Response objects, such as r, have an associate method json, which is a built-in JSON decoder for when we're dealing with JSON data. This returns a dictionary and we can then print all the key-value pairs to check out what we pulled from the OMBD API!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Hackers\n",
      "Year:  1995\n",
      "Rated:  PG-13\n",
      "Released:  15 Sep 1995\n",
      "Runtime:  105 min\n",
      "Genre:  Crime, Drama, Romance\n",
      "Director:  Iain Softley\n",
      "Writer:  Rafael Moreu\n",
      "Actors:  Jonny Lee Miller, Angelina Jolie, Jesse Bradford\n",
      "Plot:  Hackers are blamed for making a virus that will capsize five oil tankers.\n",
      "Language:  English, Italian, Spanish, Japanese, Russian\n",
      "Country:  United States\n",
      "Awards:  N/A\n",
      "Poster:  https://m.media-amazon.com/images/M/MV5BNmExMTkyYjItZTg0YS00NWYzLTkwMjItZWJiOWQ2M2ZkYjE4XkEyXkFqcGdeQXVyMTQxNzMzNDI@._V1_SX300.jpg\n",
      "Ratings:  [{'Source': 'Internet Movie Database', 'Value': '6.2/10'}, {'Source': 'Rotten Tomatoes', 'Value': '31%'}, {'Source': 'Metacritic', 'Value': '46/100'}]\n",
      "Metascore:  46\n",
      "imdbRating:  6.2\n",
      "imdbVotes:  69,565\n",
      "imdbID:  tt0113243\n",
      "Type:  movie\n",
      "DVD:  24 Apr 2001\n",
      "BoxOffice:  $7,563,728\n",
      "Production:  N/A\n",
      "Website:  N/A\n",
      "Response:  True\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url =\"http://www.omdbapi.com/?t=hackers&apikey=72bc447a\"\n",
    "res = requests.get(url)\n",
    "json_data = res.json()\n",
    "for key in json_data.keys():\n",
    "    print(key + \": \",json_data[key] )"
   ],
   "execution_count": 70,
   "cell_type": "code"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 10. What was that URL?\n",
    "\n",
    "Now the last thing to discuss is how the URL we used actually pulled data from the API. To do so, lets break it up into chunks. The http signifies that we're making an HTTP request, the 'www dot omdb dot api' that we're querying the OMDB API, then there's the \"?t equals hackers\" which is the really interesting part and something we haven't discussed yet in this course. This string that begins with a question mark is called a Query String. Query Strings are parts of URLs that do not necessarily fit into conventional a hierarchical path structure. What follows the question mark in the query string is the query we are making to the OMBD API. The query we just made was simple : querying 't equals hackers' asked the API to return the data about the movie with the title Hackers. The 't' in the query stood for title."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 11. OMDb API\n",
    "\n",
    "We knew that this was how to perform such a query from the documentation on the OMDB API's homepage. Under \"Usage\" here, they state explicitly that 'Send all data requests to: http:// www dot omdbapi dot com /?'."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 12. OMDb API\n",
    "\n",
    "They also have a query string parameters table that shows how to query a particular title or a particular movie ID."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 13. It’s a regular URL!\n",
    "\n",
    "It is also worth mentioning that there is nothing special about this URL and so you can also navigate to it in your browser of choice. It will generally look like this. I like to use a Chrome extension called JSON formatter to make it a bit prettier. Alright. Now you know all about APIs and have a basic practical understanding of how to query them,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 14. Let's practice!\n",
    "\n",
    "lets get you writing some Python to extract some data from a number of APIs! Happy coding!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## API requests\n",
    "\n",
    "<p>Now it's your turn to pull some movie data down from the Open Movie Database (OMDB) using their API.\n",
    "The movie you'll query the API about is <em>The Social Network</em>.\n",
    "Recall that, in the video, to query the API about the movie <em>Hackers</em>, Hugo's <em>query string</em> was <code>'http://www.omdbapi.com/?t=hackers'</code> and had a single argument <code>t=hackers</code>.</p>\n",
    "<p>Note: recently, OMDB has changed their API: you now also have to specify an API key. This means you'll have to add another argument to the URL: <code>apikey=72bc447a</code>.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Import the <code>requests</code> package."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assign to the variable <code>url</code> the URL of interest in order to query <code>'http://www.omdbapi.com'</code> for the data corresponding to the movie <em>The Social Network</em>. The <em>query string</em> should have two arguments: <code>apikey=72bc447a</code> and <code>t=the+social+network</code>. You can combine them as follows: <code>apikey=72bc447a&amp;t=the+social+network</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = \"http://www.omdbapi.com/?t=the+social+network&apikey=72bc447a\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the text of the response object <code>r</code> by using its <code>text</code> attribute and passing the result to the <code>print()</code> function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Title\":\"The Social Network\",\"Year\":\"2010\",\"Rated\":\"PG-13\",\"Released\":\"01 Oct 2010\",\"Runtime\":\"120 min\",\"Genre\":\"Biography, Drama\",\"Director\":\"David Fincher\",\"Writer\":\"Aaron Sorkin, Ben Mezrich\",\"Actors\":\"Jesse Eisenberg, Andrew Garfield, Justin Timberlake\",\"Plot\":\"As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea, and by the co-founder who was later squeezed out of the business.\",\"Language\":\"English, French\",\"Country\":\"United States\",\"Awards\":\"Won 3 Oscars. 172 wins & 186 nominations total\",\"Poster\":\"https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\",\"Ratings\":[{\"Source\":\"Internet Movie Database\",\"Value\":\"7.8/10\"},{\"Source\":\"Rotten Tomatoes\",\"Value\":\"96%\"},{\"Source\":\"Metacritic\",\"Value\":\"95/100\"}],\"Metascore\":\"95\",\"imdbRating\":\"7.8\",\"imdbVotes\":\"690,655\",\"imdbID\":\"tt1285016\",\"Type\":\"movie\",\"DVD\":\"11 Jan 2011\",\"BoxOffice\":\"$96,962,694\",\"Production\":\"N/A\",\"Website\":\"N/A\",\"Response\":\"True\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the text of the response\n",
    "print(r.text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## JSON–from the web to Python\n",
    "\n",
    "<p>Wow, congrats! You've just queried your first API programmatically in Python and printed the text of the response to the shell. However, as you know, your response is actually a JSON, so you can do one step better and decode the JSON. You can then print the key-value pairs of the resulting dictionary. That's what you're going to do now!</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Pass the variable <code>url</code> to the <code>requests.get()</code> function in order to send the relevant request and catch the response, assigning the resultant response message to the variable <code>r</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply the <code>json()</code> method to the response object <code>r</code> and store the resulting dictionary in the variable <code>json_data</code>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hit submit to print the key-value pairs of the dictionary <code>json_data</code> to the shell."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  The Social Network\n",
      "Year:  2010\n",
      "Rated:  PG-13\n",
      "Released:  01 Oct 2010\n",
      "Runtime:  120 min\n",
      "Genre:  Biography, Drama\n",
      "Director:  David Fincher\n",
      "Writer:  Aaron Sorkin, Ben Mezrich\n",
      "Actors:  Jesse Eisenberg, Andrew Garfield, Justin Timberlake\n",
      "Plot:  As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea, and by the co-founder who was later squeezed out of the business.\n",
      "Language:  English, French\n",
      "Country:  United States\n",
      "Awards:  Won 3 Oscars. 172 wins & 186 nominations total\n",
      "Poster:  https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\n",
      "Ratings:  [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\n",
      "Metascore:  95\n",
      "imdbRating:  7.8\n",
      "imdbVotes:  690,655\n",
      "imdbID:  tt1285016\n",
      "Type:  movie\n",
      "DVD:  11 Jan 2011\n",
      "BoxOffice:  $96,962,694\n",
      "Production:  N/A\n",
      "Website:  N/A\n",
      "Response:  True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Checking out the Wikipedia API\n",
    "\n",
    "<p>You're doing so well and having so much fun that we're going to throw one more API at you: the Wikipedia API (documented <a href=\"https://www.mediawiki.org/wiki/API:Main_page\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>). You'll figure out how to find and extract information from the Wikipedia page for <em>Pizza</em>. What gets a bit wild here is that your query will return <em>nested</em> JSONs, that is, JSONs with JSONs, but Python can handle that because it will translate them into dictionaries within dictionaries.</p>\n",
    "<p>The URL that requests the relevant query from the Wikipedia API is</p>\n",
    "<pre><code>https://en.wikipedia.org/w/api.php?action=query&amp;prop=extracts&amp;format=json&amp;exintro=&amp;titles=pizza\n",
    "</code></pre>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Assign the relevant URL to the variable <code>url</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "url =\"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Apply the <code>json()</code> method to the response object <code>r</code> and store the resulting dictionary in the variable <code>json_data</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "{'batchcomplete': '',\n 'warnings': {'extracts': {'*': 'HTML may be malformed and/or unbalanced and may omit inline images. Use at your own risk. Known problems are listed at https://www.mediawiki.org/wiki/Special:MyLanguage/Extension:TextExtracts#Caveats.'}},\n 'query': {'normalized': [{'from': 'pizza', 'to': 'Pizza'}],\n  'pages': {'24768': {'pageid': 24768,\n    'ns': 0,\n    'title': 'Pizza',\n    'extract': '<link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1033289096\">\\n<p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Pizza</b> (<small>Italian:\\xa0</small><span title=\"Representation in the International Phonetic Alphabet (IPA)\" lang=\"it-Latn-fonipa\">[ˈpittsa]</span>, <small>Neapolitan:\\xa0</small><span title=\"Representation in the International Phonetic Alphabet (IPA)\" lang=\"nap-Latn-fonipa\">[ˈpittsə]</span>) is a dish of  Italian origin consisting of a usually round, flat base of leavened wheat-based dough topped with tomatoes, cheese, and often various other ingredients (such as various types of sausage, anchovies, mushrooms, onions, olives, vegetables, meat, ham, etc.), which is then baked at a high temperature, traditionally in a wood-fired oven. A small pizza is sometimes called a pizzetta. A person who makes pizza is known as a <b>pizzaiolo</b>.\\n</p><p>In Italy, pizza served in a restaurant is presented unsliced, and is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand.\\n</p><p>The term <i>pizza</i> was first recorded in the 10th century in a Latin manuscript from the Southern Italian town of Gaeta in Lazio, on the border with Campania. Modern pizza was invented in Naples, and the dish and its variants have since become popular in many countries. It has become one of the most popular foods in the world and a common fast food item in Europe, North America and Australasia; available at pizzerias (restaurants specializing in pizza),  restaurants offering Mediterranean cuisine, via pizza delivery, and as street food. Various food companies sell ready-baked pizzas, which may be frozen, in grocery stores, to be reheated in a home oven.\\n</p><p>In 2017, the world pizza market was US$128 billion, and in the US it was $44 billion spread over 76,000 pizzerias.  Overall, 13% of the U.S. population aged 2 years and over consumed pizza on any given day.</p><p>The <i>Associazione Verace Pizza Napoletana</i> (lit. True Neapolitan Pizza Association) is a non-profit organization founded in 1984 with headquarters in Naples that aims to promote traditional Neapolitan pizza. In 2009, upon Italy\\'s request, Neapolitan pizza was registered with the European Union as a Traditional Speciality Guaranteed dish, and in 2017 the art of its making was included on UNESCO\\'s list of intangible cultural heritage.</p><p>Raffaele Esposito is often considered to be the father of modern pizza.</p>'}}}}"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "The variable <code>pizza_extract</code> holds the HTML of an extract from Wikipedia's <em>Pizza</em> page as a string; use the function <code>print()</code> to print this string to the shell."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "pizza_extract = json_data['query']['pages']['24768']['extract']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1033289096\">\n",
      "<p class=\"mw-empty-elt\">\n",
      "</p>\n",
      "<p><b>Pizza</b> (<small>Italian: </small><span title=\"Representation in the International Phonetic Alphabet (IPA)\" lang=\"it-Latn-fonipa\">[ˈpittsa]</span>, <small>Neapolitan: </small><span title=\"Representation in the International Phonetic Alphabet (IPA)\" lang=\"nap-Latn-fonipa\">[ˈpittsə]</span>) is a dish of  Italian origin consisting of a usually round, flat base of leavened wheat-based dough topped with tomatoes, cheese, and often various other ingredients (such as various types of sausage, anchovies, mushrooms, onions, olives, vegetables, meat, ham, etc.), which is then baked at a high temperature, traditionally in a wood-fired oven. A small pizza is sometimes called a pizzetta. A person who makes pizza is known as a <b>pizzaiolo</b>.\n",
      "</p><p>In Italy, pizza served in a restaurant is presented unsliced, and is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand.\n",
      "</p><p>The term <i>pizza</i> was first recorded in the 10th century in a Latin manuscript from the Southern Italian town of Gaeta in Lazio, on the border with Campania. Modern pizza was invented in Naples, and the dish and its variants have since become popular in many countries. It has become one of the most popular foods in the world and a common fast food item in Europe, North America and Australasia; available at pizzerias (restaurants specializing in pizza),  restaurants offering Mediterranean cuisine, via pizza delivery, and as street food. Various food companies sell ready-baked pizzas, which may be frozen, in grocery stores, to be reheated in a home oven.\n",
      "</p><p>In 2017, the world pizza market was US$128 billion, and in the US it was $44 billion spread over 76,000 pizzerias.  Overall, 13% of the U.S. population aged 2 years and over consumed pizza on any given day.</p><p>The <i>Associazione Verace Pizza Napoletana</i> (lit. True Neapolitan Pizza Association) is a non-profit organization founded in 1984 with headquarters in Naples that aims to promote traditional Neapolitan pizza. In 2009, upon Italy's request, Neapolitan pizza was registered with the European Union as a Traditional Speciality Guaranteed dish, and in 2017 the art of its making was included on UNESCO's list of intangible cultural heritage.</p><p>Raffaele Esposito is often considered to be the father of modern pizza.</p>\n"
     ]
    }
   ],
   "source": [
    "print(pizza_extract)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "# Diving deep into the Twitter API"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## The Twitter API and Authentication"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 1. The Twitter API and Authentication\n",
    "\n",
    "Congratulations on interacting with your very first APIs and getting data from them! You're on the home stretch now."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 2. Herein, you’ll learn\n",
    "\n",
    "As a final deep dive, you're going to stream data from the Twitter API. You'll learn how to filter incoming tweets for keywords, you'll learn about the principles of API authentication and OAuth. You'll also learn the basics of the package"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 3. Herein, you’ll learn\n",
    "\n",
    "tweepy, which many people in PythonLand use to interact with the Twitter API."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 4. Access the Twitter API\n",
    "\n",
    "One of the first major differences between the Twitter API and all the APIs you have seen so far is that you were able to access all the others anonymously and Twitter requires that you have an account. In order gain access to the Twitter API, one needs to create a twitter account if you don't already have one,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 5. Access the Twitter API\n",
    "\n",
    "log into Twitter Apps and click \"Create a New App\" - you'll need to agree to a variety of terms and conditions here,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 6. Access the Twitter API\n",
    "\n",
    "then , go to your \"Keys and Access Tokens\" tab and Copy your API key, your API secret,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 7. Access the Twitter API\n",
    "\n",
    "your Access Token and your Access Token secret. These are the Authentication credentials that will allow you to access the Twitter API from Python. In the following interactive exercises, we won't require that you create your own Twitter account and App: we'll do a mock run-through of how you would stream data and analyze it as if you had done so."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 8. Twitter has a number of APIs\n",
    "\n",
    "It is now important to mention that Twitter has a number of APIs. Firstly, they have a REST API; we won't go into the gory details of REST APIs here but I'll say two things - one: REST is short for Representational State Transfer; two: Twitter's REST API allows the user to \"read and write Twitter data\". In order to \"monitor or process Tweets in real-time\","
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 9. Twitter has a number of APIs\n",
    "\n",
    "that is, to stream Twitter data, however, we'll want to use Twitter's Streaming API. In particular,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 10. Twitter has a number of APIs\n",
    "\n",
    "we'll use the public stream, which Twitter's API documentation defines as \"Streams of the public data flowing through Twitter.\" The Public Stream itself contains a number of options. As we want to read and process tweets,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 11. Twitter has a number of APIs\n",
    "\n",
    "we'll want to use the GET statuses/sample API, which \"Returns a small random sample of all public statuses.\""
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 12. Twitter has a number of APIs\n",
    "\n",
    "If you wanted to access absolutely \"All public statuses\", you would need to use Twitter's Firehose API, which is not publicly available and would most likely cost you a pretty penny."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 13. Tweets are returned as JSONs\n",
    "\n",
    "One last point to note before we begin streaming tweets: tweets are returned to us as JSONs and they contain numerous possible fields. Check out the Twitter tweet field guide here. You can get tweet text, user, language, time of tweet,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 14. Tweets are returned as JSONs\n",
    "\n",
    "among many other fields. Let's now see how to access and stream data from the Twitter API! For first-time Python tweet-streamers, I usually recommend the package tweepy,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 15. Using Tweepy: Authentication\n",
    "\n",
    "which has a nice balance of usability and capability. Let's now use it to stream some tweets! First off, we need to create variables to store our access token and secret, plus our consumer key and secret."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 16. Using Tweepy: stream tweets!!\n",
    "\n",
    "To stream, we create an instance of tweepy's Stream class, passing our consumer key, consumer secret, access token, and access token secret. You can then stream tweets that contain keywords of choice by applying the filter method to the object stream! In the following exercises, you'll practice writing Python code to stream tweets and then you'll do some basic analysis of these tweets to see how often particular keywords are mentioned."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 17. Let's practice!\n",
    "\n",
    "Enjoy!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Streaming tweets\n",
    "\n",
    "<p>It's time to stream some tweets! Your task is to create the <code>Stream</code>object and to filter tweets according to particular keywords. <code>tweepy</code> has been imported for you.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Create your <code>Stream</code> object with the credentials given."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Filter your Stream variable for the keywords <code>\"clinton\"</code>, <code>\"trump\"</code>, <code>\"sanders\"</code>, and <code>\"cruz\"</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Load and explore your Twitter data\n",
    "\n",
    "<p>Now that you've got your Twitter data sitting locally in a text file, it's time to explore it! This is what you'll do in the next few interactive exercises. In this exercise, you'll read the Twitter data into a list: <code>tweets_data</code>.</p>\n",
    "<p><em>Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data).</em></p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Assign the filename <code>'tweets.txt'</code> to the variable <code>tweets_data_path</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# Import package\n",
    "import json\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = \"data/tweets3.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize <code>tweets_data</code> as an empty list to store the tweets in."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "\n",
    "# Initialize empty list to store tweets: tweets_data\n",
    "tweets_data = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Within the <code>for</code> loop initiated by <code>for line in tweets_file:</code>, load each tweet into a variable, <code>tweet</code>, using <code>json.loads()</code>, then append <code>tweet</code> to <code>tweets_data</code> using the <code>append()</code> method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hit submit and check out the keys of the first tweet dictionary printed to the shell."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['in_reply_to_user_id', 'created_at', 'filter_level', 'truncated', 'possibly_sensitive', 'timestamp_ms', 'user', 'text', 'extended_entities', 'in_reply_to_status_id', 'entities', 'favorited', 'retweeted', 'is_quote_status', 'id', 'favorite_count', 'retweeted_status', 'in_reply_to_status_id_str', 'in_reply_to_user_id_str', 'id_str', 'in_reply_to_screen_name', 'coordinates', 'lang', 'place', 'contributors', 'geo', 'retweet_count', 'source'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Twitter data to DataFrame\n",
    "\n",
    "<p>Now you have the Twitter data in a list of dictionaries, <code>tweets_data</code>, where each dictionary corresponds to a single tweet. Next, you're going to extract the text and language of each tweet. The text in a tweet, <code>t1</code>, is stored as the value <code>t1['text']</code>; similarly, the language is stored in <code>t1['lang']</code>. Your task is to build a DataFrame in which each row is a tweet and the columns are <code>'text'</code> and <code>'lang'</code>.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Use <code>pd.DataFrame()</code> to construct a DataFrame of tweet texts and languages; to do so, the first argument should be <code>tweets_data</code>, a list of dictionaries. The second argument to <code>pd.DataFrame()</code> is a <em>list</em> of the keys you wish to have as columns. Assign the result of the <code>pd.DataFrame()</code> call to <code>df</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns=[\"text\",\"lang\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Print the head of the DataFrame."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text lang\n",
      "0  RT @bpolitics: .@krollbondrating's Christopher...   en\n",
      "1  RT @HeidiAlpine: @dmartosko Cruz video found.....   en\n",
      "2  Njihuni me Zonjën Trump !!! | Ekskluzive https...   et\n",
      "3  Your an idiot she shouldn't have tried to grab...   en\n",
      "4  RT @AlanLohner: The anti-American D.C. elites ...   en\n"
     ]
    }
   ],
   "source": [
    "# Print head of DataFrame\n",
    "print(df.head())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## A little bit of Twitter text analysis\n",
    "\n",
    "<p>Now that you have your DataFrame of tweets set up, you're going to do a bit of text analysis to count how many tweets contain the words <code>'clinton'</code>, <code>'trump'</code>, <code>'sanders'</code> and <code>'cruz'</code>. In the pre-exercise code, we have defined the following function <code>word_in_text()</code>, which will tell you whether the first argument (a word) occurs within the 2nd argument (a tweet).</p>\n",
    "<pre><code>import re\n",
    "\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "</code></pre>\n",
    "<p>You're going to iterate over the rows of the DataFrame and calculate how many tweets contain each of our keywords! The list of objects for each candidate has been initialized to 0.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "\n",
    "    if match:\n",
    "        return True\n",
    "    return False"
   ],
   "execution_count": 93,
   "cell_type": "code"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Within the <code>for</code> loop <code>for index, row in df.iterrows():</code>, the code currently increases the value of <code>clinton</code> by <code>1</code> each time a tweet (text row) mentioning 'Clinton' is encountered; complete the code so that the same happens for <code>trump</code>, <code>sanders</code> and <code>cruz</code>."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# Initialize list to store tweet counts\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text(\"trump\", row['text'])\n",
    "    sanders += word_in_text(\"sanders\", row['text'])\n",
    "    cruz += word_in_text(\"cruz\", row['text'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Plotting your Twitter data\n",
    "\n",
    "<p>Now that you have the number of tweets that each candidate was mentioned in, you can plot a bar chart of this data. You'll use the statistical data visualization library <a href=\"https://stanford.edu/~mwaskom/software/seaborn/\" target=\"_blank\" rel=\"noopener noreferrer\"><code>seaborn</code></a>, which you may not have seen before, but we'll guide you through. You'll first import <code>seaborn</code> as <code>sns</code>. You'll then construct a barplot of the data using <code>sns.barplot</code>, passing it two arguments: </p>\n",
    "<ol>\n",
    "<li>a list of <em>labels</em> and</li>\n",
    "<li>a list containing the variables you wish to plot (<code>clinton</code>, <code>trump</code> and so on.)</li>\n",
    "</ol>\n",
    "<p>Hopefully, you'll see that Trump was unreasonably represented! We have already run the previous exercise solutions in your environment.</p>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Import both <code>matplotlib.pyplot</code> and <code>seaborn</code> using the aliases <code>plt</code> and <code>sns</code>, respectively."
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "Complete the arguments of <code>sns.barplot</code>: <ul>\n",
    "<li>The first argument should be the list of labels to appear on the x-axis (created in the previous step).</li>\n",
    "<li>The second argument should be a list of the variables you wish to plot, as produced in the previous exercise (i.e. a list containing <code>clinton</code>, <code>trump</code>, etc).</li></ul>"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a list of labels:cd\n",
    "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
    "\n",
    "# Plot the bar chart\n",
    "ax = sns.barplot(cd, [clinton,trump,sanders,cruz])\n",
    "ax.set(ylabel=\"count\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "## Final Thoughts"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 1. Final Thoughts\n",
    "\n",
    "Wowee congratulations! You've just completed your deep dive into the Twitter API, in which you streamed tweets, processed them and visualized your results. Amazing! If you've made it this far,"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 2. What you’ve learned:\n",
    "\n",
    "you're now be able to import data in Python from a wide array of sources. To recap the skill-set and data importing chops that you have gained in this course AND its prequel, you're now adept at importing basic text files and flat files, the basic bread and butter of any working Data Scientist's professional life, local files in other formats, such as Excel spreadsheets, SAS, Stata and MATLAB files, pickled and HDF5 files. These skills will make you an even better collaborator for many working professionals out there. You've built up your basic skills at writing SQL queries and can now get all types of data out of relational databases. You're able to pull data from the web: not only can you import basic web files, but you know a number of ways to issue GET requests and can even do some basic web scraping and HTML parsing! On top of all of this, you've learnt how to pull data from APIs and have had hands-on experience doing so with dives into several examples. You've learned a great deal and"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  },
  {
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "### 3. Let's practice!\n",
    "\n",
    "successfully completed these two courses on Importing Data in Python. Happy importing!"
   ],
   "execution_count": null,
   "cell_type": "markdown"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}